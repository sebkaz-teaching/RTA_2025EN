[
  {
    "objectID": "lecture4.html",
    "href": "lecture4.html",
    "title": "Lecture 4",
    "section": "",
    "text": "⏳ Duration: 1.5h"
  },
  {
    "objectID": "lecture4.html#the-evolution-of-architectural-approaches",
    "href": "lecture4.html#the-evolution-of-architectural-approaches",
    "title": "Lecture 4",
    "section": "The Evolution of Architectural Approaches",
    "text": "The Evolution of Architectural Approaches\nThe development of technology, particularly the shift from monolithic to microservices architecture, has had a profound impact on modern information systems.\nMonolithic applications, which were the dominant approach in the past, consisted of a single, large unit of code. While this approach had its advantages, such as simplicity during the initial stages of system development, it also presented significant drawbacks, including challenges with scaling, limited flexibility, and complex maintenance.\nAs technology evolved, microservices emerged as a solution. This approach involves breaking an application into smaller, independent services, each responsible for a specific functionality. The shift to microservices enabled greater flexibility, easier system scaling, and faster deployment of new features. Additionally, each service can be developed, tested, and deployed independently, simplifying code management and reducing the risk of errors.\nThanks to microservices, organizations can better adapt to changing business needs, improve system availability (by isolating failures to individual services), and innovate more quickly. Furthermore, microservices promote the use of modern techniques like containerization and cloud solutions, which streamline infrastructure management and make better use of resources.\nHowever, despite the many benefits, transitioning to microservices also comes with challenges, such as:\n\nComplexity in managing communication between services\nThe need to monitor and maintain a larger number of components\nManaging distributed transactions\n\nThese challenges require new tools and approaches to management, as well as the implementation of a DevOps culture.\nWith the development of microservices, new technologies such as serverless and containerization have emerged, serving as natural extensions of system flexibility. These technologies further enhance the efficiency of managing and scaling modern applications, becoming key components of the cloud ecosystem.\n\nServerless\nServerless is a model where developers don’t have to manage servers or infrastructure. Instead, cloud providers take care of all the infrastructure, allowing developers to focus solely on the application code. The key advantage of this approach is its scalability – applications automatically scale based on resource demand. Serverless systems allow functions to be dynamically started and stopped in response to specific events, leading to cost optimization (you only pay for the actual resource usage). This approach simplifies managing applications with variable or unpredictable traffic.\nServerless is also a great complement to microservices, enabling the deployment of independent functions in response to various events, which offers even greater flexibility. It can be used for applications such as real-time data processing, API handling, and task automation.\n\n\nContainerization\nContainerization (e.g., using Docker) is another step toward increasing flexibility. With containers, applications and their dependencies are packaged into isolated units that can be run across different environments in a consistent and predictable manner. Containers are lightweight, fast to deploy, and offer easy portability across platforms, which is crucial in microservice architectures.\nContainerization is gaining significance, especially when combined with container management tools like Kubernetes, which automatically scales applications, monitors their status, ensures high availability, and manages their lifecycle. This approach perfectly supports both microservices and serverless, enabling easy deployment, scaling, and monitoring of applications.\n\n\nCommon Goal – Flexibility\nBoth serverless and containerization represent a further step towards flexibility, offering the ability to quickly adapt to changing conditions and demands. Together with microservices, they form a modern approach to application architecture that allows for the separation of responsibilities, easier scaling, dynamic resource allocation, and better utilization of cloud infrastructure.\nThe combination of these technologies allows businesses to rapidly deploy new features, respond to changing user needs, and minimize costs by optimizing resource usage – all of which are particularly important in today’s fast-evolving technological landscape."
  },
  {
    "objectID": "lecture4.html#the-impact-of-technology-on-information-systems",
    "href": "lecture4.html#the-impact-of-technology-on-information-systems",
    "title": "Lecture 4",
    "section": "The Impact of Technology on Information Systems",
    "text": "The Impact of Technology on Information Systems\nNetworking communication, relational databases, cloud solutions, and Big Data have significantly transformed the way information systems are built and how work is carried out within them.\nSimilarly, information-sharing tools such as newspapers, radio, television, the internet, messaging apps, and social media have influenced human interactions and social structures. Each new technological medium shapes how people use computing and perceive its role in everyday life."
  },
  {
    "objectID": "lecture4.html#microservices-architecture",
    "href": "lecture4.html#microservices-architecture",
    "title": "Lecture 4",
    "section": "Microservices architecture",
    "text": "Microservices architecture\nThe concept of microservices is essential to understand when working on architectures. Although there are other ways to architecture software projects, microservices are famous for a good reason. They help teams be flexible and effective and help to keep software loose and structured.\nThe idea behind microservices is in the name: of software is represented as many small services that operate individually. When looking at the overall architecture, each of the microservices is inside a small black box with clearly defined inputs and outputs.\nAn often-chosen solution is to use Application Programming Interfaces ( API) to allow different microservices to communicate\n\nMain Advantages of Microservices:\n\nEfficiency – Each service performs a single, well-defined task (“do one thing, but do it well”).\nFlexibility – They allow for easy modifications and scaling of the system.\nArchitecture Transparency – The system consists of small, independent modules.\n\nMicroservices can be compared to pure functions in functional programming – each service operates independently and has clearly defined inputs and outputs.\nTo enable communication between microservices, Application Programming Interfaces (APIs) are often used, allowing data exchange and integration between different services.\nExample of an API in Microservices – Python & FastAPI Below is an example of a REST API microservice in Python using FastAPI, which returns user information:\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n# Przykładowe dane użytkowników\nusers = {\n    1: {\"name\": \"Anna\", \"age\": 28},\n    2: {\"name\": \"Piotr\", \"age\": 35},\n    3: {\"name\": \"Kasia\", \"age\": 22},\n}\n\n@app.get(\"/users/{user_id}\")\ndef get_user(user_id: int):\n    \"\"\"Zwraca dane użytkownika na podstawie ID.\"\"\"\n    return users.get(user_id, {\"error\": \"User not found\"})\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n\nWe start the FastAPI server.\nWe can retrieve user data by sending a GET request to http://127.0.0.1:8000/users/1.\nThe API will return the data in JSON format, for example:\n\n{\n    \"name\": \"Anna\",\n    \"age\": 28\n}\n\n\nCommunication through API\nA central component in microservice architectures is the use of APIs. An API is a part that allows you to connect two microservices. APIs are much like websites. Like a website, the server sends You the code that represents the website. Your internet browser then interprets this code and shows you a web page.\n\nLet’s take a business case with the ML model as a service.\nLet’s assume you work for a company that sells apartments in Boston. We want to increase our sales and offer better quality services to our customers with a new mobile application which can be used even by 1 000 000 people simultaneously. We can realize this by serving a prediction of house value when the user requests for pricing over the web.\n\n\nServing a Model\n\nTraining a good ML model is ONLY the first part: You do need to make your model available to your end-users You do this by either providing access to the model on your server.\nWhen serving ML Model You need: a model, an interpreter, input data.\nImportant Metrics:\n\n\nLatency,\nCost,\nThroughput (number of requests served per unit time)\n\n\nSharing data between two or more systems has always been a fundamental requirement of software development – DevOps vs MLOps.\n\nBuilding a system ready for a production environment is more complex than training the model itself: - Cleaning and loading appropriate and validated data - Calculating variables and serving them in the correct environment - Serving the model in the most cost-efficient way - Versioning, tracking, and sharing data, models, and other artifacts - Monitoring the infrastructure and the model - Deploying the model on scalable infrastructure - Automating the deployment and training process\nWhen you call an API, the API will receive your request. The request triggers your code to be run on the server and generates a response sent back to you. If something goes wrong, you may not receive any reply or receive an error code as an HTTP status code.\n\nClient-Server: Client (system A) requests to a URL hosted by system B, which returns a response. It’s identical to how a web browser works. A browser requests for a specific URL. The request is routed to a web server that returns an HTML (text) page.\nStateless: The client request should contain all the information necessary to respond.\n\nYou can call APIs with a lot of different tools. Sometimes, you can even use your web browser. Otherwise, tools such as CURL do the job on the command line. You can use tools such as Postman for calling APIs with the user interface.\n\nAll communication is covered in fixed rules and practices, which are called the HTTP protocol.\n\nExample: API Serving an ML Model Below is an example of an API service that exposes an ML model for predicting real estate prices, using FastAPI and Scikit-Learn:\nfrom fastapi import FastAPI\nimport pickle\nimport numpy as np\n\n# Tworzymy API\napp = FastAPI()\n\n# Wczytujemy wcześniej wytrenowany model ML (np. regresję liniową)\nwith open(\"model.pkl\", \"rb\") as f:\n    model = pickle.load(f)\n\n@app.get(\"/predict/\")\ndef predict_price(area: float, bedrooms: int, age: int):\n    \"\"\"\n    Real Estate Price Prediction Based on Features:\n    - area ( m²),\n    - bedrooms (#),\n    - age .\n    \"\"\"\n    features = np.array([[area, bedrooms, age]])\n    price = model.predict(features)[0]\n    return {\"estimated_price\": round(price, 2)}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n\n\n\nRequest\n\nAn Endpoint URL a domain, port, path, query string - http://mydomain:8000/getapi?&val1=43&val2=3\nThe HTTP methods - GET, POST\nHTTP headers contain authentication information, cookies metadata - Content-Type: application/json, text … Accept: application/json, Authorization: Basic abase64string, Tokens etc\nRequest body\n\nThe most common format for interaction between services is the JavaScript Object Notation format. it is a data type that very strongly resembles the dictionary format in Python - key-value object.\n{\n\"RAD\": 1,\n\"PTRATIO\": 15.3, \"INDUS\": 2.31, \"B\": 396.9,\n\"ZN\": 18,\n\"DIS\": 4.09, \"CRIM\": 0.00632, \"RM\": 6.575, \"AGE\": 65.2, \"CHAS\": 0, \"NOX\": 0.538, \"TAX\": 296, \"LSTAT\": 4.98\n}\n\n\nResponse\n\nThe response payload is defined in the response header:\n\n200 OK\nContent-Encoding: gzip\nContent-Type: text/html; charset=utf-8\nDate: Mon, 18 Jul 2016 16:06:00 GMT Server: Apache\nPath=/;\n\nHeader example: “Content-Type” =&gt; “application/json; charset=utf-8”, “Server” =&gt; “Genie/Julia/1.8.5\nBody example:\n\n{\":input\":{\"RAD\":1,\"PTRATIO\":15.3,\"INDUS\":2.31,.....}}, {\":prediction\":[29.919737211857683]}\n\nHTTP status code: • 200 OK is used for successful requests, • 40X Access Denied • 50X Internal server error\n\n\nREST API\nThe Representational State Transfer (REST) API works just like other APIs, but it follows a certain set of style rules that make it reconizable as a REST API: - Client-server architecture - Statelessnes - Cacheability - Layered system - Uniform Interface"
  },
  {
    "objectID": "lecture4.html#publishsubscribe",
    "href": "lecture4.html#publishsubscribe",
    "title": "Lecture 4",
    "section": "Publish/Subscribe",
    "text": "Publish/Subscribe\nThe “Publish/Subscribe” messaging system is crucial for data-driven applications. Pub/Sub messages are a pattern in which the sender (publisher) of a piece of data (message) does not directly target a specific receiver. Pub/Sub systems often have a broker, which is a central point where the messages are stored."
  },
  {
    "objectID": "lecture4.html#apache-kafka",
    "href": "lecture4.html#apache-kafka",
    "title": "Lecture 4",
    "section": "Apache Kafka",
    "text": "Apache Kafka\nOn the Kafka website, you will find the definition:\n\nDistributed Streaming Platform\nWhat is a “distributed streaming platform”?\nFirst, I want to remind you what a “stream” is. Streams are simply limitless data, data that never ends. They keep coming in, and you can process them in real-time.\nAnd what does “distributed” mean? Distributed means that Kafka runs in a cluster, and each node in the group is called a Broker. These brokers are just servers that perform copies of Apache Kafka.\nSo, Kafka is a set of machines working together to handle and process limitless data in real-time.\nBrokers make it reliable, scalable, and fault-tolerant. But why is there a misconception that Kafka is just another “queue-based messaging system”?\nTo answer this, we first need to explain how a queue-based messaging system works.\n\n\nQueue-Based Messaging System\nMessage passing is simply the act of sending a message from one place to another. It has three main “actors”: - Producer: Creates and sends messages to one or more queues. - Queue: A data structure that buffers messages, receiving them from producers and delivering them to consumers in a FIFO (First-In-First-Out) manner. Once a message is received, it is permanently removed from the queue; there is no chance of retrieving it. - Consumer: Subscribes to one or more queues and receives their messages after they are published.\nAnd that’s it; this is how message passing works. As you can see, there is nothing about streams, real-time processing, or clusters in this.\n\n\nApache Kafka Architecture\nFor more information about Kafka, you can visit this link.\nNow that we understand the basics of message passing, let’s dive into the world of Apache Kafka. In Kafka, there are two key concepts: Producers and Consumers, who work similarly to classic queue-based systems, producing and consuming messages.\n\n\nAs seen, Kafka resembles a classic messaging system; however, unlike traditional queues, Kafka uses Topics instead of the concept of a queue.\nTopics and Partitions\nA Topic is the fundamental data transmission channel in Kafka. It can be compared to a folder where messages are stored.\nEach topic has one or more partitions. This division impacts scalability and load balancing. When creating a topic, the number of partitions is defined.\n\nKey Features of Topics and Partitions:\n\nTopic is a logical unit where producers send messages, and consumers read them.\nPartition is a physical subdivision of a topic. It can be compared to files in a folder.\nOffset – Each message in a partition gets a unique identifier (offset), which allows consumers to track which messages have already been processed.\nKafka stores messages on disk, allowing them to be read again (unlike traditional queues, where a message is deleted after being processed).\nConsumers read messages sequentially, from the oldest to the newest.\nIn case of failure, a consumer can resume processing from the last saved offset.\n\n\n\n\n\nKafka Brokers and Cluster\nKafka operates in a distributed manner – this means it can consist of multiple brokers, which work together as a single cluster.\n\n\nKey Information about Brokers - A Broker is a single server in a Kafka cluster, responsible for storing the partitions of topics. - Each broker in the cluster has a unique identifier. - To increase availability and reliability, Kafka uses data replication. - The replication factor defines how many copies of a partition should be stored on different brokers. - If a topic has three partitions and a replication factor of three, it means each partition will be replicated across three different brokers.\nThe number of partitions should be chosen in a way that each broker handles at least one partition.\n\n\nProducers\nIn Kafka, producers are applications or services that create and send messages to topics. This works similarly to queue systems, except Kafka writes messages to partitions.\n\nHow does Kafka assign messages to partitions?\n\nMessages are distributed round-robin to available partitions.\nWe can specify a message key, and Kafka will calculate its hash to determine which partition the message will go to.\nThe message key determines the partition assignment – once the topic is created, the number of partitions cannot be changed without disrupting this mechanism.\n\nExample of message assignment to partitions: - Message 01 goes to partition 0 of topic Topic_1. - Message 02 goes to partition 1 of the same topic. - The next message may go back to partition 0 if round-robin assignment is applied.\nfrom kafka import KafkaProducer\n\n# Tworzymy producenta Kafka\nproducer = KafkaProducer(bootstrap_servers=\"localhost:9092\")\n\n# Wysyłamy wiadomość do tematu \"real_estate\"\ntopic = \"real_estate\"\nmessage = b\"new flat \"\n\nproducer.send(topic, message)\nproducer.flush()\n\nprint(f\"Message in topic: '{topic}'\")\n\n\n\nConsumers\nConsumers in Kafka read and process messages from topics. Each consumer can belong to a consumer group, which allows parallel message processing. - If multiple consumers belong to the same group, Kafka balances the load between them. - If one consumer fails, Kafka will automatically reassign its partitions to another active consumer.\nfrom kafka import KafkaConsumer\n\n# Tworzymy konsumenta, który nasłuchuje temat \"real_estate\"\nconsumer = KafkaConsumer(\"real_estate\", bootstrap_servers=\"localhost:9092\")\n\nprint(\"Wait for new message...\")\n\nfor message in consumer:\n    print(f\"new message: {message.value.decode()}\")\nAnother important concept in Kafka is Consumer Groups. This is crucial when we need to scale the reading of messages. It becomes costly when a single consumer must read from many partitions, so we need to balance the load between our consumers, and this is where consumer groups come in.\nData from a single topic will be load-balanced between consumers, ensuring that our consumers can handle and process the data efficiently. The ideal scenario is to have the same number of consumers in the group as there are partitions in the topic, so each consumer only reads from one partition. When adding consumers to a group, be cautious — if the number of consumers exceeds the number of partitions, some consumers will not read from any topic and will remain idle."
  },
  {
    "objectID": "lecture2.html",
    "href": "lecture2.html",
    "title": "Lecture 2",
    "section": "",
    "text": "⏳ Duration: 1.5h\n🎯 Lecture Objective\nUnderstanding how data has evolved in different industries and the tools used for its analysis today.\nIn this lecture, we will present the evolution of data analysis, showing how technologies and approaches to data processing have changed over the years.\nWe will start with classical tabular structures, move through more advanced graph and text models, and finish with modern approaches to stream processing."
  },
  {
    "objectID": "lecture2.html#tabular-data-sql-tables",
    "href": "lecture2.html#tabular-data-sql-tables",
    "title": "Lecture 2",
    "section": "1. Tabular Data (SQL Tables)",
    "text": "1. Tabular Data (SQL Tables)\nInitially, data was stored in tables, where each table contained organized information in columns and rows (e.g., SQL databases).\nSuch models were perfect for structured data.\n\n📌 Features:\n✅ Data divided into columns with a fixed structure.\n✅ CRUD operations (Create, Read, Update, Delete) can be applied.\n✅ Strict consistency and normalization rules.\n\n\n📌 Examples:\n➡️ Banking systems, e-commerce, ERP, CRM systems.\n\n\n🖥️ Example Python Code (SQLite):\nimport sqlite3\nconn = sqlite3.connect(':memory:')\ncursor = conn.cursor()\ncursor.execute(\"CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT, age INTEGER)\")\ncursor.execute(\"INSERT INTO users (name, age) VALUES ('Alice', 30)\")\ncursor.execute(\"SELECT * FROM users\")\nprint(cursor.fetchall())\nconn.close()"
  },
  {
    "objectID": "lecture2.html#graph-data",
    "href": "lecture2.html#graph-data",
    "title": "Lecture 2",
    "section": "2. Graph Data",
    "text": "2. Graph Data\nAs business needs grew, graph data emerged, where relationships between objects are represented as nodes and edges.\n\n📌 Features:\n✅ Data describing relationships and connections. ✅ Flexible structure (graphs instead of tables). ✅ Allows analysis of connections (e.g., PageRank algorithms, centrality).\n\n\n📌 Examples:\n➡️ Social networks (Facebook, LinkedIn), search engines (Google), recommendation systems (Netflix, Amazon).\n\n\n🖥️ Example Python Code (Karate Graph - NetworkX) :\n\n\nCode\nimport networkx as nx\nG = nx.karate_club_graph()\nnx.draw(G, with_labels=True)"
  },
  {
    "objectID": "lecture2.html#semi-structured-data-json-xml-yaml",
    "href": "lecture2.html#semi-structured-data-json-xml-yaml",
    "title": "Lecture 2",
    "section": "3. Semi-structured Data (JSON, XML, YAML)",
    "text": "3. Semi-structured Data (JSON, XML, YAML)\nThese data are not fully structured like in SQL databases, but they have some schema.\n\n📌 Features:\n✅ Hierarchical structure (e.g., key-value pairs, nested objects). ✅ No strict schema (possibility to add new fields). ✅ Popular in NoSQL systems and APIs.\n\n\n📌 Examples:\n➡️ Documents in MongoDB, configuration files, REST APIs, log files.\n\n\n🖥️ Example Python Code (JSON):\n\n\nCode\nimport json\ndata = {'name': 'Alice', 'age': 30, 'city': 'New York'}\njson_str = json.dumps(data)\nprint(json.loads(json_str))\n\n\n{'name': 'Alice', 'age': 30, 'city': 'New York'}"
  },
  {
    "objectID": "lecture2.html#text-data-nlp",
    "href": "lecture2.html#text-data-nlp",
    "title": "Lecture 2",
    "section": "4. Text Data (NLP)",
    "text": "4. Text Data (NLP)\nText has become a key source of information, especially in sentiment analysis, chatbots, and search engines.\n\n📌 Features:\n✅ Unstructured data requiring transformation. ✅ Use of embeddings (e.g., Word2Vec, BERT, GPT). ✅ Widely used in sentiment analysis and chatbots.\n\n\n📌 Examples:\n➡️ Social media, emails, chatbots, machine translation.\n\n\n🖥️ Example Python Code :\n\n\nCode\nimport ollama\n\n# Przykładowe zdanie\nsentence = \"Artificial intelligence is changing the world.\"\nresponse = ollama.embeddings(model='llama3.2', prompt=sentence)\nembedding = response['embedding']\nprint(embedding[:4])\n\n\n[-2.021953582763672, 1.5604140758514404, -0.5358548164367676, -1.3182345628738403]"
  },
  {
    "objectID": "lecture2.html#multimedia-data-images-sound-video",
    "href": "lecture2.html#multimedia-data-images-sound-video",
    "title": "Lecture 2",
    "section": "5. Multimedia Data (Images, Sound, Video)",
    "text": "5. Multimedia Data (Images, Sound, Video)\nModern data analysis systems also use images and sound.\n\n📌 Features:\n✅ Require significant computational power (AI, deep learning). ✅ Processed by CNN models (images) and RNN/Transformers (sound).\n\n\n📌 Examples:\n➡️ Face recognition, speech analysis, biometrics, video content analysis.\n\n\n🖥️ Example Python Code (Image - OpenCV) :\nimport cv2\nimage = cv2.imread('cloud.jpeg')\ncv2.waitKey(0)\ncv2.destroyAllWindows()"
  },
  {
    "objectID": "lecture2.html#hadoop-map-reduce-scaling-computation-on-big-data",
    "href": "lecture2.html#hadoop-map-reduce-scaling-computation-on-big-data",
    "title": "Lecture 2",
    "section": "Hadoop Map-Reduce – Scaling Computation on Big Data",
    "text": "Hadoop Map-Reduce – Scaling Computation on Big Data\nWhen we talk about scalable data processing, the first association might be Google.\nBut what actually enables us to search for information in a fraction of a second while processing petabytes of data?\n👉 Did you know that the name “Google” comes from the word “Googol,” which represents the number 10¹⁰⁰?\nThat’s more than the number of atoms in the known universe! 🌌\n\n🔥 Challenge: Can you write out the number Googol by the end of this lecture?"
  },
  {
    "objectID": "lecture2.html#why-are-sql-and-traditional-algorithms-insufficient",
    "href": "lecture2.html#why-are-sql-and-traditional-algorithms-insufficient",
    "title": "Lecture 2",
    "section": "🔍 Why Are SQL and Traditional Algorithms Insufficient?",
    "text": "🔍 Why Are SQL and Traditional Algorithms Insufficient?\nTraditional SQL databases and single-threaded algorithms fail when data scales beyond a single computer.\nThis is where MapReduce comes in—a revolutionary computational model developed by Google.\n\n🛠️ Google’s Solutions for Big Data:\n✅ Google File System (GFS) – a distributed file system.\n✅ Bigtable – a system for storing massive amounts of structured data.\n✅ MapReduce – an algorithm for distributing workloads across multiple machines."
  },
  {
    "objectID": "lecture2.html#graphical-representation-of-mapreduce",
    "href": "lecture2.html#graphical-representation-of-mapreduce",
    "title": "Lecture 2",
    "section": "🖼️ Graphical Representation of MapReduce",
    "text": "🖼️ Graphical Representation of MapReduce\n\n1. Mapping Splits Tasks (Map)\nEach input is divided into smaller parts and processed in parallel.\n🌍 Imagine you have a phone book and want to find all people with the last name “Nowak”.\n➡️ Divide the book into sections and give each person one section to analyze.\n\n\n2. Reducing Gathers the Results (Reduce)\nAll partial results are combined into one final answer.\n🔄 All students report their findings, and one student collects and summarizes the response."
  },
  {
    "objectID": "lecture2.html#classic-example-counting-words-in-a-text",
    "href": "lecture2.html#classic-example-counting-words-in-a-text",
    "title": "Lecture 2",
    "section": "💡 Classic Example: Counting Words in a Text",
    "text": "💡 Classic Example: Counting Words in a Text\nLet’s assume we have millions of books and we want to count how many times each word appears.\n\n🖥️ MapReduce Code in Python (Using Multiprocessing)\nfrom multiprocessing import Pool\nfrom collections import Counter\n\n# Map function (splitting text into words)\ndef map_function(text):\n    words = text.split()\n    return Counter(words)\n\n# Reduce function (summing up results)\ndef reduce_function(counters):\n    total_count = Counter()\n    for counter in counters:\n        total_count.update(counter)\n    return total_count\n\ntexts = [\n        \"big data is amazing\",\n        \"data science and big data\",\n        \"big data is everywhere\"\n    ]\nif __name__ == '__main__':    \n    with Pool() as pool:\n        mapped_results = pool.map(map_function, texts)\n    \n    final_result = reduce_function(mapped_results)\n    print(final_result)\n\n# Counter({'data': 4, 'big': 3, 'is': 2, 'amazing': 1, 'science': 1, 'and': 1, 'everywhere': 1})\n\n\n🔹 What’s Happening Here?\n✅ Each text fragment is processed independently (map). ✅ The results are collected and summed (reduce). ✅ Outcome: We can process terabytes of text in parallel!"
  },
  {
    "objectID": "lecture2.html#visualization-comparison-of-the-classic-approach-and-mapreduce",
    "href": "lecture2.html#visualization-comparison-of-the-classic-approach-and-mapreduce",
    "title": "Lecture 2",
    "section": "🎨 Visualization – Comparison of the Classic Approach and MapReduce",
    "text": "🎨 Visualization – Comparison of the Classic Approach and MapReduce\n📊 Old Approach – A single computer processes everything sequentially.\n📊 New Approach (MapReduce) – Each machine processes a fragment, and the results are aggregated."
  },
  {
    "objectID": "lecture2.html#challenge-for-you",
    "href": "lecture2.html#challenge-for-you",
    "title": "Lecture 2",
    "section": "🚀 Challenge for You!",
    "text": "🚀 Challenge for You!\n🔹 Find and run your own MapReduce algorithm in any programming language!\n🔹 Can you implement your own MapReduce for a different task? (e.g., log analysis, counting website clicks)"
  },
  {
    "objectID": "lecture2.html#big-data-1",
    "href": "lecture2.html#big-data-1",
    "title": "Lecture 2",
    "section": "Big Data",
    "text": "Big Data\nBig Data systems can serve as a source for data warehouses (e.g., Data Lake, Enterprise Data Hub).\nHowever, Data Warehouses are not Big Data systems!\n\n1. Data Warehouses\n\nStore highly structured data\n\nFocused on analytics and reporting processes\n\n100% accuracy\n\n\n\n2. Big Data\n\nCan handle data of any structure\n\nUsed for various data-driven purposes (analytics, data science, etc.)\n\nLess than 100% accuracy\n\n\n“Big Data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so everyone claims they are doing it.”\n— Dan Ariely, Professor of Psychology and Behavioral Economics, Duke University\n\n\n\nOne, Two, … Four V\n\nVolume – The size of data produced worldwide is growing at an exponential rate.\n\nVelocity – The speed at which data is generated, transmitted, and processed.\n\nVariety – Traditional data is alphanumeric, consisting of letters and numbers. Today, we also deal with images, audio, video files, and IoT data streams.\n\nVeracity – Are the data complete and accurate? Do they objectively reflect reality? Are they a reliable basis for decision-making?\n\nValue – The actual worth of the data. Ultimately, it’s about cost and benefits.\n\n\n“The purpose of computing is insight, not numbers.” – R.W. Hamming, 1962."
  },
  {
    "objectID": "lecture2.html#data-processing-models",
    "href": "lecture2.html#data-processing-models",
    "title": "Lecture 2",
    "section": "Data Processing Models",
    "text": "Data Processing Models\nData has always been processed in business.\nOver the past decades, the amount of processed data has been steadily increasing, affecting the way data is prepared and handled.\n\nA Bit of History\n\n1960s: Data collections, databases\n\n1970s: Relational data models and their implementation in OLTP systems\n\n1975: First personal computers\n\n1980s: Advanced data models (extended-relational, object-oriented, application-oriented, etc.)\n\n1983: The beginning of the Internet\n\n1990s: Data mining, data warehouses, OLAP systems\n\nLater: NoSQL, Hadoop, SPARK, data lakes\n\n2002: AWS, 2005: Hadoop, Cloud computing\n\n\nMost data is stored in databases or data warehouses.\nTypically, data access is performed through applications by executing queries.\nThe method of utilizing and accessing a database is called the data processing model.\nThe two most commonly used implementations are:\n\n\nTraditional Model\nThe traditional model refers to online transaction processing (OLTP),\nwhich excels at handling real-time tasks such as customer service, order management, and sales processing.\nIt is commonly used in Enterprise Resource Planning (ERP) systems, Customer Relationship Management (CRM) software, and web-based applications.\n\nThis model provides efficient solutions for:\n\nSecure and efficient data storage\n\nTransactional data recovery after failures\n\nOptimized data access\n\nConcurrency management\n\nEvent processing → read → write\n\nHowever, what happens when we need to deal with:\n\nAggregation of data from multiple systems (e.g., multiple stores)\n\nReporting and data summarization\n\nOptimization of complex queries\n\nBusiness decision support\n\nResearch on these topics led to the formulation of a new data processing model and a new type of database – Data Warehouses.\n\n\nOLAP Model\nOnline Analytical Processing (OLAP)\nOLAP supports data analysis and provides tools for multidimensional analysis\nbased on dimensions such as time, location, and product.\nThe process of extracting data from various systems into a single database is known as Extract-Transform-Load (ETL),\nwhich involves normalization, encoding, and schema transformation.\nAnalyzing data in a data warehouse mainly involves calculating aggregates (summaries) across different dimensions.\nThis process is entirely user-driven.\n\n\nExample\nImagine we have access to a data warehouse storing sales information from a supermarket.\nHow can we analyze queries such as:\n\nWhat is the total sales of products in the subsequent quarters, months, and weeks?\nWhat is the sales breakdown by product categories?\nWhat is the sales breakdown by supermarket branches?\n\nAnswers to these questions help identify bottlenecks in product sales, plan inventory levels, and compare sales across different product groups and supermarket branches.\nIn a Data Warehouse, two types of queries are most commonly executed (both in batch mode):\n\nPeriodic queries that generate business statistics, such as reporting queries.\nAd-hoc queries that support critical business decisions."
  },
  {
    "objectID": "lecture1.html",
    "href": "lecture1.html",
    "title": "Lecture 1",
    "section": "",
    "text": "⏳ Duration: 1.5h\n🎯 Lecture Goal\nIntroducing students to the fundamentals of real-time analytics, the differences between data processing modes (batch, streaming, real-time), as well as key applications and challenges."
  },
  {
    "objectID": "lecture1.html#what-is-real-time-data-analytics",
    "href": "lecture1.html#what-is-real-time-data-analytics",
    "title": "Lecture 1",
    "section": "What is Real-Time Data Analytics?",
    "text": "What is Real-Time Data Analytics?\n\nDefinition and Key Concepts\nReal-Time Data Analytics is the process of processing and analyzing data immediately after it is generated, without the need for storage and later processing. The goal is to obtain instant insights and responses to changing conditions in business, technology, and scientific systems.\n\n\nKey Features of Real-Time Data Analytics:\n\nLow latency – data is analyzed within milliseconds or seconds of being generated.\nStreaming vs. Batch Processing – data analysis can occur continuously (streaming) or at predefined intervals (batch).\nIntegration with IoT, AI, and ML – real-time analytics often works in conjunction with the Internet of Things (IoT) and artificial intelligence algorithms.\nReal-time decision-making – e.g., instant fraud detection in banking transactions."
  },
  {
    "objectID": "lecture1.html#business-applications-of-real-time-data-analytics",
    "href": "lecture1.html#business-applications-of-real-time-data-analytics",
    "title": "Lecture 1",
    "section": "Business Applications of Real-Time Data Analytics",
    "text": "Business Applications of Real-Time Data Analytics\n\nFinance and Banking\n\nFraud Detection – real-time transaction analysis helps identify anomalies indicating fraud.\nAutomated Trading – HFT (High-Frequency Trading) systems analyze millions of data points in fractions of a second.\nDynamic Credit Scoring – instant risk assessment of a customer’s creditworthiness.\n\n\n\nE-Commerce and Digital Marketing\n\nReal-Time Offer Personalization – dynamic product recommendations based on users’ current behavior.\nDynamic Pricing – companies like Uber, Amazon, and hotels adjust prices in real time based on demand.\nSocial Media Monitoring – sentiment analysis of customer feedback and immediate response to negative comments.\n\n\n\nTelecommunications and IoT\n\nNetwork Infrastructure Monitoring – real-time log analysis helps detect failures before they occur.\nSmart Cities – real-time traffic analysis optimizes traffic light systems dynamically.\nIoT Analytics – IoT devices generate data streams that can be analyzed in real time (e.g., smart energy meters).\n\n\n\nHealthcare\n\nPatient Monitoring – real-time analysis of medical device signals to detect life-threatening conditions instantly.\nEpidemiological Analytics – tracking disease outbreaks based on real-time data.\n\nReal-time data analytics is a key component of modern IT systems, enabling businesses to make faster and more precise decisions. It is widely used across industries—from finance and e-commerce to healthcare and IoT."
  },
  {
    "objectID": "lecture1.html#differences-between-batch-processing-near-real-time-analytics-and-real-time-analytics",
    "href": "lecture1.html#differences-between-batch-processing-near-real-time-analytics-and-real-time-analytics",
    "title": "Lecture 1",
    "section": "Differences Between Batch Processing, Near Real-Time Analytics, and Real-Time Analytics",
    "text": "Differences Between Batch Processing, Near Real-Time Analytics, and Real-Time Analytics\nThere are three main approaches to data processing:\n\nBatch Processing\n\nNear Real-Time Analytics\n\nReal-Time Analytics\n\nEach differs in processing speed, technological requirements, and business applications.\n\nBatch Processing\n📌 Definition:\nBatch Processing involves collecting large amounts of data and processing them at scheduled intervals (e.g., hourly, daily, or weekly).\n📌 Characteristics:\n\n✅ High efficiency for large datasets\n\n✅ Processes data after it has been collected\n\n✅ Does not require immediate analysis\n\n✅ Typically cheaper than real-time processing\n\n❌ Delays – results are available only after processing is complete\n\n📌 Use Cases:\n\nGenerating financial reports at the end of a day/month\n\nAnalyzing sales trends based on historical data\n\nCreating offline machine learning models\n\n📌 Example Technologies:\n\nHadoop MapReduce\n\nApache Spark (in batch mode)\n\nGoogle BigQuery\n\nimport pandas as pd  \ndf = pd.read_csv(\"transactions.csv\")  \n\ndf['transaction_date'] = pd.to_datetime(df['transaction_date'])\ndf['month'] = df['transaction_date'].dt.to_period('M') \n\n# Agg\nmonthly_sales = df.groupby(['month'])['amount'].sum()\n\nmonthly_sales.to_csv(\"monthly_report.csv\")  \n\nprint(\"Raport save!\")\nIf you wanted to create data for an example.\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)\ndata = {\n    'transaction_id': [f'TX{str(i).zfill(4)}' for i in range(1, 1001)],\n    'amount': np.random.uniform(10, 10000, 1000), \n    'transaction_date': pd.date_range(start=\"2025-01-01\", periods=1000, freq='h'), \n    'merchant': np.random.choice(['Merchant_A', 'Merchant_B', 'Merchant_C', 'Merchant_D'], 1000),\n    'card_type': np.random.choice(['Visa', 'MasterCard', 'AmEx'], 1000)\n}\n\ndf = pd.DataFrame(data)\ncsv_file = 'transactions.csv'\ndf.to_csv(csv_file, index=False)"
  },
  {
    "objectID": "lecture1.html#near-real-time-analytics-analysis-nearly-in-real-time",
    "href": "lecture1.html#near-real-time-analytics-analysis-nearly-in-real-time",
    "title": "Lecture 1",
    "section": "Near Real-Time Analytics – Analysis Nearly in Real Time",
    "text": "Near Real-Time Analytics – Analysis Nearly in Real Time\n📌 Definition:\nNear Real-Time Analytics refers to data analysis that occurs with minimal delay (typically from a few seconds to a few minutes). It is used in scenarios where full real-time analysis is not necessary, but excessive delays could impact business operations.\n📌 Characteristics:\n\n✅ Processes data in short intervals (a few seconds to minutes)\n\n✅ Enables quick decision-making but does not require millisecond-level reactions\n\n✅ Optimal balance between cost and speed\n\n❌ Not suitable for systems requiring immediate response\n\n📌 Use Cases:\n\nMonitoring banking transactions and detecting fraud (e.g., analysis within 30 seconds)\n\nDynamically adjusting online ads based on user behavior\n\nAnalyzing server and network logs to detect anomalies\n\n📌 Example Technologies:\n\nApache Kafka + Spark Streaming\n\nElasticsearch + Kibana (e.g., IT log analysis)\n\nAmazon Kinesis\n\nExample of a data producer sending transactions to an Apache Kafka system.\nfrom kafka import KafkaProducer\nimport json\nimport random\nimport time\nfrom datetime import datetime\n\n# Ustawienia dla producenta\nbootstrap_servers = 'localhost:9092'\ntopic = 'transactions' \n\n\ndef generate_transaction():\n    transaction = {\n        'transaction_id': f'TX{random.randint(1000, 9999)}',\n        'amount': round(random.uniform(10, 10000), 2),  \n        'transaction_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        'merchant': random.choice(['Merchant_A', 'Merchant_B', 'Merchant_C', 'Merchant_D']),\n        'card_type': random.choice(['Visa', 'MasterCard', 'AmEx']),\n    }\n    return transaction\n\nproducer = KafkaProducer(\n    bootstrap_servers=bootstrap_servers,\n    value_serializer=lambda v: json.dumps(v).encode('utf-8') \n)\n\n\nfor _ in range(1000):  \n    transaction = generate_transaction()\n    producer.send(topic, value=transaction) \n    print(f\"Sent: {transaction}\")\n    time.sleep(1) \n\nproducer.flush()\nproducer.close()\nConsument example\nfrom kafka import KafkaConsumer\nimport json  \n\nconsumer = KafkaConsumer(\n    'transactions',\n    bootstrap_servers='localhost:9092',\n    auto_offset_reset='earliest',\n    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n)\n\n\nfor message in consumer:\n    transaction = message.value\n    if transaction[\"amount\"] &gt; 8000:\n        print(f\"🚨 Anomaly transaction: {transaction}\")\nExample of the DataSet\n{\n    \"transaction_id\": \"TX1234\",\n    \"amount\": 523.47,\n    \"transaction_date\": \"2025-02-11 08:10:45\",\n    \"merchant\": \"Merchant_A\",\n    \"card_type\": \"Visa\"\n}\n\nReal-Time Analytics\n📌 Definition:\nReal-Time Analytics refers to the immediate analysis of data and decision-making within fractions of a second (milliseconds to one second). It is used in systems requiring real-time responses, such as stock market transactions, IoT systems, and cybersecurity.\n📌 Characteristics:\n\n✅ Extremely low latency (milliseconds to seconds)\n\n✅ Enables instant system response\n\n✅ Requires high computational power and scalable architecture\n\n❌ More expensive and technologically complex than batch processing\n\n📌 Use Cases:\n\nHigh-Frequency Trading (HFT) – making stock market transaction decisions within milliseconds\n\nAutonomous Vehicles – real-time analysis of data streams from cameras and sensors\n\nCybersecurity – detecting network attacks in fractions of a second\n\nIoT Analytics – instant anomaly detection in industrial sensor data\n\n📌 Example Technologies:\n\nApache Flink\n\nApache Storm\n\nGoogle Dataflow\n\n🔎 Comparison:\n\n\n\n\n\n\n\n\n\nFeature\nBatch Processing\nNear Real-Time Analytics\nReal-Time Analytics\n\n\n\n\nLatency\nMinutes – hours – days\nSeconds – minutes\nMilliseconds – seconds\n\n\nProcessing Type\nBatch (offline)\nStreaming (but not fully immediate)\nStreaming (true real-time)\n\n\nInfrastructure Cost\n📉 Low\n📈 Medium\n📈📈 High\n\n\nImplementation Complexity\n📉 Simple\n📈 Medium\n📈📈 Difficult\n\n\nUse Cases\nReports, offline ML, historical analysis\nTransaction monitoring, dynamic ads\nHFT, IoT, real-time fraud detection\n\n\n\n📌 When to Use Batch Processing?\n\n✅ When immediate analysis is not required\n\n✅ When handling large volumes of data processed periodically\n\n✅ When aiming to reduce costs\n\n📌 When to Use Near Real-Time Analytics?\n\n✅ When analysis is needed within a short time (seconds – minutes)\n\n✅ When fresher data is required but not full real-time processing\n\n✅ When seeking a balance between performance and cost\n\n📌 When to Use Real-Time Analytics?\n\n✅ When every millisecond matters (e.g., stock trading, autonomous vehicles)\n\n✅ When detecting fraud, anomalies, or incidents instantly\n\n✅ When a system must respond to events immediately\n\nReal-time analytics is not always necessary—often, near real-time is sufficient and more cost-effective. The key is to understand business requirements before choosing the right solution."
  },
  {
    "objectID": "lecture1.html#why-is-real-time-analytics-important",
    "href": "lecture1.html#why-is-real-time-analytics-important",
    "title": "Lecture 1",
    "section": "Why is Real-Time Analytics Important?",
    "text": "Why is Real-Time Analytics Important?\nReal-time analytics is becoming increasingly crucial across various industries as it enables organizations to make immediate decisions based on up-to-date data. Here are some key reasons why real-time analytics matters:\n\nFaster Decision-Making\nReal-time analytics allows businesses to react to changes and events instantly. This is essential in dynamic environments such as:\n\nMarketing: Ads can be adjusted in real time based on user behavior (e.g., personalized content recommendations).\n\nFinance: Fraud detection in real-time, where every minute counts in preventing financial losses.\n\n\n\nReal-Time Monitoring\nCompanies can continuously track key operational metrics. Examples:\n\nIoT (Internet of Things): Monitoring the condition of machines and equipment in factories to detect failures and prevent downtime.\n\nHealthtech: Tracking patients’ vital signs and detecting anomalies, which can save lives.\n\n\n\nImproved Operational Efficiency\nReal-time analytics helps identify and address operational issues before they escalate. Examples:\n\nLogistics: Tracking shipments and monitoring transport status in real time to improve efficiency and reduce delays.\n\nRetail: Monitoring inventory levels in real-time and adjusting orders accordingly.\n\n\n\nCompetitive Advantage\nOrganizations leveraging real-time analytics gain an edge by responding faster to market changes, customer needs, and crises. With real-time insights:\n\nBusinesses can make proactive decisions ahead of competitors.\n\nCompanies can enhance customer relationships by responding instantly to their needs (e.g., adjusting offerings dynamically).\n\n\n\nEnhanced User Experience (Customer Experience)\nReal-time data analysis enables personalized user interactions as they happen. Examples:\n\nE-commerce: Analyzing shopping cart behavior in real time to offer discounts or remind users of abandoned items.\n\nStreaming Services: Optimizing video/streaming quality based on available bandwidth.\n\n\n\nAnomaly Detection and Threat Prevention\nIn today’s data-driven world, real-time anomaly detection is critical for security. Examples:\n\nCybersecurity: Detecting suspicious network activities and preventing attacks in real time (e.g., DDoS attacks, unauthorized logins).\n\nFraud Prevention: Instant identification of suspicious transactions in banking and credit card systems.\n\n\n\nCost Optimization\nReal-time analytics helps optimize resources and reduce costs. Examples:\n\nEnergy Management: Monitoring energy consumption in real time to optimize corporate energy expenses.\n\nSupply Chain Optimization: Tracking inventory and deliveries to reduce storage and transportation costs.\n\n\n\nPredictive Capabilities\nReal-time analytics supports predictive processes that anticipate future behaviors or problems and address them before they occur. Examples:\n\nPredictive Maintenance: Combining real-time data with predictive models to foresee machine failures.\n\nDemand Forecasting: Adjusting production or stock levels based on live market trends.\n\nReal-time analytics is not just about data analysis—it is a crucial element of business strategy in a world that demands agility, flexibility, and rapid adaptation. Companies that implement these technologies can significantly enhance their financial performance, customer service, operational efficiency, and competitive advantage."
  },
  {
    "objectID": "info.html",
    "href": "info.html",
    "title": "All tools",
    "section": "",
    "text": "For our first a few laboratories we will use just python codes. Check what is Your Python3 environment.\nIn the terminal try first:\npython\n# and\npython3\nI have python3 (You shouldn’t use python 2.7 version) so i create a new and a clear python environment.\nThe easiest way how to run a JupyterLab with your new python env. For  You can choose what You want.\npython3 -m venv &lt;name of Your env&gt;\n\nsource &lt;name of your env&gt;/bin/activate\n# . env/bin/activate\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# or\npip install -r requirements.txt\n\njupyterlab\ngo to web browser: localhost:8888\nIf You want rerun jupyterlab (after computer reset) just go to Your folder and run:\nsource &lt;name of your env&gt;/bin/activate\njupyterlab"
  },
  {
    "objectID": "info.html#python-env-with-jupyter-lab",
    "href": "info.html#python-env-with-jupyter-lab",
    "title": "All tools",
    "section": "",
    "text": "For our first a few laboratories we will use just python codes. Check what is Your Python3 environment.\nIn the terminal try first:\npython\n# and\npython3\nI have python3 (You shouldn’t use python 2.7 version) so i create a new and a clear python environment.\nThe easiest way how to run a JupyterLab with your new python env. For  You can choose what You want.\npython3 -m venv &lt;name of Your env&gt;\n\nsource &lt;name of your env&gt;/bin/activate\n# . env/bin/activate\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# or\npip install -r requirements.txt\n\njupyterlab\ngo to web browser: localhost:8888\nIf You want rerun jupyterlab (after computer reset) just go to Your folder and run:\nsource &lt;name of your env&gt;/bin/activate\njupyterlab"
  },
  {
    "objectID": "info.html#python-env-with-jupyterlab-docker-version",
    "href": "info.html#python-env-with-jupyterlab-docker-version",
    "title": "All tools",
    "section": "Python env with JupyterLAB Docker Version",
    "text": "Python env with JupyterLAB Docker Version\n\nCookiecutter project\nFrom GitHub repository You can find how to use a cookiecutter for any data science project or other kind of programs.\nTo run and build full dockerfile project: Create python env and install cookiecutter library.\npython3 -m venv venv\nsource venv/bin/activate\npip --no-cache install --upgrade pip setuptools\npip install cookiecutter\nand run:\ncookiecutter https://github.com/sebkaz/jupyterlab-project\nYou can run a cookiecutter project directly from GitHub repo.\nAnswer questions:\ncd jupyterlab\ndocker-compose up -d --build\nTo stop:\ndocker-compose down\n\n\nCookiecutter with config yaml file\n\nPython, Julia, R\nAll + Apache Spark\n\nClone repo and run:\npython3 -m cookiecutter https://github.com/sebkaz/jupyterlab-project --no-input --config-file=spark_template.yml --overwrite-if-exists"
  },
  {
    "objectID": "info.html#start-with-github",
    "href": "info.html#start-with-github",
    "title": "All tools",
    "section": "Start with GitHub",
    "text": "Start with GitHub\nText from web site\nWhen You working on a project, e.g. a master’s thesis, (alone or in a team) you often need to check what changes, when and by whom were introduced to the project. The “version control system” or GIT works great for this task.\nYou can download and install Git like a regular program on any computer. However, most often (small projects) you use websites with some kind of git system. One of the most recognized is GitHub (www.github.com) which allows you to use the git system without installing it on your computer.\nIn the free version of the GitHub website, you can store your files in public (everyone has access) repositories. We will only focus on the free version of GitHub:\ngit --version"
  },
  {
    "objectID": "info.html#github",
    "href": "info.html#github",
    "title": "All tools",
    "section": "GitHub",
    "text": "GitHub\nAt the highest level, there are individual accounts (eg. http://github.com/sebkaz or those set up by organizations. Individual users can create ** repositories ** public (public) or private (private).\nOne file should not exceed 100 MB.\nRepo (shortcut to repository) is created with Create a new repository. Each repo should have an individual name.\n\nBranches\nThe main (created by default) branch of the repository is named master.\n\n\nMost important commends\n\nclone of Your repository\n\ngit clone https://adres_repo.git\n\nIn github case, you can download the repository as a ‘zip’ file.\n\n\nRepository for local directory\n\n# new directory\nmkdir datamining\ncd datamining\n# init repo\ngit init\n# there sould be a .git new directory\n# add file\necho \"Info \" &gt;&gt; README.md\n\nlocal and web version connection\n\ngit remote add origin https://github.com/&lt;twojGit&gt;/nazwa.git\n\n3 steps\n\n# status check\ngit status\n# 1. add all changes\ngit add .\n# 2. commit all changes with message\ngit commit -m \" message \"\n# 3. and\ngit push origin master\nYou can watch Youtube course.\nAll the necessary programs will be delivered in the form of docker containers."
  },
  {
    "objectID": "info.html#start-with-docker",
    "href": "info.html#start-with-docker",
    "title": "All tools",
    "section": "Start with Docker",
    "text": "Start with Docker\nIn order to download the docer software to your system, go to the page.\nIf everything is installed correctly, follow these instructions:\n\nCheck the installed version\n\ndocker --version\n\nDownload and run the image Hello World and\n\ndocker run hello-world\n\nOverview of downloaded images:\n\ndocker image ls\n\ndocker images\n\nOverview of running containers:\n\ndocker ps \n\ndocker ps -all\n\nStopping a running container:\n\ndocker stop &lt;CONTAINER ID&gt;\n\nContainer removal\n\ndocker rm -f &lt;CONTAINER ID&gt;\nI also recommend short intro"
  },
  {
    "objectID": "info.html#docker-as-an-application-continuation-tool",
    "href": "info.html#docker-as-an-application-continuation-tool",
    "title": "All tools",
    "section": "Docker as an application continuation tool",
    "text": "Docker as an application continuation tool\nDocker with jupyter notebook"
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Books and WWW pages",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\n\n\n\n\n\nA. Bellemare Mikrousługi oparte na zdarzeniach. Wykorzystanie danych w organizacji na dużą skalę Zobacz opis lub Kup\nG. Shapira, T. Palino, R. Sivaram, K. Petty Kafka The Definitive Guide. Real-time data and stream processing at scale. Second edition, 2021. O’REILLY.\nJ. Korstanje Machine Learning for Streaming Data with Python, 2022. PACKT\n\n\n\n\n\nA. Geron Uczenie maszynowe z użyciem Scikit-Learn i TensorFlow. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nSebastian Raschka Python. Uczenie maszynowe. Wydanie II. Zobacz opis lub Kup książkę.\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocą pakietów Pandas i NumPy oraz środowiska IPython. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieć. Algorytmy przyszłości. Wydanie II (ebook) Zobacz opis lub Kup książkę, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup książkę, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup książkę.\nR. Schutt, C. O’Neil Badanie danych. Raport z pierwszej lini działań. Zobacz opis lub Kup książkę.\nT. Segaran Nowe usługi 2.0. Przewodnik po analizie zbiorów danych Zobacz opis lub Kup książkę, Kup e-book\nT. Morzy Eksploracja Danych. Metody i algorytmy, PWN, 2013.\nKrzyśko, Wołyński, Górecki, Skorzybut, Systemy uczące się . WNT, 2008\n\n\n\n\n\nF. Chollet Deep Learning. Praca z językiem Python i biblioteką Keras. Zobacz opis lub Kup książkę, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie głębokie z językiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup książkę, Kup e-book\nS. Weidman Uczenie głębokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup książkę, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyć komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup książkę, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programistów. Budowanie aplikacji AI za pomocą fastai i PyTorch Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nSpark. Zaawansowana analiza danych (ebook) Zobacz opis lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\nJ. Quddus Machine Learning with Apache Spark Quick Start Guide (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind Zrozumieć programowanie Zobacz opis lub Kup książkę, Kup e-book\nA. Allain C++. Przewodnik dla początkujących Zobacz opis lub Kup książkę, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdrażanie aplikacji Zobacz opis lub Kup książkę, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zadań z pythonem. Zobacz opis lub Kup książkę, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzędzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuka tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage.\nL. Suskind, Mechanika kwantowa, teoretyczne minimum, 2014, Prószyński i s-ka"
  },
  {
    "objectID": "books.html#books",
    "href": "books.html#books",
    "title": "Books and WWW pages",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\n\n\n\n\n\nA. Bellemare Mikrousługi oparte na zdarzeniach. Wykorzystanie danych w organizacji na dużą skalę Zobacz opis lub Kup\nG. Shapira, T. Palino, R. Sivaram, K. Petty Kafka The Definitive Guide. Real-time data and stream processing at scale. Second edition, 2021. O’REILLY.\nJ. Korstanje Machine Learning for Streaming Data with Python, 2022. PACKT\n\n\n\n\n\nA. Geron Uczenie maszynowe z użyciem Scikit-Learn i TensorFlow. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nSebastian Raschka Python. Uczenie maszynowe. Wydanie II. Zobacz opis lub Kup książkę.\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocą pakietów Pandas i NumPy oraz środowiska IPython. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieć. Algorytmy przyszłości. Wydanie II (ebook) Zobacz opis lub Kup książkę, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup książkę, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup książkę.\nR. Schutt, C. O’Neil Badanie danych. Raport z pierwszej lini działań. Zobacz opis lub Kup książkę.\nT. Segaran Nowe usługi 2.0. Przewodnik po analizie zbiorów danych Zobacz opis lub Kup książkę, Kup e-book\nT. Morzy Eksploracja Danych. Metody i algorytmy, PWN, 2013.\nKrzyśko, Wołyński, Górecki, Skorzybut, Systemy uczące się . WNT, 2008\n\n\n\n\n\nF. Chollet Deep Learning. Praca z językiem Python i biblioteką Keras. Zobacz opis lub Kup książkę, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie głębokie z językiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup książkę, Kup e-book\nS. Weidman Uczenie głębokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup książkę, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyć komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup książkę, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programistów. Budowanie aplikacji AI za pomocą fastai i PyTorch Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nSpark. Zaawansowana analiza danych (ebook) Zobacz opis lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\nJ. Quddus Machine Learning with Apache Spark Quick Start Guide (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind Zrozumieć programowanie Zobacz opis lub Kup książkę, Kup e-book\nA. Allain C++. Przewodnik dla początkujących Zobacz opis lub Kup książkę, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdrażanie aplikacji Zobacz opis lub Kup książkę, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zadań z pythonem. Zobacz opis lub Kup książkę, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzędzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuka tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage.\nL. Suskind, Mechanika kwantowa, teoretyczne minimum, 2014, Prószyński i s-ka"
  },
  {
    "objectID": "books.html#www-pages",
    "href": "books.html#www-pages",
    "title": "Books and WWW pages",
    "section": "WWW Pages",
    "text": "WWW Pages\n\nSoftware\n\nGithub\nGit-instrukcja\nwww.python.org\nPyPI python libraries\nAnaconda\nDocker\n\n\n\nPython libraries for data analysis\n\nNumPy\nSciPy\nPandas\nScikit-learn\nJupyter\nMatplotlib\nBeautiful Soup\nTheano\nKeras\nTensorFlow\nVirtual ENV\n\n\n\nText editors\n\nNotepad++\nSublime Text\nVisual Studio Code\n\n\n\nMarkdown\n\nMD\n\n\n\nJupyter notebook\n\nGaleria ciekawych notatników\nIntro\nKernels\nBringing the best out of jupyter for data science\nJupyter extensions\nI don’t like notebooks\nJupyter lab\nSpeed up jupyter notebook\n\n\n\nETL\n\ndata cookbook\n\n\n\nDatasets\n\nInternet Archive\nReddit\nKDnuggets\nKaggle\nList of datasets for machine learning research\nUCI Machine Learning Repo\nPublic API\nGoogle Datatset Search\n\n\n\nPython\n\nChris Albon Technical Notes on Using Data Science & AI\n40+ Python Statistics For Data Science Resources\nPractical Business Python\n\n\n\nML course\n\nKurs Machine Learning - Andrew Ng, Stanford"
  },
  {
    "objectID": "sylabus.html",
    "href": "sylabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Real Time Analytics\nSGH Warsaw School of Economics\nECTS: 3\nLanguage: EN\nlevel: medium\nday of week: Monday/Tuesday\nTeacher: Sebastian Zając, sebastian.zajac@sgh.waw.pl\nWebsite: http://sebkaz-teaching.github.io/EN"
  },
  {
    "objectID": "sylabus.html#description",
    "href": "sylabus.html#description",
    "title": "Syllabus",
    "section": "Description",
    "text": "Description\nMaking the right decisions based on data and their analysis in business is a process and daily. Modern methods of modeling by machine learning (ML), artificial intelligence (AI), or deep learning not only allow better understanding of business, but also support making key decisions for it. The development of technology and increasingly new business concepts of working directly with the client require not only correct but also fast decisions. The classes offered are designed to provide students with experience and comprehensive theoretical knowledge in the field of real-time data processing and analysis, and to present the latest technologies (free and commercial) for the processing of structured data (originating e.g. from data warehouses) and unstructured (e.g. images, sound, video streaming) in on-line mode. The course will present the so called lambda and kappa structures for data processing into data lake along with a discussion of the problems and difficulties encountered in implementing real-time modeling for large amounts of data. Theoretical knowledge will be gained (apart from the lecture part) through the implementation of test cases in tools such as Apache Spark, Nifi, Microsoft Azure and SAS. During laboratory classes student will benefit from fully understand the latest information technologies related to real-time data processing."
  },
  {
    "objectID": "sylabus.html#list-of-topics",
    "href": "sylabus.html#list-of-topics",
    "title": "Syllabus",
    "section": "List of Topics",
    "text": "List of Topics\n\nModelling, learning and prediction in batch mode (offline learning) and incremental (online learning) modes. Problems of incremental machine learning.\nData processing models in Big Data. From flat files to Data Lake. Real-time data myth and facts\nNRT systems (near real-time systems), data acquisition, streaming and analytics.\nAlgorithms for estimating model parameters in incremental mode. Stochastic Gradient Descent.\nLambda and Kappa architecture. Designing IT architecture for real-time data processing.\nPreparation of the micro-service with the ML model for prediction use.\nStructured and unstructured data. Relational databases and NoSQL databases.\nAggregations and reporting in NoSQL databases (on the example of the MongoDB or Cassandra)\nBasic of object-oriented programming in Python in linear and logistic regression, neural network analysis using the sklearn, TensorFlow and Keras.\nIT architecture of Big Data processing. Preparation of a virtual env for Apache Spark."
  },
  {
    "objectID": "sylabus.html#conditions-for-passing",
    "href": "sylabus.html#conditions-for-passing",
    "title": "Syllabus",
    "section": "Conditions for passing",
    "text": "Conditions for passing\n\ntest 30%\npractical test 30% (IF)\ngroup project 40% (70%)"
  },
  {
    "objectID": "sylabus.html#books",
    "href": "sylabus.html#books",
    "title": "Syllabus",
    "section": "Books",
    "text": "Books\n\nS. Zajac, “Modelowanie dla biznesu, Analityka w czasie rzeczywistym - narzędzia informatyczne i biznesowe”. SGH (2022)\nFrątczak E., red. “Modelowanie dla biznesu, Regresja logistyczna, Regresja Poissona, Survival Data Mining, CRM, Credit Scoring”. SGH, Warszawa 2019.\nFrątczak E., red., “Zaawansowane metody analiz statystycznych”, Oficyna Wydawnicza SGH, Warszawa 2012.\nIndest A., Wild Knowledge. Outthik the Revolution. LID publishing.com 2017.\nReal Time Analytic. “The Key to Unlocking Customer Insights & Driving the Customer Experience”. Harvard Business Review Analytics Series, Harvard Business School Publishing, 2018.\nSvolba G., “Applying Data Science. Business Case Studies Using SAS”. SAS Institute Inc., Cary NC, USA, 2017.\nEllis B. “Real-Time Analytics Techniques to Analyze and Visualize Streaming data.” , Wiley, 2014\nFamiliar B., Barnes J. “Business in Real-Time Using Azure IoT and Cortana Intelligence Suite” Apress, 2017"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Info",
    "section": "",
    "text": "Code: 222891-D\nSemester: 2024/2025 University: SGH Warsaw School of Economics\nBasic course information can be found in the sylabus.\nRecommended reading is available in the books section."
  },
  {
    "objectID": "index.html#real-time-analytics",
    "href": "index.html#real-time-analytics",
    "title": "Info",
    "section": "",
    "text": "Code: 222891-D\nSemester: 2024/2025 University: SGH Warsaw School of Economics\nBasic course information can be found in the sylabus.\nRecommended reading is available in the books section."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Info",
    "section": "Schedule",
    "text": "Schedule\n\nLectures\nThe lectures are conducted in a hybrid mode. Attendance is optional, and in-person sessions take place in Aula VI, Building G.\n\n\n18-02-2025 (Tuesday) 11:40-13:20 - Lecture 1\n\n\n25-02-2025 (Tuesday) 11:40-13:20 - Lecture 2\n\n\n04-03-2025 (Tuesday) 11:40-13:20 - Online Lecture 3\n\n11-03-2025 (Tuesday) 11:40-13:20 - Lecture 4\n18-03-2025 (Tuesday) 11:40-13:20 - Lecture 5\n\nThe lecture 5 concludes with a TEST.\nFormat: 20 questions | 30 minutes Platform: MS Teams\n\n\nTEST\nLectures will end with a test (last class). Positive evaluation of the test (above 13 points) entitles you to carry out the exercises.\nAfter the exercises, homework will be carried out via the MS teams’ platform. Passing all exercises and tasks entitles you to complete the project.\nThe project should be carried out in groups of no more than 5 people.\nProject requirements:\n\nThe project should present a BUSINESS PROBLEM that can be implemented using the information provided online. (This does not mean that you cannot use batch processing, e.g. to generate a model).\nData should be sent to Apache Kafka and further processed and analyzed from there.\nThe programming language is free - applies to each component of the project.\nBI tools can be used\nData sources can be a table, artificially generated data, IoT, etc."
  },
  {
    "objectID": "index.html#technology",
    "href": "index.html#technology",
    "title": "Info",
    "section": "Technology",
    "text": "Technology\nParticipating in the classes, you must know and at least use the following information technologies:\n\nGIT\nPython, Jupyter notebook, Jupyter lab, Colab\nDocker\nApache Spark, Apache Flink, Apache Kafka, Apache Beam\nDatabricks Community edition Web page."
  },
  {
    "objectID": "lecture3.html",
    "href": "lecture3.html",
    "title": "Lecture 3",
    "section": "",
    "text": "⏳ Duration: 1.5h\n🎯 Lecture Goal\nIntroducing students to the fundamentals of real-time analytics."
  },
  {
    "objectID": "lecture3.html#definitions",
    "href": "lecture3.html#definitions",
    "title": "Lecture 3",
    "section": "Definitions",
    "text": "Definitions\n\nLearn more about streaming data\n\nDefinition 1 – Event: Any observable occurrence at a specific point in time. In data streams, events are immutable records that represent a specific action.\nDefinition 2 – In the context of data, an event is encoded as JSON, XML, CSV, or in a binary format.\nDefinition 3 – Continuous Event Stream: An ongoing sequence of events that happen over time, such as logs from servers, sensor readings, or user activity.\nDefinition 4 – Data Stream: A continuous flow of data from dynamic sources like sensor readings or log entries. These streams can be generated in real-time or derived from static sources like databases or files.\nA business is an organization that generates and responds to a continuous stream of events.\n\n\n\nStream Analytics\nStream Analytics (also called Event Stream Processing) refers to processing large amounts of data as they are generated, in real-time.\nRegardless of the technology used, all data exists as a continuous stream of events, including: - User actions on websites,\n- System logs,\n- Sensor measurements."
  },
  {
    "objectID": "lecture3.html#time-in-real-time-data-analysis",
    "href": "lecture3.html#time-in-real-time-data-analysis",
    "title": "Lecture 3",
    "section": "Time in Real-Time Data Analysis",
    "text": "Time in Real-Time Data Analysis\nIn batch processing, we analyze historical data, and the timing of the process is unrelated to when the events actually occurred.\nIn stream processing, there are two key concepts of time: 1. Event Time – the actual moment the event occurred. 2. Processing Time – the moment the system processes the event.\n\nIdeal Data Processing\nIn an ideal scenario, processing happens immediately after the event occurs:\n\n\n\nReal Data Processing\nIn reality, data processing always involves some delay, visible as points below the ideal processing line (below the diagonal in the chart):\n\nIn stream processing applications, the difference between event time and processing time is crucial. Common causes of delays include:\n\nData transmission over the network,\nLack of communication between the device and the network.\n\nAn example of this is tracking the location of a car via a GPS application – passing through a tunnel might cause temporary data loss.\n\n\nHandling Delays in Stream Processing\nDelays in event processing can be managed in two main ways: 1. Monitoring the number of missed events and triggering an alarm if the number of rejected events exceeds a threshold. 2. Applying correction using watermarking, which is an additional mechanism that accounts for delayed events.\nThe process of real-time event processing can be represented as a step function:\n\nNot all events contribute to the analysis – some may be discarded due to excessive delays.\nBy using watermarking, additional time is allowed for the appearance of delayed events. This process includes all events above the dashed line. However, there might still be cases where some points are skipped.\n\nThe situations illustrated in the charts explicitly indicate why the concept of time is a critical factor and needs to be clearly defined at the business requirements level. Assigning timestamps to data (events) is a complex task."
  },
  {
    "objectID": "lecture3.html#time-windows-in-stream-analytics",
    "href": "lecture3.html#time-windows-in-stream-analytics",
    "title": "Lecture 3",
    "section": "Time Windows in Stream Analytics",
    "text": "Time Windows in Stream Analytics\nIn stream processing, time windows allow grouping data into time-limited segments, enabling event analysis within specific time intervals. Depending on the use case, various types of windows are applied, tailored to the characteristics of the data and analytical requirements.\n\n\n1. Tumbling Window\nA tumbling window is a fixed-length window that does not overlap – each event belongs to only one window.\n✅ Characteristics:\n- Fixed window length\n- No overlap between windows\n- Ideal for dividing data into equal time segments\n📌 Example: Analyzing the number of orders in an online store every 5 minutes.\n\n\n\n\n2. Sliding Window\nA sliding window includes all events occurring within a specific time interval, where the window slides continuously.\n✅ Characteristics:\n- Each event can belong to multiple windows\n- The window shifts by a specified interval\n- Useful for detecting trends and anomalies\n📌 Example: Tracking the average temperature over the last 10 minutes, updated every 2 minutes.\n\n\n\n\n3. Hopping Window\nA hopping window is similar to a tumbling window, but it allows overlapping windows, meaning one event can belong to multiple windows. It is used to smooth data.\n✅ Characteristics:\n- Fixed window length\n- Overlapping windows\n- Useful for noise reduction in data\n📌 Example: Analyzing the number of website visitors every 10 minutes, but updated every 5 minutes to better capture trends.\n\n\n\n\n4. Session Window\nA session window groups events based on activity periods and closes after a specified period of inactivity.\n✅ Characteristics:\n- Dynamic window length\n- Defined by user activity\n- Used in user session analysis\n📌 Example: Analyzing user sessions on a website – the session lasts as long as the user is active, but ends after 15 minutes of inactivity.\n\n\n\nSummary\nDifferent types of time windows are applied depending on the data’s characteristics and the analysis objectives. Choosing the right window impacts the accuracy of results and the efficiency of the analytical system.\n\n\n\n\n\n\n\n\nWindow Type\nCharacteristics\nUse Cases\n\n\n\n\nTumbling\nFixed length, no overlap\nPeriodic reports\n\n\nSliding\nFixed length, overlapping windows\nTrend detection, anomaly detection\n\n\nHopping\nFixed length, partial overlap\nData smoothing\n\n\nSession\nDynamic length, activity-dependent\nUser session analysis\n\n\n\nEach window type has its unique use cases and helps with better interpretation of streaming data. The choice of method depends on business needs and the nature of the analyzed data.\nIn stream data analysis, interpreting time is a complex issue due to: 1. Different systems having different clocks, leading to inconsistencies, 2. Data arriving with delays, requiring watermarking and time window techniques, 3. Different approaches to event time vs. processing time impacting result accuracy."
  },
  {
    "objectID": "lecture5.html",
    "href": "lecture5.html",
    "title": "Lecture 5",
    "section": "",
    "text": "Before designing a solution to a business problem, it is essential to consider the complexity of the issue.\n\n\n\nAlgorithms Processing Large Amounts of Data\n\nProcessing vast datasets requires an appropriate approach to organizing and analyzing them. When the amount of data exceeds the available memory of a computing unit, iterative processing methods are often used.\n🔹 Example: Recommendation Systems in E-commerce (e.g., Amazon, Netflix)\n\nAnalyzes large datasets about users, their purchase history, and viewed content.\nProcesses data iteratively (e.g., stream processing in Apache Spark).\nUses collaborative filtering or graph-based algorithms to predict user preferences.\n\n🔹 Other Applications:\n\nReal-time server log analysis (e.g., DDoS attack detection).\nIoT network monitoring (e.g., sensor data analysis in smart cities).\n\n\nAlgorithms Performing Intensive Computations\n\nThese require significant computing power but typically do not operate on large datasets. An example is an algorithm searching for large prime numbers. Parallel computation techniques are often used to optimize performance.\n🔹 Example: Cryptography and Finding Large Prime Numbers (e.g., RSA)\n\nGenerates large prime numbers essential for RSA encryption.\nRequires intensive computations but does not operate on vast datasets.\nOften employs parallel methods, such as the Miller-Rabin probabilistic algorithm for primality testing.\n\n🔹 Other Applications:\n\nPhysical simulations (e.g., weather forecasting, climate models).\nOptimization algorithms (e.g., solving the traveling salesman problem).\n\n\nAlgorithms Processing Large Data and Performing Intensive Computations\n\nThese combine the requirements of the previous types, demanding both high computational power and handling of large datasets. An example is sentiment analysis in live video streams.\n🔹 Example: Sentiment Analysis in Live Video Streams (e.g., YouTube, Twitch)\n\nAnalyzes both text (chat) and video/audio in real time.\nRequires both significant computational resources (NLP and CV processing) and large-scale data handling.\nUses Transformer models (e.g., BERT) for text analysis and CNN/RNN for image and audio processing.\n\n🔹 Other Applications:\n\nAutonomous vehicles (real-time image analysis and decision-making).\nAnomaly detection in massive financial datasets (e.g., fraud detection in banking).\n\n\n\n\nTo determine the dimensionality of a problem’s data, it is not enough to consider just the amount of storage required. Three main aspects are crucial:\n\nInput Size – Expected volume of data to be processed.\nGrowth Rate – The rate at which new data is generated during algorithm execution.\nStructural Diversity – The types of data that the algorithm must handle.\n\n\n\n\nThis concerns processing resources and computational power. For example, deep learning (DL) algorithms require significant computational power, making it necessary to provide a parallelized architecture using GPUs or TPUs, significantly speeding up computations.\n\n\n\nIn many cases, modeling is used in critical situations, such as in software for administering medications. In such cases, explaining the reasons behind each algorithmic decision is crucial to ensure that the outcomes are error-free and unbiased.\nThe ability of an algorithm to indicate the mechanisms generating its results is called explainability. Ethical analysis is a standard part of the algorithm validation process.\nAchieving high explainability is particularly challenging for machine learning (ML) and deep learning (DL) algorithms. For instance, banks using algorithms for credit decision-making must ensure transparency and justify their decisions.\nOne method for improving algorithm explainability is LIME (Local Interpretable Model-Agnostic Explanations), published in 2016. This method introduces small changes to input data and analyzes their impact on the output, allowing the identification of local decision-making rules within the model.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lime.lime_tabular import LimeTabularExplainer\n\n# Wczytanie danych Iris\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Podział na zbiór treningowy i testowy\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Trenowanie modelu\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Tworzenie obiektu LIME do interpretacji modelu\nexplainer = LimeTabularExplainer(X_train, feature_names=iris.feature_names, class_names=iris.target_names, discretize_continuous=True)\n\n# Interpretacja losowego przykładu ze zbioru testowego\ni = np.random.randint(0, len(X_test))  # Wybór losowego przykładu\nexp = explainer.explain_instance(X_test[i], model.predict_proba)\n\n# Wyświetlenie interpretacji\nexp.show_in_notebook()\n\n\n\n        \n        \n        \n        \n        \n        \n\n\nHow Does This Code Work?\n\nLoading Data and Training the Model\n\n\nUses the Iris dataset, containing 150 examples of flowers from three species:\nSetosa\nVersicolor\nVirginica\nThe RandomForestClassifier model is trained on this data.\n\n\nCreating an Interpretable Model Using LIME\n\n\nLIME generates local explanations, interpreting the model for individual predictions.\nA random test example is selected.\n\n\nExploring the Outcome for a Single Example\n\n\nLIME slightly modifies input values and observes how the prediction changes.\nIt creates a “local” linear model that shows which features had the most influence on the decision.\n\nLet’s assume our model selects a sample flower and classifies it as Virginica.\nInterpretation of Results:\n\nKey Features Affecting Model Decision:\n\n\nPetal length: The most significant factor (e.g., a longer petal suggests Virginica).\nPetal width: Also a crucial factor (e.g., above a certain threshold indicates Virginica).\nSepal length: A less significant but still relevant factor.\nSepal width: Usually the least important feature.\n\n\nVisualization of Results:\n\n\nLIME generates a bar chart showing the impact of each feature on classification.\nThe chart highlights which features increased or decreased the probability of a specific classification.\n\n\nWhat Does This Mean?\n\n\nIf the model predicts Virginica with high probability, key features (e.g., long petals) strongly indicate this species.\nIf the features had mixed influences, it suggests the model had difficulty classifying the instance (e.g., petal width was ambiguous)."
  },
  {
    "objectID": "lecture5.html#algorithms",
    "href": "lecture5.html#algorithms",
    "title": "Lecture 5",
    "section": "",
    "text": "Before designing a solution to a business problem, it is essential to consider the complexity of the issue.\n\n\n\nAlgorithms Processing Large Amounts of Data\n\nProcessing vast datasets requires an appropriate approach to organizing and analyzing them. When the amount of data exceeds the available memory of a computing unit, iterative processing methods are often used.\n🔹 Example: Recommendation Systems in E-commerce (e.g., Amazon, Netflix)\n\nAnalyzes large datasets about users, their purchase history, and viewed content.\nProcesses data iteratively (e.g., stream processing in Apache Spark).\nUses collaborative filtering or graph-based algorithms to predict user preferences.\n\n🔹 Other Applications:\n\nReal-time server log analysis (e.g., DDoS attack detection).\nIoT network monitoring (e.g., sensor data analysis in smart cities).\n\n\nAlgorithms Performing Intensive Computations\n\nThese require significant computing power but typically do not operate on large datasets. An example is an algorithm searching for large prime numbers. Parallel computation techniques are often used to optimize performance.\n🔹 Example: Cryptography and Finding Large Prime Numbers (e.g., RSA)\n\nGenerates large prime numbers essential for RSA encryption.\nRequires intensive computations but does not operate on vast datasets.\nOften employs parallel methods, such as the Miller-Rabin probabilistic algorithm for primality testing.\n\n🔹 Other Applications:\n\nPhysical simulations (e.g., weather forecasting, climate models).\nOptimization algorithms (e.g., solving the traveling salesman problem).\n\n\nAlgorithms Processing Large Data and Performing Intensive Computations\n\nThese combine the requirements of the previous types, demanding both high computational power and handling of large datasets. An example is sentiment analysis in live video streams.\n🔹 Example: Sentiment Analysis in Live Video Streams (e.g., YouTube, Twitch)\n\nAnalyzes both text (chat) and video/audio in real time.\nRequires both significant computational resources (NLP and CV processing) and large-scale data handling.\nUses Transformer models (e.g., BERT) for text analysis and CNN/RNN for image and audio processing.\n\n🔹 Other Applications:\n\nAutonomous vehicles (real-time image analysis and decision-making).\nAnomaly detection in massive financial datasets (e.g., fraud detection in banking).\n\n\n\n\nTo determine the dimensionality of a problem’s data, it is not enough to consider just the amount of storage required. Three main aspects are crucial:\n\nInput Size – Expected volume of data to be processed.\nGrowth Rate – The rate at which new data is generated during algorithm execution.\nStructural Diversity – The types of data that the algorithm must handle.\n\n\n\n\nThis concerns processing resources and computational power. For example, deep learning (DL) algorithms require significant computational power, making it necessary to provide a parallelized architecture using GPUs or TPUs, significantly speeding up computations.\n\n\n\nIn many cases, modeling is used in critical situations, such as in software for administering medications. In such cases, explaining the reasons behind each algorithmic decision is crucial to ensure that the outcomes are error-free and unbiased.\nThe ability of an algorithm to indicate the mechanisms generating its results is called explainability. Ethical analysis is a standard part of the algorithm validation process.\nAchieving high explainability is particularly challenging for machine learning (ML) and deep learning (DL) algorithms. For instance, banks using algorithms for credit decision-making must ensure transparency and justify their decisions.\nOne method for improving algorithm explainability is LIME (Local Interpretable Model-Agnostic Explanations), published in 2016. This method introduces small changes to input data and analyzes their impact on the output, allowing the identification of local decision-making rules within the model.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lime.lime_tabular import LimeTabularExplainer\n\n# Wczytanie danych Iris\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Podział na zbiór treningowy i testowy\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Trenowanie modelu\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Tworzenie obiektu LIME do interpretacji modelu\nexplainer = LimeTabularExplainer(X_train, feature_names=iris.feature_names, class_names=iris.target_names, discretize_continuous=True)\n\n# Interpretacja losowego przykładu ze zbioru testowego\ni = np.random.randint(0, len(X_test))  # Wybór losowego przykładu\nexp = explainer.explain_instance(X_test[i], model.predict_proba)\n\n# Wyświetlenie interpretacji\nexp.show_in_notebook()\n\n\n\n        \n        \n        \n        \n        \n        \n\n\nHow Does This Code Work?\n\nLoading Data and Training the Model\n\n\nUses the Iris dataset, containing 150 examples of flowers from three species:\nSetosa\nVersicolor\nVirginica\nThe RandomForestClassifier model is trained on this data.\n\n\nCreating an Interpretable Model Using LIME\n\n\nLIME generates local explanations, interpreting the model for individual predictions.\nA random test example is selected.\n\n\nExploring the Outcome for a Single Example\n\n\nLIME slightly modifies input values and observes how the prediction changes.\nIt creates a “local” linear model that shows which features had the most influence on the decision.\n\nLet’s assume our model selects a sample flower and classifies it as Virginica.\nInterpretation of Results:\n\nKey Features Affecting Model Decision:\n\n\nPetal length: The most significant factor (e.g., a longer petal suggests Virginica).\nPetal width: Also a crucial factor (e.g., above a certain threshold indicates Virginica).\nSepal length: A less significant but still relevant factor.\nSepal width: Usually the least important feature.\n\n\nVisualization of Results:\n\n\nLIME generates a bar chart showing the impact of each feature on classification.\nThe chart highlights which features increased or decreased the probability of a specific classification.\n\n\nWhat Does This Mean?\n\n\nIf the model predicts Virginica with high probability, key features (e.g., long petals) strongly indicate this species.\nIf the features had mixed influences, it suggests the model had difficulty classifying the instance (e.g., petal width was ambiguous)."
  },
  {
    "objectID": "lecture5.html#anomaly-detection",
    "href": "lecture5.html#anomaly-detection",
    "title": "Lecture 5",
    "section": "Anomaly detection",
    "text": "Anomaly detection\nAn outlier is an observation (a row in a dataset) that is significantly different from the other elements in the sample. This means that the relationship between independent and dependent variables for this observation may differ from other cases.\nFor single variables, outliers can be identified using a box plot, which is based on quartiles:\n\nFirst quartile and third quartile define the edges of the box,\nSecond quartile (median) is marked inside the box,\n\nOutliers satisfy the condition:\n[ x_{out} &lt; Q_1 - 1.5 IQR x_{out} &gt; Q_3 + 1.5 IQR ]\nWhere: [ IQR = Q_3 - Q_1 ]\nAn example of an outlier could be a Formula 1 car – in terms of speed, it is an anomaly among regular cars.\n\nUse of Anomaly Detection\nAnomaly detection has a wide range of applications, such as:\n\nFinance – detecting fraudulent transactions in banking data analysis,\nCybersecurity – identifying intruders in a network based on user behavior,\nMedicine – monitoring health parameters and detecting abnormalities,\nIndustry – detecting faulty components through image analysis.\n\n\n\nAnomaly Detection Methods\n\n1. Supervised Methods (supervised learning)\nUsed when labeled data is available (e.g., fraud cases in transactions). - Neural networks, - K-Nearest Neighbors algorithm (KNN), - Bayesian networks.\n\n\n2. Unsupervised Methods (unsupervised learning)\nAssumes that most data is correct, and anomalies are a small percentage of cases. - K-Means clustering, - Autoencoders in neural networks, - Statistical tests.\n\n\n\nClassical Method – Detection Based on Probability\nTo determine whether a given observation is an anomaly, we can use the probability \\(p(x)\\): - If \\(p(x) &lt; \\epsilon\\), we consider the value as an outlier. - In practice, we assume the data follows a normal distribution \\(N(\\mu, \\sigma)\\). - We estimate the parameters \\(\\mu\\) (mean) and \\(\\sigma^2\\) (variance) based on a sample. - Then, for each value, we calculate the probability of its occurrence and compare it with .\nExample: Salary Analysis in a Company\nWe detect whether there are individuals in the company whose salaries significantly deviate from the average.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsalaries = [40, 42, 45, 47, 50, 55, 60, 70, 90, 150]  # 150 to outlier\n\nQ1 = np.percentile(salaries, 25)\nQ3 = np.percentile(salaries, 75)\nIQR = Q3 - Q1\n\noutlier_threshold_low = Q1 - 1.5 * IQR\noutlier_threshold_high = Q3 + 1.5 * IQR\n\noutliers = [x for x in salaries if x &lt; outlier_threshold_low or x &gt; outlier_threshold_high]\n\nprint(f\"Outliers: {outliers}\")\n\nsns.boxplot(salaries)\nplt.title(\"Box plot\")\nplt.show()\n\n\nOutliers: [150]\n\n\n\n\n\n\n\n\n\nResult: The box plot shows that 150k is an anomaly.\n\n\nIsolation Forest – Anomaly Detection Using Isolation Forest\nIsolation Forest is an algorithm based on decision trees, proposed by Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou in 2008. It identifies anomalies by isolating outliers during the data partitioning process: - Randomly selects a feature and a split value, - Outliers are isolated more quickly (closer to the root of the tree), - The result is aggregated based on multiple trees.\nIts advantages include low computational requirements and effectiveness in analyzing high-dimensional data.\nAnomaly Detection Methods in sklearn\nExample: Detecting Credit Card Fraud\nA bank analyzes credit card transactions and detects those that may be unauthorized.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\n\n# Przykładowe dane transakcji (kwota, liczba transakcji w tygodniu)\ndata = np.array([\n    [100, 5], [120, 6], [130, 5], [110, 4], [5000, 1],  # Outlier (duża kwota, rzadkość)\n    [125, 5], [115, 5], [140, 7], [135, 6], [145, 5]\n])\n\n# Model Isolation Forest\nclf = IsolationForest(contamination=0.1)  # 10% transakcji uznajemy za anomalie\nclf.fit(data)\n\n# Predykcja (1 = normalna transakcja, -1 = oszustwo)\npredictions = clf.predict(data)\ndf = pd.DataFrame(data, columns=[\"Kwota\", \"Liczba transakcji\"])\ndf[\"Anomalia\"] = predictions\n\nprint(df)\n\n\n   Kwota  Liczba transakcji  Anomalia\n0    100                  5         1\n1    120                  6         1\n2    130                  5         1\n3    110                  4         1\n4   5000                  1        -1\n5    125                  5         1\n6    115                  5         1\n7    140                  7         1\n8    135                  6         1\n9    145                  5         1\n\n\nResult: A transaction of 5000 PLN will be detected as an anomaly."
  }
]