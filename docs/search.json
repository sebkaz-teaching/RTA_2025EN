[
  {
    "objectID": "kafka_codes/kafka2.html",
    "href": "kafka_codes/kafka2.html",
    "title": "Kafka Producer",
    "section": "",
    "text": "Navigate to the home directory and list all elements. Check if the kafka directory is present in the list.\ncd ~\nls -la",
    "crumbs": [
      "Home",
      "Labs",
      "Apache Kafka",
      "Kafka Producer"
    ]
  },
  {
    "objectID": "kafka_codes/kafka2.html#checking-directories-and-kafka-availability",
    "href": "kafka_codes/kafka2.html#checking-directories-and-kafka-availability",
    "title": "Kafka Producer",
    "section": "",
    "text": "Navigate to the home directory and list all elements. Check if the kafka directory is present in the list.\ncd ~\nls -la",
    "crumbs": [
      "Home",
      "Labs",
      "Apache Kafka",
      "Kafka Producer"
    ]
  },
  {
    "objectID": "kafka_codes/kafka2.html#checking-the-list-of-topics",
    "href": "kafka_codes/kafka2.html#checking-the-list-of-topics",
    "title": "Kafka Producer",
    "section": "2Ô∏è‚É£ Checking the List of Topics",
    "text": "2Ô∏è‚É£ Checking the List of Topics\nRun the following command to check the list of topics on the Kafka server:\nkafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092",
    "crumbs": [
      "Home",
      "Labs",
      "Apache Kafka",
      "Kafka Producer"
    ]
  },
  {
    "objectID": "kafka_codes/kafka2.html#creating-a-new-topic",
    "href": "kafka_codes/kafka2.html#creating-a-new-topic",
    "title": "Kafka Producer",
    "section": "3Ô∏è‚É£ Creating a New Topic",
    "text": "3Ô∏è‚É£ Creating a New Topic\nCreate a topic named streaming:\nkafka/bin/kafka-topics.sh --bootstrap-server broker:9092 --create --topic streaming\nCheck the list of topics again to ensure that streaming has been added:\nkafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092 | grep streaming",
    "crumbs": [
      "Home",
      "Labs",
      "Apache Kafka",
      "Kafka Producer"
    ]
  },
  {
    "objectID": "kafka_codes/kafka2.html#creating-a-new-python-producer-script",
    "href": "kafka_codes/kafka2.html#creating-a-new-python-producer-script",
    "title": "Kafka Producer",
    "section": "4Ô∏è‚É£ Creating a New Python Producer script",
    "text": "4Ô∏è‚É£ Creating a New Python Producer script\nIn a new terminal, create a file named stream.py and paste the following code:\n\n%%file stream.py\nimport json\nimport random\nimport sys\nfrom datetime import datetime, timedelta\nfrom time import sleep\n\nfrom kafka import KafkaProducer\n\nSERVER = \"broker:9092\"\nTOPIC = \"streaming\"\n\nif __name__ == \"__main__\":\n    \n    \n    producer = KafkaProducer(\n        bootstrap_servers=[SERVER],\n        value_serializer=lambda x: json.dumps(x).encode(\"utf-8\")\n    )\n    \n    try:\n        while True:\n            \n            message = {\n                # Your Code Here\n            }\n            producer.send(TOPIC, value=message)\n            sleep(1)\n    except KeyboardInterrupt:\n        producer.close()\n\nWriting stream.py",
    "crumbs": [
      "Home",
      "Labs",
      "Apache Kafka",
      "Kafka Producer"
    ]
  },
  {
    "objectID": "kafka_codes/kafka2.html#running-a-console-consumer",
    "href": "kafka_codes/kafka2.html#running-a-console-consumer",
    "title": "Kafka Producer",
    "section": "Running a Console Consumer",
    "text": "Running a Console Consumer\nTo check if message sending works, open another terminal window and start the consumer:\nkafka/bin/kafka-console-consumer.sh --bootstrap-server broker:9092 --topic streaming --from-beginning\nNow, all messages sent by the producer should appear in the consumer console.",
    "crumbs": [
      "Home",
      "Labs",
      "Apache Kafka",
      "Kafka Producer"
    ]
  },
  {
    "objectID": "kafka_codes/kafka2.html#finishing-up",
    "href": "kafka_codes/kafka2.html#finishing-up",
    "title": "Kafka Producer",
    "section": "Finishing Up",
    "text": "Finishing Up\nRemember to run commands from the appropriate directory. When you‚Äôre done with the exercises, use Ctrl+C to stop both the producer and the consumer.\n\nNow you have a basic understanding of how Apache Kafka works üöÄ",
    "crumbs": [
      "Home",
      "Labs",
      "Apache Kafka",
      "Kafka Producer"
    ]
  },
  {
    "objectID": "kafka_codes/kafka3.html",
    "href": "kafka_codes/kafka3.html",
    "title": "Kafka Consumer",
    "section": "",
    "text": "Go to the home directory:\ncd ~\nkafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092\n\n\n\nkafka/bin/kafka-topics.sh --create --topic mytopic --bootstrap-server broker:9092\n\n\n\nkafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092 | grep mytopic\n\n\n\nfrom kafka import KafkaConsumer\nimport json  \n\nSERVER = \"broker:9092\"\nTOPIC  = \"mytopic\"\n\n# Konsumer do pobierania danych z Kafka\nconsumer = KafkaConsumer(\n    TOPIC,\n    bootstrap_servers=SERVER,\n    auto_offset_reset='earliest',\n    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n)\n\n# Pobieranie transakcji w niemal real-time i analiza\nfor message in consumer:\n    transaction = message.value\n    if transaction[\"values\"] &gt; 80:\n        print(f\"üö® BAD TRANSACTION: {transaction}\")",
    "crumbs": [
      "Home",
      "Labs",
      "Apache Kafka",
      "Kafka Consumer"
    ]
  },
  {
    "objectID": "kafka_codes/kafka3.html#check-topic-list",
    "href": "kafka_codes/kafka3.html#check-topic-list",
    "title": "Kafka Consumer",
    "section": "",
    "text": "Go to the home directory:\ncd ~\nkafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092",
    "crumbs": [
      "Home",
      "Labs",
      "Apache Kafka",
      "Kafka Consumer"
    ]
  },
  {
    "objectID": "kafka_codes/kafka3.html#create-new-topis-with-name-mytopic",
    "href": "kafka_codes/kafka3.html#create-new-topis-with-name-mytopic",
    "title": "Kafka Consumer",
    "section": "",
    "text": "kafka/bin/kafka-topics.sh --create --topic mytopic --bootstrap-server broker:9092",
    "crumbs": [
      "Home",
      "Labs",
      "Apache Kafka",
      "Kafka Consumer"
    ]
  },
  {
    "objectID": "kafka_codes/kafka3.html#recheck-topic-list",
    "href": "kafka_codes/kafka3.html#recheck-topic-list",
    "title": "Kafka Consumer",
    "section": "",
    "text": "kafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092 | grep mytopic",
    "crumbs": [
      "Home",
      "Labs",
      "Apache Kafka",
      "Kafka Consumer"
    ]
  },
  {
    "objectID": "kafka_codes/kafka3.html#kafka-producer",
    "href": "kafka_codes/kafka3.html#kafka-producer",
    "title": "Kafka Consumer",
    "section": "",
    "text": "from kafka import KafkaConsumer\nimport json  \n\nSERVER = \"broker:9092\"\nTOPIC  = \"mytopic\"\n\n# Konsumer do pobierania danych z Kafka\nconsumer = KafkaConsumer(\n    TOPIC,\n    bootstrap_servers=SERVER,\n    auto_offset_reset='earliest',\n    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n)\n\n# Pobieranie transakcji w niemal real-time i analiza\nfor message in consumer:\n    transaction = message.value\n    if transaction[\"values\"] &gt; 80:\n        print(f\"üö® BAD TRANSACTION: {transaction}\")",
    "crumbs": [
      "Home",
      "Labs",
      "Apache Kafka",
      "Kafka Consumer"
    ]
  },
  {
    "objectID": "lectures/lecture5.html",
    "href": "lectures/lecture5.html",
    "title": "Lecture 5",
    "section": "",
    "text": "Before designing a solution to a business problem, it is essential to consider the complexity of the issue.\n\n\n\nAlgorithms Processing Large Amounts of Data\n\nProcessing vast datasets requires an appropriate approach to organizing and analyzing them. When the amount of data exceeds the available memory of a computing unit, iterative processing methods are often used.\nüîπ Example: Recommendation Systems in E-commerce (e.g., Amazon, Netflix)\n\nAnalyzes large datasets about users, their purchase history, and viewed content.\nProcesses data iteratively (e.g., stream processing in Apache Spark).\nUses collaborative filtering or graph-based algorithms to predict user preferences.\n\nüîπ Other Applications:\n\nReal-time server log analysis (e.g., DDoS attack detection).\nIoT network monitoring (e.g., sensor data analysis in smart cities).\n\n\nAlgorithms Performing Intensive Computations\n\nThese require significant computing power but typically do not operate on large datasets. An example is an algorithm searching for large prime numbers. Parallel computation techniques are often used to optimize performance.\nüîπ Example: Cryptography and Finding Large Prime Numbers (e.g., RSA)\n\nGenerates large prime numbers essential for RSA encryption.\nRequires intensive computations but does not operate on vast datasets.\nOften employs parallel methods, such as the Miller-Rabin probabilistic algorithm for primality testing.\n\nüîπ Other Applications:\n\nPhysical simulations (e.g., weather forecasting, climate models).\nOptimization algorithms (e.g., solving the traveling salesman problem).\n\n\nAlgorithms Processing Large Data and Performing Intensive Computations\n\nThese combine the requirements of the previous types, demanding both high computational power and handling of large datasets. An example is sentiment analysis in live video streams.\nüîπ Example: Sentiment Analysis in Live Video Streams (e.g., YouTube, Twitch)\n\nAnalyzes both text (chat) and video/audio in real time.\nRequires both significant computational resources (NLP and CV processing) and large-scale data handling.\nUses Transformer models (e.g., BERT) for text analysis and CNN/RNN for image and audio processing.\n\nüîπ Other Applications:\n\nAutonomous vehicles (real-time image analysis and decision-making).\nAnomaly detection in massive financial datasets (e.g., fraud detection in banking).\n\n\n\n\nTo determine the dimensionality of a problem‚Äôs data, it is not enough to consider just the amount of storage required. Three main aspects are crucial:\n\nInput Size ‚Äì Expected volume of data to be processed.\nGrowth Rate ‚Äì The rate at which new data is generated during algorithm execution.\nStructural Diversity ‚Äì The types of data that the algorithm must handle.\n\n\n\n\nThis concerns processing resources and computational power. For example, deep learning (DL) algorithms require significant computational power, making it necessary to provide a parallelized architecture using GPUs or TPUs, significantly speeding up computations.\n\n\n\nIn many cases, modeling is used in critical situations, such as in software for administering medications. In such cases, explaining the reasons behind each algorithmic decision is crucial to ensure that the outcomes are error-free and unbiased.\nThe ability of an algorithm to indicate the mechanisms generating its results is called explainability. Ethical analysis is a standard part of the algorithm validation process.\nAchieving high explainability is particularly challenging for machine learning (ML) and deep learning (DL) algorithms. For instance, banks using algorithms for credit decision-making must ensure transparency and justify their decisions.\nOne method for improving algorithm explainability is LIME (Local Interpretable Model-Agnostic Explanations), published in 2016. This method introduces small changes to input data and analyzes their impact on the output, allowing the identification of local decision-making rules within the model.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lime.lime_tabular import LimeTabularExplainer\n\n# Wczytanie danych Iris\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Podzia≈Ç na zbi√≥r treningowy i testowy\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Trenowanie modelu\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Tworzenie obiektu LIME do interpretacji modelu\nexplainer = LimeTabularExplainer(X_train, feature_names=iris.feature_names, class_names=iris.target_names, discretize_continuous=True)\n\n# Interpretacja losowego przyk≈Çadu ze zbioru testowego\ni = np.random.randint(0, len(X_test))  # Wyb√≥r losowego przyk≈Çadu\nexp = explainer.explain_instance(X_test[i], model.predict_proba)\n\n# Wy≈õwietlenie interpretacji\nexp.show_in_notebook()\n\n\n\n        \n        \n        \n        \n        \n        \n\n\nHow Does This Code Work?\n\nLoading Data and Training the Model\n\n\nUses the Iris dataset, containing 150 examples of flowers from three species:\nSetosa\nVersicolor\nVirginica\nThe RandomForestClassifier model is trained on this data.\n\n\nCreating an Interpretable Model Using LIME\n\n\nLIME generates local explanations, interpreting the model for individual predictions.\nA random test example is selected.\n\n\nExploring the Outcome for a Single Example\n\n\nLIME slightly modifies input values and observes how the prediction changes.\nIt creates a ‚Äúlocal‚Äù linear model that shows which features had the most influence on the decision.\n\nLet‚Äôs assume our model selects a sample flower and classifies it as Virginica.\nInterpretation of Results:\n\nKey Features Affecting Model Decision:\n\n\nPetal length: The most significant factor (e.g., a longer petal suggests Virginica).\nPetal width: Also a crucial factor (e.g., above a certain threshold indicates Virginica).\nSepal length: A less significant but still relevant factor.\nSepal width: Usually the least important feature.\n\n\nVisualization of Results:\n\n\nLIME generates a bar chart showing the impact of each feature on classification.\nThe chart highlights which features increased or decreased the probability of a specific classification.\n\n\nWhat Does This Mean?\n\n\nIf the model predicts Virginica with high probability, key features (e.g., long petals) strongly indicate this species.\nIf the features had mixed influences, it suggests the model had difficulty classifying the instance (e.g., petal width was ambiguous).",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 5"
    ]
  },
  {
    "objectID": "lectures/lecture5.html#algorithms",
    "href": "lectures/lecture5.html#algorithms",
    "title": "Lecture 5",
    "section": "",
    "text": "Before designing a solution to a business problem, it is essential to consider the complexity of the issue.\n\n\n\nAlgorithms Processing Large Amounts of Data\n\nProcessing vast datasets requires an appropriate approach to organizing and analyzing them. When the amount of data exceeds the available memory of a computing unit, iterative processing methods are often used.\nüîπ Example: Recommendation Systems in E-commerce (e.g., Amazon, Netflix)\n\nAnalyzes large datasets about users, their purchase history, and viewed content.\nProcesses data iteratively (e.g., stream processing in Apache Spark).\nUses collaborative filtering or graph-based algorithms to predict user preferences.\n\nüîπ Other Applications:\n\nReal-time server log analysis (e.g., DDoS attack detection).\nIoT network monitoring (e.g., sensor data analysis in smart cities).\n\n\nAlgorithms Performing Intensive Computations\n\nThese require significant computing power but typically do not operate on large datasets. An example is an algorithm searching for large prime numbers. Parallel computation techniques are often used to optimize performance.\nüîπ Example: Cryptography and Finding Large Prime Numbers (e.g., RSA)\n\nGenerates large prime numbers essential for RSA encryption.\nRequires intensive computations but does not operate on vast datasets.\nOften employs parallel methods, such as the Miller-Rabin probabilistic algorithm for primality testing.\n\nüîπ Other Applications:\n\nPhysical simulations (e.g., weather forecasting, climate models).\nOptimization algorithms (e.g., solving the traveling salesman problem).\n\n\nAlgorithms Processing Large Data and Performing Intensive Computations\n\nThese combine the requirements of the previous types, demanding both high computational power and handling of large datasets. An example is sentiment analysis in live video streams.\nüîπ Example: Sentiment Analysis in Live Video Streams (e.g., YouTube, Twitch)\n\nAnalyzes both text (chat) and video/audio in real time.\nRequires both significant computational resources (NLP and CV processing) and large-scale data handling.\nUses Transformer models (e.g., BERT) for text analysis and CNN/RNN for image and audio processing.\n\nüîπ Other Applications:\n\nAutonomous vehicles (real-time image analysis and decision-making).\nAnomaly detection in massive financial datasets (e.g., fraud detection in banking).\n\n\n\n\nTo determine the dimensionality of a problem‚Äôs data, it is not enough to consider just the amount of storage required. Three main aspects are crucial:\n\nInput Size ‚Äì Expected volume of data to be processed.\nGrowth Rate ‚Äì The rate at which new data is generated during algorithm execution.\nStructural Diversity ‚Äì The types of data that the algorithm must handle.\n\n\n\n\nThis concerns processing resources and computational power. For example, deep learning (DL) algorithms require significant computational power, making it necessary to provide a parallelized architecture using GPUs or TPUs, significantly speeding up computations.\n\n\n\nIn many cases, modeling is used in critical situations, such as in software for administering medications. In such cases, explaining the reasons behind each algorithmic decision is crucial to ensure that the outcomes are error-free and unbiased.\nThe ability of an algorithm to indicate the mechanisms generating its results is called explainability. Ethical analysis is a standard part of the algorithm validation process.\nAchieving high explainability is particularly challenging for machine learning (ML) and deep learning (DL) algorithms. For instance, banks using algorithms for credit decision-making must ensure transparency and justify their decisions.\nOne method for improving algorithm explainability is LIME (Local Interpretable Model-Agnostic Explanations), published in 2016. This method introduces small changes to input data and analyzes their impact on the output, allowing the identification of local decision-making rules within the model.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lime.lime_tabular import LimeTabularExplainer\n\n# Wczytanie danych Iris\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Podzia≈Ç na zbi√≥r treningowy i testowy\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Trenowanie modelu\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Tworzenie obiektu LIME do interpretacji modelu\nexplainer = LimeTabularExplainer(X_train, feature_names=iris.feature_names, class_names=iris.target_names, discretize_continuous=True)\n\n# Interpretacja losowego przyk≈Çadu ze zbioru testowego\ni = np.random.randint(0, len(X_test))  # Wyb√≥r losowego przyk≈Çadu\nexp = explainer.explain_instance(X_test[i], model.predict_proba)\n\n# Wy≈õwietlenie interpretacji\nexp.show_in_notebook()\n\n\n\n        \n        \n        \n        \n        \n        \n\n\nHow Does This Code Work?\n\nLoading Data and Training the Model\n\n\nUses the Iris dataset, containing 150 examples of flowers from three species:\nSetosa\nVersicolor\nVirginica\nThe RandomForestClassifier model is trained on this data.\n\n\nCreating an Interpretable Model Using LIME\n\n\nLIME generates local explanations, interpreting the model for individual predictions.\nA random test example is selected.\n\n\nExploring the Outcome for a Single Example\n\n\nLIME slightly modifies input values and observes how the prediction changes.\nIt creates a ‚Äúlocal‚Äù linear model that shows which features had the most influence on the decision.\n\nLet‚Äôs assume our model selects a sample flower and classifies it as Virginica.\nInterpretation of Results:\n\nKey Features Affecting Model Decision:\n\n\nPetal length: The most significant factor (e.g., a longer petal suggests Virginica).\nPetal width: Also a crucial factor (e.g., above a certain threshold indicates Virginica).\nSepal length: A less significant but still relevant factor.\nSepal width: Usually the least important feature.\n\n\nVisualization of Results:\n\n\nLIME generates a bar chart showing the impact of each feature on classification.\nThe chart highlights which features increased or decreased the probability of a specific classification.\n\n\nWhat Does This Mean?\n\n\nIf the model predicts Virginica with high probability, key features (e.g., long petals) strongly indicate this species.\nIf the features had mixed influences, it suggests the model had difficulty classifying the instance (e.g., petal width was ambiguous).",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 5"
    ]
  },
  {
    "objectID": "lectures/lecture5.html#anomaly-detection",
    "href": "lectures/lecture5.html#anomaly-detection",
    "title": "Lecture 5",
    "section": "Anomaly detection",
    "text": "Anomaly detection\nAn outlier is an observation (a row in a dataset) that is significantly different from the other elements in the sample. This means that the relationship between independent and dependent variables for this observation may differ from other cases.\nFor single variables, outliers can be identified using a box plot, which is based on quartiles:\n\nFirst quartile and third quartile define the edges of the box,\nSecond quartile (median) is marked inside the box,\n\nOutliers satisfy the condition:\n[ x_{out} &lt; Q_1 - 1.5 IQR x_{out} &gt; Q_3 + 1.5 IQR ]\nWhere: [ IQR = Q_3 - Q_1 ]\nAn example of an outlier could be a Formula 1 car ‚Äì in terms of speed, it is an anomaly among regular cars.\n\nUse of Anomaly Detection\nAnomaly detection has a wide range of applications, such as:\n\nFinance ‚Äì detecting fraudulent transactions in banking data analysis,\nCybersecurity ‚Äì identifying intruders in a network based on user behavior,\nMedicine ‚Äì monitoring health parameters and detecting abnormalities,\nIndustry ‚Äì detecting faulty components through image analysis.\n\n\n\nAnomaly Detection Methods\n\n1. Supervised Methods (supervised learning)\nUsed when labeled data is available (e.g., fraud cases in transactions). - Neural networks, - K-Nearest Neighbors algorithm (KNN), - Bayesian networks.\n\n\n2. Unsupervised Methods (unsupervised learning)\nAssumes that most data is correct, and anomalies are a small percentage of cases. - K-Means clustering, - Autoencoders in neural networks, - Statistical tests.\n\n\n\nClassical Method ‚Äì Detection Based on Probability\nTo determine whether a given observation is an anomaly, we can use the probability \\(p(x)\\): - If \\(p(x) &lt; \\epsilon\\), we consider the value as an outlier. - In practice, we assume the data follows a normal distribution \\(N(\\mu, \\sigma)\\). - We estimate the parameters \\(\\mu\\) (mean) and \\(\\sigma^2\\) (variance) based on a sample. - Then, for each value, we calculate the probability of its occurrence and compare it with .\nExample: Salary Analysis in a Company\nWe detect whether there are individuals in the company whose salaries significantly deviate from the average.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsalaries = [40, 42, 45, 47, 50, 55, 60, 70, 90, 150]  # 150 to outlier\n\nQ1 = np.percentile(salaries, 25)\nQ3 = np.percentile(salaries, 75)\nIQR = Q3 - Q1\n\noutlier_threshold_low = Q1 - 1.5 * IQR\noutlier_threshold_high = Q3 + 1.5 * IQR\n\noutliers = [x for x in salaries if x &lt; outlier_threshold_low or x &gt; outlier_threshold_high]\n\nprint(f\"Outliers: {outliers}\")\n\nsns.boxplot(salaries)\nplt.title(\"Box plot\")\nplt.show()\n\n\nOutliers: [150]\n\n\n\n\n\n\n\n\n\nResult: The box plot shows that 150k is an anomaly.\n\n\nIsolation Forest ‚Äì Anomaly Detection Using Isolation Forest\nIsolation Forest is an algorithm based on decision trees, proposed by Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou in 2008. It identifies anomalies by isolating outliers during the data partitioning process: - Randomly selects a feature and a split value, - Outliers are isolated more quickly (closer to the root of the tree), - The result is aggregated based on multiple trees.\nIts advantages include low computational requirements and effectiveness in analyzing high-dimensional data.\nAnomaly Detection Methods in sklearn\nExample: Detecting Credit Card Fraud\nA bank analyzes credit card transactions and detects those that may be unauthorized.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\n\n# Przyk≈Çadowe dane transakcji (kwota, liczba transakcji w tygodniu)\ndata = np.array([\n    [100, 5], [120, 6], [130, 5], [110, 4], [5000, 1],  # Outlier (du≈ºa kwota, rzadko≈õƒá)\n    [125, 5], [115, 5], [140, 7], [135, 6], [145, 5]\n])\n\n# Model Isolation Forest\nclf = IsolationForest(contamination=0.1)  # 10% transakcji uznajemy za anomalie\nclf.fit(data)\n\n# Predykcja (1 = normalna transakcja, -1 = oszustwo)\npredictions = clf.predict(data)\ndf = pd.DataFrame(data, columns=[\"Kwota\", \"Liczba transakcji\"])\ndf[\"Anomalia\"] = predictions\n\nprint(df)\n\n\n   Kwota  Liczba transakcji  Anomalia\n0    100                  5         1\n1    120                  6         1\n2    130                  5         1\n3    110                  4         1\n4   5000                  1        -1\n5    125                  5         1\n6    115                  5         1\n7    140                  7         1\n8    135                  6         1\n9    145                  5         1\n\n\nResult: A transaction of 5000 PLN will be detected as an anomaly.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 5"
    ]
  },
  {
    "objectID": "lectures/lecture3.html",
    "href": "lectures/lecture3.html",
    "title": "Lecture 3",
    "section": "",
    "text": "‚è≥ Duration: 1.5h\nüéØ Lecture Goal\nIntroducing students to the fundamentals of real-time analytics.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 3"
    ]
  },
  {
    "objectID": "lectures/lecture3.html#definitions",
    "href": "lectures/lecture3.html#definitions",
    "title": "Lecture 3",
    "section": "Definitions",
    "text": "Definitions\n\nLearn more about streaming data\n\nDefinition 1 ‚Äì Event: Any observable occurrence at a specific point in time. In data streams, events are immutable records that represent a specific action.\nDefinition 2 ‚Äì In the context of data, an event is encoded as JSON, XML, CSV, or in a binary format.\nDefinition 3 ‚Äì Continuous Event Stream: An ongoing sequence of events that happen over time, such as logs from servers, sensor readings, or user activity.\nDefinition 4 ‚Äì Data Stream: A continuous flow of data from dynamic sources like sensor readings or log entries. These streams can be generated in real-time or derived from static sources like databases or files.\nA business is an organization that generates and responds to a continuous stream of events.\n\n\n\nStream Analytics\nStream Analytics (also called Event Stream Processing) refers to processing large amounts of data as they are generated, in real-time.\nRegardless of the technology used, all data exists as a continuous stream of events, including: - User actions on websites,\n- System logs,\n- Sensor measurements.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 3"
    ]
  },
  {
    "objectID": "lectures/lecture3.html#time-in-real-time-data-analysis",
    "href": "lectures/lecture3.html#time-in-real-time-data-analysis",
    "title": "Lecture 3",
    "section": "Time in Real-Time Data Analysis",
    "text": "Time in Real-Time Data Analysis\nIn batch processing, we analyze historical data, and the timing of the process is unrelated to when the events actually occurred.\nIn stream processing, there are two key concepts of time: 1. Event Time ‚Äì the actual moment the event occurred. 2. Processing Time ‚Äì the moment the system processes the event.\n\nIdeal Data Processing\nIn an ideal scenario, processing happens immediately after the event occurs:\n\n\n\nReal Data Processing\nIn reality, data processing always involves some delay, visible as points below the ideal processing line (below the diagonal in the chart):\n\nIn stream processing applications, the difference between event time and processing time is crucial. Common causes of delays include:\n\nData transmission over the network,\nLack of communication between the device and the network.\n\nAn example of this is tracking the location of a car via a GPS application ‚Äì passing through a tunnel might cause temporary data loss.\n\n\nHandling Delays in Stream Processing\nDelays in event processing can be managed in two main ways: 1. Monitoring the number of missed events and triggering an alarm if the number of rejected events exceeds a threshold. 2. Applying correction using watermarking, which is an additional mechanism that accounts for delayed events.\nThe process of real-time event processing can be represented as a step function:\n\nNot all events contribute to the analysis ‚Äì some may be discarded due to excessive delays.\nBy using watermarking, additional time is allowed for the appearance of delayed events. This process includes all events above the dashed line. However, there might still be cases where some points are skipped.\n\nThe situations illustrated in the charts explicitly indicate why the concept of time is a critical factor and needs to be clearly defined at the business requirements level. Assigning timestamps to data (events) is a complex task.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 3"
    ]
  },
  {
    "objectID": "lectures/lecture3.html#time-windows-in-stream-analytics",
    "href": "lectures/lecture3.html#time-windows-in-stream-analytics",
    "title": "Lecture 3",
    "section": "Time Windows in Stream Analytics",
    "text": "Time Windows in Stream Analytics\nIn stream processing, time windows allow grouping data into time-limited segments, enabling event analysis within specific time intervals. Depending on the use case, various types of windows are applied, tailored to the characteristics of the data and analytical requirements.\n\n\n1. Tumbling Window\nA tumbling window is a fixed-length window that does not overlap ‚Äì each event belongs to only one window.\n‚úÖ Characteristics:\n- Fixed window length\n- No overlap between windows\n- Ideal for dividing data into equal time segments\nüìå Example: Analyzing the number of orders in an online store every 5 minutes.\n\n\n\n\n2. Sliding Window\nA sliding window includes all events occurring within a specific time interval, where the window slides continuously.\n‚úÖ Characteristics:\n- Each event can belong to multiple windows\n- The window shifts by a specified interval\n- Useful for detecting trends and anomalies\nüìå Example: Tracking the average temperature over the last 10 minutes, updated every 2 minutes.\n\n\n\n\n3. Hopping Window\nA hopping window is similar to a tumbling window, but it allows overlapping windows, meaning one event can belong to multiple windows. It is used to smooth data.\n‚úÖ Characteristics:\n- Fixed window length\n- Overlapping windows\n- Useful for noise reduction in data\nüìå Example: Analyzing the number of website visitors every 10 minutes, but updated every 5 minutes to better capture trends.\n\n\n\n\n4. Session Window\nA session window groups events based on activity periods and closes after a specified period of inactivity.\n‚úÖ Characteristics:\n- Dynamic window length\n- Defined by user activity\n- Used in user session analysis\nüìå Example: Analyzing user sessions on a website ‚Äì the session lasts as long as the user is active, but ends after 15 minutes of inactivity.\n\n\n\nSummary\nDifferent types of time windows are applied depending on the data‚Äôs characteristics and the analysis objectives. Choosing the right window impacts the accuracy of results and the efficiency of the analytical system.\n\n\n\n\n\n\n\n\nWindow Type\nCharacteristics\nUse Cases\n\n\n\n\nTumbling\nFixed length, no overlap\nPeriodic reports\n\n\nSliding\nFixed length, overlapping windows\nTrend detection, anomaly detection\n\n\nHopping\nFixed length, partial overlap\nData smoothing\n\n\nSession\nDynamic length, activity-dependent\nUser session analysis\n\n\n\nEach window type has its unique use cases and helps with better interpretation of streaming data. The choice of method depends on business needs and the nature of the analyzed data.\nIn stream data analysis, interpreting time is a complex issue due to: 1. Different systems having different clocks, leading to inconsistencies, 2. Data arriving with delays, requiring watermarking and time window techniques, 3. Different approaches to event time vs.¬†processing time impacting result accuracy.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 3"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Info",
    "section": "",
    "text": "Code: 222891-D\nSemester: 2024/2025 University: SGH Warsaw School of Economics\nBasic course information can be found in the sylabus.\nRecommended reading is available in the books section.",
    "crumbs": [
      "Home",
      "Info"
    ]
  },
  {
    "objectID": "index.html#real-time-analytics",
    "href": "index.html#real-time-analytics",
    "title": "Info",
    "section": "",
    "text": "Code: 222891-D\nSemester: 2024/2025 University: SGH Warsaw School of Economics\nBasic course information can be found in the sylabus.\nRecommended reading is available in the books section.",
    "crumbs": [
      "Home",
      "Info"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Info",
    "section": "Schedule",
    "text": "Schedule\n\nLectures\nLectures are conducted in a hybrid format. Attendance is optional, with in-person sessions held in Aula VI, Building G.\n\n18-02-2025 (Tuesday) 11:40-13:20 - Lecture 1\n25-02-2025 (Tuesday) 11:40-13:20 - Lecture 2\n04-03-2025 (Tuesday) 11:40-13:20 - Online Lecture 3\n11-03-2025 (Tuesday) 11:40-13:20 - Lecture 4\n18-03-2025 (Tuesday) 11:40-13:20 - Lecture 5\n\nLecture 5 concludes with a test.\nFormat: 20 questions | 30 minutes Platform: MS Teams\n\n\nTEST\nLectures conclude with a test during the final class. Achieving a score above 13 points qualifies you to participate in the exercises.\nAfter completing the exercises, homework assignments will be submitted via the MS Teams platform\nThe project should be carried out in groups of no more than 5 people.\nProject requirements:\n\nThe project should address a business problem that can be implemented using publicly available data.\nData should be ingested into Apache Kafka and subsequently processed and analyzed.\nYou may choose any programming language for each component of the project.\nBusiness Intelligence (BI) tools may be utilized.\nData sources may include tables, synthetically generated data, IoT devices, etc.",
    "crumbs": [
      "Home",
      "Info"
    ]
  },
  {
    "objectID": "index.html#technology",
    "href": "index.html#technology",
    "title": "Info",
    "section": "Technology",
    "text": "Technology\nTo participate in the classes, you should be familiar with and utilize the following technologies:\n\nGIT\nPython, Jupyter notebook, Jupyter lab, Colab\nDocker\nApache Spark, Apache Flink, Apache Kafka, Apache Beam\nDatabricks Community edition Web page.",
    "crumbs": [
      "Home",
      "Info"
    ]
  },
  {
    "objectID": "sylabus.html",
    "href": "sylabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Real Time Analytics\nSGH Warsaw School of Economics\nECTS: 3\nLanguage: EN\nlevel: medium\nday of week: Monday/Tuesday\nTeacher: Sebastian ZajƒÖc, sebastian.zajac@sgh.waw.pl\nWebsite: http://sebkaz-teaching.github.io/EN"
  },
  {
    "objectID": "sylabus.html#description",
    "href": "sylabus.html#description",
    "title": "Syllabus",
    "section": "Description",
    "text": "Description\nMaking the right decisions based on data and their analysis in business is a process and daily. Modern methods of modeling by machine learning (ML), artificial intelligence (AI), or deep learning not only allow better understanding of business, but also support making key decisions for it. The development of technology and increasingly new business concepts of working directly with the client require not only correct but also fast decisions. The classes offered are designed to provide students with experience and comprehensive theoretical knowledge in the field of real-time data processing and analysis, and to present the latest technologies (free and commercial) for the processing of structured data (originating e.g.¬†from data warehouses) and unstructured (e.g.¬†images, sound, video streaming) in on-line mode. The course will present the so called lambda and kappa structures for data processing into data lake along with a discussion of the problems and difficulties encountered in implementing real-time modeling for large amounts of data. Theoretical knowledge will be gained (apart from the lecture part) through the implementation of test cases in tools such as Apache Spark, Nifi, Microsoft Azure and SAS. During laboratory classes student will benefit from fully understand the latest information technologies related to real-time data processing."
  },
  {
    "objectID": "sylabus.html#list-of-topics",
    "href": "sylabus.html#list-of-topics",
    "title": "Syllabus",
    "section": "List of Topics",
    "text": "List of Topics\n\nModelling, learning and prediction in batch mode (offline learning) and incremental (online learning) modes. Problems of incremental machine learning.\nData processing models in Big Data. From flat files to Data Lake. Real-time data myth and facts\nNRT systems (near real-time systems), data acquisition, streaming and analytics.\nAlgorithms for estimating model parameters in incremental mode. Stochastic Gradient Descent.\nLambda and Kappa architecture. Designing IT architecture for real-time data processing.\nPreparation of the micro-service with the ML model for prediction use.\nStructured and unstructured data. Relational databases and NoSQL databases.\nAggregations and reporting in NoSQL databases (on the example of the MongoDB or Cassandra)\nBasic of object-oriented programming in Python in linear and logistic regression, neural network analysis using the sklearn, TensorFlow and Keras.\nIT architecture of Big Data processing. Preparation of a virtual env for Apache Spark."
  },
  {
    "objectID": "sylabus.html#conditions-for-passing",
    "href": "sylabus.html#conditions-for-passing",
    "title": "Syllabus",
    "section": "Conditions for passing",
    "text": "Conditions for passing\n\ntest 30%\npractical test 30% (IF)\ngroup project 40% (70%)"
  },
  {
    "objectID": "sylabus.html#books",
    "href": "sylabus.html#books",
    "title": "Syllabus",
    "section": "Books",
    "text": "Books\n\nS. Zajac, ‚ÄúModelowanie dla biznesu, Analityka w czasie rzeczywistym - narzƒôdzia informatyczne i biznesowe‚Äù. SGH (2022)\nFraÃ®tczak E., red. ‚ÄúModelowanie dla biznesu, Regresja logistyczna, Regresja Poissona, Survival Data Mining, CRM, Credit Scoring‚Äù. SGH, Warszawa 2019.\nFraÃ®tczak E., red., ‚ÄúZaawansowane metody analiz statystycznych‚Äù, Oficyna Wydawnicza SGH, Warszawa 2012.\nIndest A., Wild Knowledge. Outthik the Revolution. LID publishing.com 2017.\nReal Time Analytic. ‚ÄúThe Key to Unlocking Customer Insights & Driving the Customer Experience‚Äù. Harvard Business Review Analytics Series, Harvard Business School Publishing, 2018.\nSvolba G., ‚ÄúApplying Data Science. Business Case Studies Using SAS‚Äù. SAS Institute Inc., Cary NC, USA, 2017.\nEllis B. ‚ÄúReal-Time Analytics Techniques to Analyze and Visualize Streaming data.‚Äù , Wiley, 2014\nFamiliar B., Barnes J. ‚ÄúBusiness in Real-Time Using Azure IoT and Cortana Intelligence Suite‚Äù Apress, 2017"
  },
  {
    "objectID": "labs/lab1.html",
    "href": "labs/lab1.html",
    "title": "REST API with Flask",
    "section": "",
    "text": "Client-Serwer\nThe Flask library handles adding subpages and communication with the server using functions wrapped with a decorator. Example of a decorator:\n# CODE\nimport subprocess\nimport requests\n%%file app1.py\n\nfrom flask import Flask\n\n# Create a flask\napp = Flask(__name__)\n\n# Create an API end point\n@app.route('/hello')\ndef say_hello():\n    return \"Hello World\"\n\n@app.route('/')\ndef say_he():\n    return \"Hello from main site\"\n\nif __name__ == '__main__':\n    app.run()\np = subprocess.Popen([\"python\", \"app1.py\"])\nadres_url = \"http://127.0.0.1:5000/hello\"\nresponse = requests.get(adres_url)\n# take content field from response object\n# YOUR CODE\n# take bad adress\n\nadres_url = \" \"\n\nresponse = requests.get(adres_url)\n\n# Check if status_code field is 200\n# YOUR CODE\nrun kill() method for p object\n# YOUR CODE\nCheck any request and raspone whe your serwer is down\n# YOUR CODE\nresponse = ...",
    "crumbs": [
      "Home",
      "Labs",
      "Python codes",
      "REST API with Flask"
    ]
  },
  {
    "objectID": "labs/lab1.html#url-adress-with-get-method-for-a-sanding-data",
    "href": "labs/lab1.html#url-adress-with-get-method-for-a-sanding-data",
    "title": "REST API with Flask",
    "section": "URL Adress with GET method for a sanding data",
    "text": "URL Adress with GET method for a sanding data\n\n%%file app2.py\n\nfrom flask import Flask\nfrom flask import request\n\n# Create a flask\napp = Flask(__name__)\n\n# Create an API end point\n@app.route('/hello', methods=['GET'])\ndef say_hello():\n    name = request.args.get(\"name\", \"\") # tutaj leci str\n    title = request.args.get(\"title\", \"\")\n    if name:\n        resp = f\"Hello {title} {name}\" if title else f\"Hello {name}\"\n    else:\n        resp = f\"Hello {title}\" if title else \"Hello\"\n    return resp\n\nif __name__ == '__main__':\n    app.run(port=5005)\n\n\np = subprocess.Popen([\"python\", \"app2.py\"])\n\n\nresponse = requests.get(\"http://127.0.0.1:5005/hello\")\nresponse.content\n\nadd variables for url adress ?name=....\n\nresponse = requests.get(\"http://127.0.0.1:5005/hello?name=Sebastian\")\nresponse.content\n\nTry json answear\nfrom flask import jsonify\n\ndef moja_f():\n    ...\n    return jsonify(reponse=resp)\n\nAn interesting and more functional solution for ML models is the [litServe](https://lightning.ai/litserve) library.\n\nNotice the pipeline, which is written in a functional style.\n\n::: {#389512cc-55fe-43e6-b0d5-a627e60399a7 .cell execution_count=4}\n``` {.python .cell-code}\n%%file app_lit.py\nimport litserve as ls\n\nclass SimpleLitAPI(ls.LitAPI):\n    def setup(self, device):\n        self.model1 = lambda x: x**2\n        self.model2 = lambda x: x**3\n\n    def decode_request(self, request):\n        return request[\"input\"]\n\n    def predict(self, x):\n        squared = self.model1(x)\n        cubed = self.model2(x)\n        output = squared + cubed\n        return {\"output\": output}\n\n    def encode_response(self, output):\n        return {\"output\": output}\n\nif __name__ == \"__main__\":\n    api = SimpleLitAPI()\n    server = ls.LitServer(api)\n    server.run(port=5001)\n\nOverwriting app_lit.py\n\n:::\n\nimport subprocess\n\np = subprocess.Popen([\"python\", \"app_lit.py\"])\n\n\nimport requests\n\nresponse = requests.post(\"http://127.0.0.1:5001/predict\", json={\"input\": 4.0})\nprint(f\"Status: {response.status_code}\\nResponse:\\n {response.text}\")\n\nStatus: 200\nResponse:\n {\"output\":{\"output\":80.0}}\n\n\n\np.kill()",
    "crumbs": [
      "Home",
      "Labs",
      "Python codes",
      "REST API with Flask"
    ]
  },
  {
    "objectID": "labs/lab1.html#plik-aplikacji-app.py",
    "href": "labs/lab1.html#plik-aplikacji-app.py",
    "title": "REST API with Flask",
    "section": "Plik aplikacji app.py",
    "text": "Plik aplikacji app.py\n\n%%file app.py\n\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/')\ndef hello():\n    return \"&lt;h1&gt;hello world&lt;/h1&gt;\"\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)",
    "crumbs": [
      "Home",
      "Labs",
      "Python codes",
      "REST API with Flask"
    ]
  },
  {
    "objectID": "labs/lab1.html#plik-requirements.txt-w-kt√≥rym-zamie≈õcimy-potrzebne-biblioteki",
    "href": "labs/lab1.html#plik-requirements.txt-w-kt√≥rym-zamie≈õcimy-potrzebne-biblioteki",
    "title": "REST API with Flask",
    "section": "plik requirements.txt w kt√≥rym zamie≈õcimy potrzebne biblioteki",
    "text": "plik requirements.txt w kt√≥rym zamie≈õcimy potrzebne biblioteki\n\n%%file requirements.txt\nFlask==3.0.1",
    "crumbs": [
      "Home",
      "Labs",
      "Python codes",
      "REST API with Flask"
    ]
  },
  {
    "objectID": "labs/lab1.html#dockerfile",
    "href": "labs/lab1.html#dockerfile",
    "title": "REST API with Flask",
    "section": "Dockerfile",
    "text": "Dockerfile\n\npobranie obrazu systemu z pythonem\nkopia pliku z wymaganymi bibliotekami\ninstalacja wymaganych bibliotek w ≈õrodowisku\nskopiowanie pliku aplikacji\nuruchomienie aplikacji\n\n\n%%file Dockerfile\nFROM python:3.11-slim-buster\n\nWORKDIR /app\n\nCOPY requirements.txt requirements.txt\n\nRUN pip install -r requirements.txt\n\nCOPY app.py .\n\nENV FLASK_APP=app\n\nEXPOSE 5000\nCMD [\"flask\", \"run\", \"--host\", \"0.0.0.0\", \"--port\", \"5000\"]\n\ndocker build -t test_hello .\n\ndocker run -p 5000:5000 test_hello",
    "crumbs": [
      "Home",
      "Labs",
      "Python codes",
      "REST API with Flask"
    ]
  },
  {
    "objectID": "labs/lab2.html",
    "href": "labs/lab2.html",
    "title": "API with Flask Excercises",
    "section": "",
    "text": "In this exercise, you will learn how to create a simple API in Flask, run it, send requests to it, and use a decision model based on a basic logical rule.",
    "crumbs": [
      "Home",
      "Labs",
      "Python codes",
      "API with Flask Excercises"
    ]
  },
  {
    "objectID": "labs/lab2.html#creating-a-basic-api",
    "href": "labs/lab2.html#creating-a-basic-api",
    "title": "API with Flask Excercises",
    "section": "1Ô∏è‚É£ Creating a Basic API",
    "text": "1Ô∏è‚É£ Creating a Basic API\nFirst, we will create a basic Flask application.\n\nSaving the API Code to a File\nIn Jupyter Notebook, use the magic command %%file to save the basic Flask application code to a file named app.py. You can find the code at lab1.\nFor the main page text, use Welcome to my API!.\n\n%%file app.py\n###\n# YOUR CODE HERE\n###\n\nWriting app.py\n\n\nNow, start the API in the terminal by typing:\npython app.py\nFlask will start the server locally at http://127.0.0.1:5000/.\n\n\nChecking API Functionality\nIn Jupyter Notebook, perform a GET request to the homepage. Based on the status_code field, write a conditional expression that will display the response content (from the content field) if the status_code is 200.\n\nimport requests\nresponse = #YOUR CODE\n\nIf everything is working correctly, you will see the message Welcome to my API!.",
    "crumbs": [
      "Home",
      "Labs",
      "Python codes",
      "API with Flask Excercises"
    ]
  },
  {
    "objectID": "labs/lab2.html#adding-a-new-subpage",
    "href": "labs/lab2.html#adding-a-new-subpage",
    "title": "API with Flask Excercises",
    "section": "2Ô∏è‚É£ Adding a New Subpage",
    "text": "2Ô∏è‚É£ Adding a New Subpage\nLet‚Äôs add a new subpage mypage that will return the message This is my page!.\n\n%%file app.py\n###\n# YOUR CODE HERE \n###\n\nRestart the API and make a request to the page \"http://127.0.0.1:5000/mypage\":\n\nresponse = pass # YOUR CODE HERE\n\nYou should see: This is my page!",
    "crumbs": [
      "Home",
      "Labs",
      "Python codes",
      "API with Flask Excercises"
    ]
  },
  {
    "objectID": "labs/lab2.html#automatically-starting-the-server-from-jupyter-notebook",
    "href": "labs/lab2.html#automatically-starting-the-server-from-jupyter-notebook",
    "title": "API with Flask Excercises",
    "section": "3Ô∏è‚É£ Automatically Starting the Server from Jupyter Notebook",
    "text": "3Ô∏è‚É£ Automatically Starting the Server from Jupyter Notebook\nClose the previously started server (Ctrl+C in the terminal) and restart it directly from Jupyter Notebook using subprocess.Popen:\n\nimport subprocess\n# YOUR CODE \nserver = pass\n\nAfter testing, close the server using the kill method:\n\n# Your code here",
    "crumbs": [
      "Home",
      "Labs",
      "Python codes",
      "API with Flask Excercises"
    ]
  },
  {
    "objectID": "labs/lab2.html#handling-parameters-in-the-url",
    "href": "labs/lab2.html#handling-parameters-in-the-url",
    "title": "API with Flask Excercises",
    "section": "4Ô∏è‚É£ Handling Parameters in the URL",
    "text": "4Ô∏è‚É£ Handling Parameters in the URL\nLet‚Äôs add a new subpage /hello that will accept a name parameter.\nEdit app.py by adding the appropriate code.\n\n%%file app.py\n###\n# Your Code\n###\n\nStart the server and check the API functionality:\nres1 = requests.get(\"http://127.0.0.1:5000/hello\")\nprint(res1.content)  # Powinno zwr√≥ciƒá \"Hello!\"\n\nres2 = requests.get(\"http://127.0.0.1:5000/hello?name=Sebastian\")\nprint(res2.content)  # Powinno zwr√≥ciƒá \"Hello Sebastian!\"",
    "crumbs": [
      "Home",
      "Labs",
      "Python codes",
      "API with Flask Excercises"
    ]
  },
  {
    "objectID": "labs/lab2.html#creating-an-api-with-a-simple-ml-model",
    "href": "labs/lab2.html#creating-an-api-with-a-simple-ml-model",
    "title": "API with Flask Excercises",
    "section": "5Ô∏è‚É£ Creating an API with a Simple ML Model",
    "text": "5Ô∏è‚É£ Creating an API with a Simple ML Model\nWe will create a new subpage /api/v1.0/predict, which accepts two numbers and returns the result of a decision rule: - If the sum of the two numbers is greater than 5.8, it returns 1. - Otherwise, it returns 0.\nVerify API\nres = requests.get(\"http://127.0.0.1:5000/api/v1.0/predict?num1=3&num2=4\")\nprint(res.json())  # Powinno zwr√≥ciƒá {\"prediction\": 1, \"features\": {\"num1\": 3.0, \"num2\": 4.0}}",
    "crumbs": [
      "Home",
      "Labs",
      "Python codes",
      "API with Flask Excercises"
    ]
  },
  {
    "objectID": "labs/lab2.html#summary",
    "href": "labs/lab2.html#summary",
    "title": "API with Flask Excercises",
    "section": "Summary",
    "text": "Summary\nAfter completing this exercise, students will be able to: ‚úÖ Create a basic API in Flask. ‚úÖ Add subpages and handle URL parameters. ‚úÖ Send GET requests and analyze responses. ‚úÖ Automatically start the server from Jupyter Notebook. ‚úÖ Implement a simple decision model in the API.\nReady for the next challenges? üöÄ",
    "crumbs": [
      "Home",
      "Labs",
      "Python codes",
      "API with Flask Excercises"
    ]
  },
  {
    "objectID": "labs/lab3.html",
    "href": "labs/lab3.html",
    "title": "Structured data",
    "section": "",
    "text": "# variables\ncustomer1_age = 38\ncustomer1_height = 178\ncustomer1_loan = 34.23\ncustomer1_name = 'Zajac'\n\n\nWhy don‚Äôt we use variables for data analysis?\n\nIn Python, regardless of the type of data being analyzed and processed, we can collect data and represent it as a form of list.\n\n# python lists - what we can put on list ?\ncustomers = [[38, 278, 34.23, 'Zajac'],[38, 278, 34.23, 'kowalski']]\nprint(customers)\n\n[[38, 278, 34.23, 'Zajac'], [38, 278, 34.23, 'kowalski']]\n\n\n\n# different types in one object\ntype(customers)\n\nlist\n\n\n\nWhy lists aren‚Äôt the best place to store data?\n\nLet‚Äôs take two numerical lists.‚Äù\n\n# two numerical lists\na = [1,2,3]\nb = [4,5,6]\n\nTypical operations on lists in data analysis\n\n# add lists\nprint(f\"a+b: {a+b}\")\n# we can use .format also \nprint(\"a+b: {}\".format(a+b))\n\na+b: [1, 2, 3, 4, 5, 6]\na+b: [1, 2, 3, 4, 5, 6]\n\n\n\n# multiplication\ntry:\n    print(a*b)\nexcept TypeError:\n    print(\"no-defined operation\")\n\nno-defined operation\n\n\n\nimport numpy as np\naa = np.array(a)\nbb = np.array(b)\n\nprint(aa,bb)\n\n[1 2 3] [4 5 6]\n\n\n\nnp.array([34, 234.23])\n\narray([ 34.  , 234.23])\n\n\n\nprint(f\"aa+bb: {aa+bb}\")\n# add - working\ntry:\n    print(\"=\"*50)\n    print(aa*bb)\n    print(\"aa*bb - is this correct ?\")\n    print(np.dot(aa,bb))\n    print(\"np.dot - is this correct ?\")\nexcept TypeError:\n    print(\"no-defined operation\")\n# multiplication\n\naa+bb: [5 7 9]\n==================================================\n[ 4 10 18]\naa*bb - is this correct ?\n32\nnp.dot - is this correct ?\n\n\n\n# array properties\nx = np.array(range(4))\nprint(x)\nx.shape\n\n[0 1 2 3]\n\n\n(4,)\n\n\n\nA = np.array([range(4),range(4)])\n# transposition  row i -&gt; column j, column j -&gt; row i \nA.T\n\narray([[0, 0],\n       [1, 1],\n       [2, 2],\n       [3, 3]])\n\n\n\n# 0-dim object\nscalar = np.array(5)\nprint(f\"scalar object dim: {scalar.ndim}\")\n# 1-dim object\nvector_1d = np.array([3, 5, 7])\nprint(f\"vector object dim: {vector_1d.ndim}\")\n# 2 rows for 3 features\nmatrix_2d = np.array([[1,2,3],[3,4,5]])\nprint(f\"matrix object dim: {matrix_2d.ndim}\")\n\nscalar object dim: 0\nvector object dim: 1\nmatrix object dim: 2\n\n\n\nSebastian Raschka Course",
    "crumbs": [
      "Home",
      "Labs",
      "Python codes",
      "Structured data"
    ]
  },
  {
    "objectID": "labs/lab3.html#data-as-variables",
    "href": "labs/lab3.html#data-as-variables",
    "title": "Structured data",
    "section": "",
    "text": "# variables\ncustomer1_age = 38\ncustomer1_height = 178\ncustomer1_loan = 34.23\ncustomer1_name = 'Zajac'\n\n\nWhy don‚Äôt we use variables for data analysis?\n\nIn Python, regardless of the type of data being analyzed and processed, we can collect data and represent it as a form of list.\n\n# python lists - what we can put on list ?\ncustomers = [[38, 278, 34.23, 'Zajac'],[38, 278, 34.23, 'kowalski']]\nprint(customers)\n\n[[38, 278, 34.23, 'Zajac'], [38, 278, 34.23, 'kowalski']]\n\n\n\n# different types in one object\ntype(customers)\n\nlist\n\n\n\nWhy lists aren‚Äôt the best place to store data?\n\nLet‚Äôs take two numerical lists.‚Äù\n\n# two numerical lists\na = [1,2,3]\nb = [4,5,6]\n\nTypical operations on lists in data analysis\n\n# add lists\nprint(f\"a+b: {a+b}\")\n# we can use .format also \nprint(\"a+b: {}\".format(a+b))\n\na+b: [1, 2, 3, 4, 5, 6]\na+b: [1, 2, 3, 4, 5, 6]\n\n\n\n# multiplication\ntry:\n    print(a*b)\nexcept TypeError:\n    print(\"no-defined operation\")\n\nno-defined operation\n\n\n\nimport numpy as np\naa = np.array(a)\nbb = np.array(b)\n\nprint(aa,bb)\n\n[1 2 3] [4 5 6]\n\n\n\nnp.array([34, 234.23])\n\narray([ 34.  , 234.23])\n\n\n\nprint(f\"aa+bb: {aa+bb}\")\n# add - working\ntry:\n    print(\"=\"*50)\n    print(aa*bb)\n    print(\"aa*bb - is this correct ?\")\n    print(np.dot(aa,bb))\n    print(\"np.dot - is this correct ?\")\nexcept TypeError:\n    print(\"no-defined operation\")\n# multiplication\n\naa+bb: [5 7 9]\n==================================================\n[ 4 10 18]\naa*bb - is this correct ?\n32\nnp.dot - is this correct ?\n\n\n\n# array properties\nx = np.array(range(4))\nprint(x)\nx.shape\n\n[0 1 2 3]\n\n\n(4,)\n\n\n\nA = np.array([range(4),range(4)])\n# transposition  row i -&gt; column j, column j -&gt; row i \nA.T\n\narray([[0, 0],\n       [1, 1],\n       [2, 2],\n       [3, 3]])\n\n\n\n# 0-dim object\nscalar = np.array(5)\nprint(f\"scalar object dim: {scalar.ndim}\")\n# 1-dim object\nvector_1d = np.array([3, 5, 7])\nprint(f\"vector object dim: {vector_1d.ndim}\")\n# 2 rows for 3 features\nmatrix_2d = np.array([[1,2,3],[3,4,5]])\nprint(f\"matrix object dim: {matrix_2d.ndim}\")\n\nscalar object dim: 0\nvector object dim: 1\nmatrix object dim: 2\n\n\n\nSebastian Raschka Course",
    "crumbs": [
      "Home",
      "Labs",
      "Python codes",
      "Structured data"
    ]
  },
  {
    "objectID": "labs/lab3.html#pytorch",
    "href": "labs/lab3.html#pytorch",
    "title": "Structured data",
    "section": "PyTorch",
    "text": "PyTorch\nPyTorch is an open-source Python-based deep learning library. PyTorch has been the most widely used deep learning library for research since 2019 by a wide margin. In short, for many practitioners and researchers, PyTorch offers just the right balance between usability and features.\n\nPyTorch is a tensor library that extends the concept of array-oriented programming library NumPy with the additional feature of accelerated computation on GPUs, thus providing a seamless switch between CPUs and GPUs.\nPyTorch is an automatic differentiation engine, also known as autograd, which enables the automatic computation of gradients for tensor operations, simplifying backpropagation and model optimization.\nPyTorch is a deep learning library, meaning that it offers modular, flexible, and efficient building blocks (including pre-trained models, loss functions, and optimizers) for designing and training a wide range of deep learning models, catering to both researchers and developers.\n\n\nimport torch\n\n\ntorch.cuda.is_available()\n\nFalse\n\n\n\ntensor0d = torch.tensor(1) \ntensor1d = torch.tensor([1, 2, 3])\ntensor2d = torch.tensor([[1, 2, 2], [3, 4, 5]])\ntensor3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n\n\nprint(tensor1d.dtype)\n\ntorch.int64\n\n\n\ntorch.tensor([1.0, 2.0, 3.0]).dtype\n\ntorch.float32\n\n\n\ntensor2d\n\ntensor([[1, 2, 2],\n        [3, 4, 5]])\n\n\n\ntensor2d.shape\n\ntorch.Size([2, 3])\n\n\n\nprint(tensor2d.reshape(3, 2))\n\ntensor([[1, 2],\n        [2, 3],\n        [4, 5]])\n\n\n\nprint(tensor2d.T)\n\ntensor([[1, 3],\n        [2, 4],\n        [2, 5]])\n\n\n\nprint(tensor2d.matmul(tensor2d.T))\n\ntensor([[ 9, 21],\n        [21, 50]])\n\n\n\nprint(tensor2d @ tensor2d.T)\n\ntensor([[ 9, 21],\n        [21, 50]])\n\n\nmore info on pytorch",
    "crumbs": [
      "Home",
      "Labs",
      "Python codes",
      "Structured data"
    ]
  },
  {
    "objectID": "labs/lab3.html#data-modeling",
    "href": "labs/lab3.html#data-modeling",
    "title": "Structured data",
    "section": "Data Modeling",
    "text": "Data Modeling\nLet‚Äôs take one variable (xs) and one target variable (ys - target).\nxs = np.array([-1,0,1,2,3,4])\nys = np.array([-3,-1,1,3,5,7])\nWhat kind of model we can use?\n\n# Regresja liniowa \n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nxs = np.array([-1,0,1,2,3,4])\n# a raczej \nxs = xs.reshape(-1, 1)\n\nys = np.array([-3, -1, 1, 3, 5, 7])\n\nreg = LinearRegression()\nmodel = reg.fit(xs,ys)\n\nprint(f\"solution: x1={model.coef_[0]}, x0={reg.intercept_}\")\n\n\nsolution: x1=2.0, x0=-1.0\n\n\narray([1.])\n\n\n\nmodel.predict(np.array([1,5,5,2,4]).reshape(-1,1))\n\narray([1., 9., 9., 3., 7.])\n\n\nThe simple code fully accomplishes our task of finding a linear regression model.\nWhat can we use such a generated model for?\nTo make use of it, we need to export it to a file.\n\n# save model\nimport pickle\nwith open('model.pkl', \"wb\") as picklefile:\n    pickle.dump(model, picklefile)\n\nNow we can import it (for example, on GitHub) and utilize it in other projects.\n\n# load model\nwith open('model.pkl',\"rb\") as picklefile:\n    mreg = pickle.load(picklefile)\n\nBut !!! remember about Python Env\n\nmreg.predict(xs)\n\narray([-3., -1.,  1.,  3.,  5.,  7.])\n\n\nOther ways of acquiring data\n\nReady-made sources in Python libraries.\nData from external files (e.g., CSV, JSON, TXT) from a local disk or the internet.\nData from databases (e.g., MySQL, PostgreSQL, MongoDB).\nData generated artificially for a chosen modeling problem.\nData streams.\n\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\n\n\n# find all keys\niris.keys()\n\ndict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n\n\n\n# print description\nprint(iris.DESCR)\n\n.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n:Number of Instances: 150 (50 in each of three classes)\n:Number of Attributes: 4 numeric, predictive attributes and the class\n:Attribute Information:\n    - sepal length in cm\n    - sepal width in cm\n    - petal length in cm\n    - petal width in cm\n    - class:\n            - Iris-Setosa\n            - Iris-Versicolour\n            - Iris-Virginica\n\n:Summary Statistics:\n\n============== ==== ==== ======= ===== ====================\n                Min  Max   Mean    SD   Class Correlation\n============== ==== ==== ======= ===== ====================\nsepal length:   4.3  7.9   5.84   0.83    0.7826\nsepal width:    2.0  4.4   3.05   0.43   -0.4194\npetal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\npetal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n============== ==== ==== ======= ===== ====================\n\n:Missing Attribute Values: None\n:Class Distribution: 33.3% for each of 3 classes.\n:Creator: R.A. Fisher\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n:Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher's paper. Note that it's the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher's paper is a classic in the field and\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n.. dropdown:: References\n\n  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n    Mathematical Statistics\" (John Wiley, NY, 1950).\n  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n    Structure and Classification Rule for Recognition in Partially Exposed\n    Environments\".  IEEE Transactions on Pattern Analysis and Machine\n    Intelligence, Vol. PAMI-2, No. 1, 67-71.\n  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n    on Information Theory, May 1972, 431-433.\n  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n    conceptual clustering system finds 3 classes in the data.\n  - Many, many more ...\n\n\n\n\niris.data\n\narray([[5.1, 3.5, 1.4, 0.2],\n       [4.9, 3. , 1.4, 0.2],\n       [4.7, 3.2, 1.3, 0.2],\n       [4.6, 3.1, 1.5, 0.2],\n       [5. , 3.6, 1.4, 0.2],\n       [5.4, 3.9, 1.7, 0.4],\n       [4.6, 3.4, 1.4, 0.3],\n       [5. , 3.4, 1.5, 0.2],\n       [4.4, 2.9, 1.4, 0.2],\n       [4.9, 3.1, 1.5, 0.1],\n       [5.4, 3.7, 1.5, 0.2],\n       [4.8, 3.4, 1.6, 0.2],\n       [4.8, 3. , 1.4, 0.1],\n       [4.3, 3. , 1.1, 0.1],\n       [5.8, 4. , 1.2, 0.2],\n       [5.7, 4.4, 1.5, 0.4],\n       [5.4, 3.9, 1.3, 0.4],\n       [5.1, 3.5, 1.4, 0.3],\n       [5.7, 3.8, 1.7, 0.3],\n       [5.1, 3.8, 1.5, 0.3],\n       [5.4, 3.4, 1.7, 0.2],\n       [5.1, 3.7, 1.5, 0.4],\n       [4.6, 3.6, 1. , 0.2],\n       [5.1, 3.3, 1.7, 0.5],\n       [4.8, 3.4, 1.9, 0.2],\n       [5. , 3. , 1.6, 0.2],\n       [5. , 3.4, 1.6, 0.4],\n       [5.2, 3.5, 1.5, 0.2],\n       [5.2, 3.4, 1.4, 0.2],\n       [4.7, 3.2, 1.6, 0.2],\n       [4.8, 3.1, 1.6, 0.2],\n       [5.4, 3.4, 1.5, 0.4],\n       [5.2, 4.1, 1.5, 0.1],\n       [5.5, 4.2, 1.4, 0.2],\n       [4.9, 3.1, 1.5, 0.2],\n       [5. , 3.2, 1.2, 0.2],\n       [5.5, 3.5, 1.3, 0.2],\n       [4.9, 3.6, 1.4, 0.1],\n       [4.4, 3. , 1.3, 0.2],\n       [5.1, 3.4, 1.5, 0.2],\n       [5. , 3.5, 1.3, 0.3],\n       [4.5, 2.3, 1.3, 0.3],\n       [4.4, 3.2, 1.3, 0.2],\n       [5. , 3.5, 1.6, 0.6],\n       [5.1, 3.8, 1.9, 0.4],\n       [4.8, 3. , 1.4, 0.3],\n       [5.1, 3.8, 1.6, 0.2],\n       [4.6, 3.2, 1.4, 0.2],\n       [5.3, 3.7, 1.5, 0.2],\n       [5. , 3.3, 1.4, 0.2],\n       [7. , 3.2, 4.7, 1.4],\n       [6.4, 3.2, 4.5, 1.5],\n       [6.9, 3.1, 4.9, 1.5],\n       [5.5, 2.3, 4. , 1.3],\n       [6.5, 2.8, 4.6, 1.5],\n       [5.7, 2.8, 4.5, 1.3],\n       [6.3, 3.3, 4.7, 1.6],\n       [4.9, 2.4, 3.3, 1. ],\n       [6.6, 2.9, 4.6, 1.3],\n       [5.2, 2.7, 3.9, 1.4],\n       [5. , 2. , 3.5, 1. ],\n       [5.9, 3. , 4.2, 1.5],\n       [6. , 2.2, 4. , 1. ],\n       [6.1, 2.9, 4.7, 1.4],\n       [5.6, 2.9, 3.6, 1.3],\n       [6.7, 3.1, 4.4, 1.4],\n       [5.6, 3. , 4.5, 1.5],\n       [5.8, 2.7, 4.1, 1. ],\n       [6.2, 2.2, 4.5, 1.5],\n       [5.6, 2.5, 3.9, 1.1],\n       [5.9, 3.2, 4.8, 1.8],\n       [6.1, 2.8, 4. , 1.3],\n       [6.3, 2.5, 4.9, 1.5],\n       [6.1, 2.8, 4.7, 1.2],\n       [6.4, 2.9, 4.3, 1.3],\n       [6.6, 3. , 4.4, 1.4],\n       [6.8, 2.8, 4.8, 1.4],\n       [6.7, 3. , 5. , 1.7],\n       [6. , 2.9, 4.5, 1.5],\n       [5.7, 2.6, 3.5, 1. ],\n       [5.5, 2.4, 3.8, 1.1],\n       [5.5, 2.4, 3.7, 1. ],\n       [5.8, 2.7, 3.9, 1.2],\n       [6. , 2.7, 5.1, 1.6],\n       [5.4, 3. , 4.5, 1.5],\n       [6. , 3.4, 4.5, 1.6],\n       [6.7, 3.1, 4.7, 1.5],\n       [6.3, 2.3, 4.4, 1.3],\n       [5.6, 3. , 4.1, 1.3],\n       [5.5, 2.5, 4. , 1.3],\n       [5.5, 2.6, 4.4, 1.2],\n       [6.1, 3. , 4.6, 1.4],\n       [5.8, 2.6, 4. , 1.2],\n       [5. , 2.3, 3.3, 1. ],\n       [5.6, 2.7, 4.2, 1.3],\n       [5.7, 3. , 4.2, 1.2],\n       [5.7, 2.9, 4.2, 1.3],\n       [6.2, 2.9, 4.3, 1.3],\n       [5.1, 2.5, 3. , 1.1],\n       [5.7, 2.8, 4.1, 1.3],\n       [6.3, 3.3, 6. , 2.5],\n       [5.8, 2.7, 5.1, 1.9],\n       [7.1, 3. , 5.9, 2.1],\n       [6.3, 2.9, 5.6, 1.8],\n       [6.5, 3. , 5.8, 2.2],\n       [7.6, 3. , 6.6, 2.1],\n       [4.9, 2.5, 4.5, 1.7],\n       [7.3, 2.9, 6.3, 1.8],\n       [6.7, 2.5, 5.8, 1.8],\n       [7.2, 3.6, 6.1, 2.5],\n       [6.5, 3.2, 5.1, 2. ],\n       [6.4, 2.7, 5.3, 1.9],\n       [6.8, 3. , 5.5, 2.1],\n       [5.7, 2.5, 5. , 2. ],\n       [5.8, 2.8, 5.1, 2.4],\n       [6.4, 3.2, 5.3, 2.3],\n       [6.5, 3. , 5.5, 1.8],\n       [7.7, 3.8, 6.7, 2.2],\n       [7.7, 2.6, 6.9, 2.3],\n       [6. , 2.2, 5. , 1.5],\n       [6.9, 3.2, 5.7, 2.3],\n       [5.6, 2.8, 4.9, 2. ],\n       [7.7, 2.8, 6.7, 2. ],\n       [6.3, 2.7, 4.9, 1.8],\n       [6.7, 3.3, 5.7, 2.1],\n       [7.2, 3.2, 6. , 1.8],\n       [6.2, 2.8, 4.8, 1.8],\n       [6.1, 3. , 4.9, 1.8],\n       [6.4, 2.8, 5.6, 2.1],\n       [7.2, 3. , 5.8, 1.6],\n       [7.4, 2.8, 6.1, 1.9],\n       [7.9, 3.8, 6.4, 2. ],\n       [6.4, 2.8, 5.6, 2.2],\n       [6.3, 2.8, 5.1, 1.5],\n       [6.1, 2.6, 5.6, 1.4],\n       [7.7, 3. , 6.1, 2.3],\n       [6.3, 3.4, 5.6, 2.4],\n       [6.4, 3.1, 5.5, 1.8],\n       [6. , 3. , 4.8, 1.8],\n       [6.9, 3.1, 5.4, 2.1],\n       [6.7, 3.1, 5.6, 2.4],\n       [6.9, 3.1, 5.1, 2.3],\n       [5.8, 2.7, 5.1, 1.9],\n       [6.8, 3.2, 5.9, 2.3],\n       [6.7, 3.3, 5.7, 2.5],\n       [6.7, 3. , 5.2, 2.3],\n       [6.3, 2.5, 5. , 1.9],\n       [6.5, 3. , 5.2, 2. ],\n       [6.2, 3.4, 5.4, 2.3],\n       [5.9, 3. , 5.1, 1.8]])\n\n\n\nimport pandas as pd\nimport numpy as np\n\n# create DataFrame\ndf = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n                  columns= iris['feature_names'] + ['target'])\n\n\n# show last\ndf.tail(10)\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n140\n6.7\n3.1\n5.6\n2.4\n2.0\n\n\n141\n6.9\n3.1\n5.1\n2.3\n2.0\n\n\n142\n5.8\n2.7\n5.1\n1.9\n2.0\n\n\n143\n6.8\n3.2\n5.9\n2.3\n2.0\n\n\n144\n6.7\n3.3\n5.7\n2.5\n2.0\n\n\n145\n6.7\n3.0\n5.2\n2.3\n2.0\n\n\n146\n6.3\n2.5\n5.0\n1.9\n2.0\n\n\n147\n6.5\n3.0\n5.2\n2.0\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n2.0\n\n\n149\n5.9\n3.0\n5.1\n1.8\n2.0\n\n\n\n\n\n\n\n\n# show info about NaN values and a type of each column.\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 5 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   sepal length (cm)  150 non-null    float64\n 1   sepal width (cm)   150 non-null    float64\n 2   petal length (cm)  150 non-null    float64\n 3   petal width (cm)   150 non-null    float64\n 4   target             150 non-null    float64\ndtypes: float64(5)\nmemory usage: 6.0 KB\n\n\n\n# statistics\ndf.describe()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n1.000000\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n0.819232\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n0.000000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n0.000000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n1.000000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n2.000000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n2.000000\n\n\n\n\n\n\n\n\n# new features\ndf['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n\n\n# remove features (columns) \ndf = df.drop(columns=['target'])\n# filtering first 100 rows and 4'th column\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"whitegrid\", palette=\"husl\")\n\niris_melt = pd.melt(df, \"species\", var_name=\"measurement\")\nf, ax = plt.subplots(1, figsize=(15,9))\nsns.stripplot(x=\"measurement\", y=\"value\", hue=\"species\", data=iris_melt, jitter=True, edgecolor=\"white\", ax=ax)\n\n\n\n\n\n\n\n\n\nX = df.iloc[:100,[0,2]].values\ny = df.iloc[0:100,4].values\n\n\ny = np.where(y == 'setosa',-1,1)\n\n\nplt.scatter(X[:50,0],X[:50,1],color='red', marker='o',label='setosa')\nplt.scatter(X[50:100,0],X[50:100,1],color='blue', marker='x',label='versicolor')\nplt.xlabel('sepal length (cm)')\nplt.ylabel('petal length (cm)')\nplt.legend(loc='upper left')\nplt.show()\n\n\n\n\n\n\n\n\nFor this type of linearly separable data, use logistic regression model or neural network.\n\nfrom sklearn.linear_model import Perceptron\n\nper_clf = Perceptron()\nper_clf.fit(X,y)\n\ny_pred = per_clf.predict([[2, 0.5],[4,5.5]])\ny_pred\n\narray([-1,  1])",
    "crumbs": [
      "Home",
      "Labs",
      "Python codes",
      "Structured data"
    ]
  },
  {
    "objectID": "labs/lab3.html#data-storage-and-connection-to-a-simple-sql-database",
    "href": "labs/lab3.html#data-storage-and-connection-to-a-simple-sql-database",
    "title": "Structured data",
    "section": "Data Storage and Connection to a Simple SQL Database",
    "text": "Data Storage and Connection to a Simple SQL Database\n\nIRIS_PATH = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\ncol_names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\ndf = pd.read_csv(IRIS_PATH, names=col_names)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nclass\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n\n\n\n\n\n\n\n\n# save to sqlite\nimport sqlite3\n# generate database\nconn = sqlite3.connect(\"iris.db\")\n# pandas to_sql\n\ntry:\n    df.to_sql(\"iris\", conn, index=False)\nexcept:\n    print(\"tabela ju≈º istnieje\")\n\n\n# sql to pandas\nresult = pd.read_sql(\"SELECT * FROM iris WHERE sepal_length &gt; 5\", conn)\n\n\nresult.head(3)\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nclass\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n1\n5.4\n3.9\n1.7\n0.4\nIris-setosa\n\n\n2\n5.4\n3.7\n1.5\n0.2\nIris-setosa\n\n\n\n\n\n\n\n\n# Artificial data\nfrom sklearn import datasets\nX, y = datasets.make_classification(n_samples=10**4,\nn_features=20, n_informative=2, n_redundant=2)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# train test split by heand\ntrain_samples = 7000 # 70% \n\nX_train = X[:train_samples]\nX_test = X[train_samples:]\ny_train = y[:train_samples]\ny_test = y[train_samples:]\n\nrfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier() \n\n\n\nrfc.predict(X_train[0].reshape(1, -1))\n\narray([1])",
    "crumbs": [
      "Home",
      "Labs",
      "Python codes",
      "Structured data"
    ]
  },
  {
    "objectID": "labs/lab3.html#zadania",
    "href": "labs/lab3.html#zadania",
    "title": "Structured data",
    "section": "ZADANIA",
    "text": "ZADANIA\n\nLoad data from the train.csv file and put it into the panda‚Äôs data frame\n\n\n## YOUR CODE HERE\ndf = \n\n\nShow number of row and number of columns\n\n\n## YOUR CODE HERE\n\nPerform missing data handling:\n\nOption 1 - remove rows containing missing data (dropna())\nOption 2 - remove columns containing missing data (drop())\nOption 3 - perform imputation using mean values (fillna())\n\nWhich columns did you choose for each option and why?\n\n## YOUR CODE HERE\n\n\nUsing the nunique() method, remove columns that are not suitable for modeling.\n\n\n## YOUR CODE HERE\n\n\nConvert categorical variables using LabelEncoder into numerical form.\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n## YOUR CODE HERE\n\n\nUtilize MinMaxScaler to transform floating-point data to a common scale\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n## YOUR CODE HERE\n\n\nSplit the data into training set (80%) and test set (20%)\n\n\nfrom sklearn.model_selection import train_test_split\n## YOUR CODE HERE\nX_train, X_test, y_train, y_test = train_test_split(...., random_state=44)\n\n\nUsing mapping, you can classify each passenger. The run() function requires providing a classifier for a single case.\n\nWrite a classifier that assigns a value of 0 or 1 randomly (you can use the random.randint(0,1) function).\nExecute the evaluate() function and check how well the random classifier performs.‚Äù\n\n\n\nclassify = ...\n\n\ndef run(f_classify, x):\n    return list(map(f_classify, x))\n\ndef evaluate(predictions, actual):\n    correct = list(filter(\n        lambda item: item[0] == item[1],\n        list(zip(predictions, actual))\n    ))\n    return f\"{len(correct)} correct answers from {len(actual)}. Accuracy ({len(correct)/len(actual)*100:.0f}%)\"\n\n\nevaluate(run(classify, X_train.values), y_train.values)",
    "crumbs": [
      "Home",
      "Labs",
      "Python codes",
      "Structured data"
    ]
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Books and WWW pages",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\n\n\n\n\n\nA. Bellemare Mikrous≈Çugi oparte na zdarzeniach. Wykorzystanie danych w organizacji na du≈ºƒÖ skalƒô Zobacz opis lub Kup\nG. Shapira, T. Palino, R. Sivaram, K. Petty Kafka The Definitive Guide. Real-time data and stream processing at scale. Second edition, 2021. O‚ÄôREILLY.\nJ. Korstanje Machine Learning for Streaming Data with Python, 2022. PACKT\n\n\n\n\n\nA. Geron Uczenie maszynowe z u≈ºyciem Scikit-Learn i TensorFlow. Wydanie II. Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book.\nSebastian Raschka Python. Uczenie maszynowe. Wydanie II. Zobacz opis lub Kup ksiƒÖ≈ºkƒô.\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocƒÖ pakiet√≥w Pandas i NumPy oraz ≈õrodowiska IPython. Wydanie II Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieƒá. Algorytmy przysz≈Ço≈õci. Wydanie II (ebook) Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup ksiƒÖ≈ºkƒô.\nR. Schutt, C. O‚ÄôNeil Badanie danych. Raport z pierwszej lini dzia≈Ça≈Ñ. Zobacz opis lub Kup ksiƒÖ≈ºkƒô.\nT. Segaran Nowe us≈Çugi 2.0. Przewodnik po analizie zbior√≥w danych Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\nT. Morzy Eksploracja Danych. Metody i algorytmy, PWN, 2013.\nKrzy≈õko, Wo≈Çy≈Ñski, G√≥recki, Skorzybut, Systemy uczƒÖce siƒô . WNT, 2008\n\n\n\n\n\nF. Chollet Deep Learning. Praca z jƒôzykiem Python i bibliotekƒÖ Keras. Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie g≈Çƒôbokie z jƒôzykiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\nS. Weidman Uczenie g≈Çƒôbokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyƒá komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programist√≥w. Budowanie aplikacji AI za pomocƒÖ fastai i PyTorch Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\n\n\n\n\n\nSpark. Zaawansowana analiza danych (ebook) Zobacz opis lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\nJ. Quddus Machine Learning with Apache Spark Quick Start Guide (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind Zrozumieƒá programowanie Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\nA. Allain C++. Przewodnik dla poczƒÖtkujƒÖcych Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdra≈ºanie aplikacji Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zada≈Ñ z pythonem. Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzƒôdzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuka tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage.\nL. Suskind, Mechanika kwantowa, teoretyczne minimum, 2014, Pr√≥szy≈Ñski i s-ka"
  },
  {
    "objectID": "books.html#books",
    "href": "books.html#books",
    "title": "Books and WWW pages",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\n\n\n\n\n\nA. Bellemare Mikrous≈Çugi oparte na zdarzeniach. Wykorzystanie danych w organizacji na du≈ºƒÖ skalƒô Zobacz opis lub Kup\nG. Shapira, T. Palino, R. Sivaram, K. Petty Kafka The Definitive Guide. Real-time data and stream processing at scale. Second edition, 2021. O‚ÄôREILLY.\nJ. Korstanje Machine Learning for Streaming Data with Python, 2022. PACKT\n\n\n\n\n\nA. Geron Uczenie maszynowe z u≈ºyciem Scikit-Learn i TensorFlow. Wydanie II. Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book.\nSebastian Raschka Python. Uczenie maszynowe. Wydanie II. Zobacz opis lub Kup ksiƒÖ≈ºkƒô.\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocƒÖ pakiet√≥w Pandas i NumPy oraz ≈õrodowiska IPython. Wydanie II Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieƒá. Algorytmy przysz≈Ço≈õci. Wydanie II (ebook) Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup ksiƒÖ≈ºkƒô.\nR. Schutt, C. O‚ÄôNeil Badanie danych. Raport z pierwszej lini dzia≈Ça≈Ñ. Zobacz opis lub Kup ksiƒÖ≈ºkƒô.\nT. Segaran Nowe us≈Çugi 2.0. Przewodnik po analizie zbior√≥w danych Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\nT. Morzy Eksploracja Danych. Metody i algorytmy, PWN, 2013.\nKrzy≈õko, Wo≈Çy≈Ñski, G√≥recki, Skorzybut, Systemy uczƒÖce siƒô . WNT, 2008\n\n\n\n\n\nF. Chollet Deep Learning. Praca z jƒôzykiem Python i bibliotekƒÖ Keras. Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie g≈Çƒôbokie z jƒôzykiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\nS. Weidman Uczenie g≈Çƒôbokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyƒá komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programist√≥w. Budowanie aplikacji AI za pomocƒÖ fastai i PyTorch Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\n\n\n\n\n\nSpark. Zaawansowana analiza danych (ebook) Zobacz opis lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\nJ. Quddus Machine Learning with Apache Spark Quick Start Guide (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind Zrozumieƒá programowanie Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\nA. Allain C++. Przewodnik dla poczƒÖtkujƒÖcych Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdra≈ºanie aplikacji Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zada≈Ñ z pythonem. Zobacz opis lub Kup ksiƒÖ≈ºkƒô, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzƒôdzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuka tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage.\nL. Suskind, Mechanika kwantowa, teoretyczne minimum, 2014, Pr√≥szy≈Ñski i s-ka"
  },
  {
    "objectID": "books.html#www-pages",
    "href": "books.html#www-pages",
    "title": "Books and WWW pages",
    "section": "WWW Pages",
    "text": "WWW Pages\n\nSoftware\n\nGithub\nGit-instrukcja\nwww.python.org\nPyPI python libraries\nAnaconda\nDocker\n\n\n\nPython libraries for data analysis\n\nNumPy\nSciPy\nPandas\nScikit-learn\nJupyter\nMatplotlib\nBeautiful Soup\nTheano\nKeras\nTensorFlow\nVirtual ENV\n\n\n\nText editors\n\nNotepad++\nSublime Text\nVisual Studio Code\n\n\n\nMarkdown\n\nMD\n\n\n\nJupyter notebook\n\nGaleria ciekawych notatnik√≥w\nIntro\nKernels\nBringing the best out of jupyter for data science\nJupyter extensions\nI don‚Äôt like notebooks\nJupyter lab\nSpeed up jupyter notebook\n\n\n\nETL\n\ndata cookbook\n\n\n\nDatasets\n\nInternet Archive\nReddit\nKDnuggets\nKaggle\nList of datasets for machine learning research\nUCI Machine Learning Repo\nPublic API\nGoogle Datatset Search\n\n\n\nPython\n\nChris Albon Technical Notes on Using Data Science & AI\n40+ Python Statistics For Data Science Resources\nPractical Business Python\n\n\n\nML course\n\nKurs Machine Learning - Andrew Ng, Stanford"
  },
  {
    "objectID": "info.html",
    "href": "info.html",
    "title": "All tools",
    "section": "",
    "text": "For our first a few laboratories we will use just python codes. Check what is Your Python3 environment.\nIn the terminal try first:\npython\n# and\npython3\nI have python3 (You shouldn‚Äôt use python 2.7 version) so i create a new and a clear python environment.\nThe easiest way how to run a JupyterLab with your new python env. For  You can choose what You want.\npython3 -m venv &lt;name of Your env&gt;\n\nsource &lt;name of your env&gt;/bin/activate\n# . env/bin/activate\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# or\npip install -r requirements.txt\n\njupyterlab\ngo to web browser: localhost:8888\nIf You want rerun jupyterlab (after computer reset) just go to Your folder and run:\nsource &lt;name of your env&gt;/bin/activate\njupyterlab"
  },
  {
    "objectID": "info.html#python-env-with-jupyter-lab",
    "href": "info.html#python-env-with-jupyter-lab",
    "title": "All tools",
    "section": "",
    "text": "For our first a few laboratories we will use just python codes. Check what is Your Python3 environment.\nIn the terminal try first:\npython\n# and\npython3\nI have python3 (You shouldn‚Äôt use python 2.7 version) so i create a new and a clear python environment.\nThe easiest way how to run a JupyterLab with your new python env. For  You can choose what You want.\npython3 -m venv &lt;name of Your env&gt;\n\nsource &lt;name of your env&gt;/bin/activate\n# . env/bin/activate\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# or\npip install -r requirements.txt\n\njupyterlab\ngo to web browser: localhost:8888\nIf You want rerun jupyterlab (after computer reset) just go to Your folder and run:\nsource &lt;name of your env&gt;/bin/activate\njupyterlab"
  },
  {
    "objectID": "info.html#python-env-with-jupyterlab-docker-version",
    "href": "info.html#python-env-with-jupyterlab-docker-version",
    "title": "All tools",
    "section": "Python env with JupyterLAB Docker Version",
    "text": "Python env with JupyterLAB Docker Version\n\nCookiecutter project\nFrom GitHub repository You can find how to use a cookiecutter for any data science project or other kind of programs.\nTo run and build full dockerfile project: Create python env and install cookiecutter library.\npython3 -m venv venv\nsource venv/bin/activate\npip --no-cache install --upgrade pip setuptools\npip install cookiecutter\nand run:\ncookiecutter https://github.com/sebkaz/jupyterlab-project\nYou can run a cookiecutter project directly from GitHub repo.\nAnswer questions:\ncd jupyterlab\ndocker-compose up -d --build\nTo stop:\ndocker-compose down\n\n\nCookiecutter with config yaml file\n\nPython, Julia, R\nAll + Apache Spark\n\nClone repo and run:\npython3 -m cookiecutter https://github.com/sebkaz/jupyterlab-project --no-input --config-file=spark_template.yml --overwrite-if-exists"
  },
  {
    "objectID": "info.html#start-with-github",
    "href": "info.html#start-with-github",
    "title": "All tools",
    "section": "Start with GitHub",
    "text": "Start with GitHub\nText from web site\nWhen You working on a project, e.g.¬†a master‚Äôs thesis, (alone or in a team) you often need to check what changes, when and by whom were introduced to the project. The ‚Äúversion control system‚Äù or GIT works great for this task.\nYou can download and install Git like a regular program on any computer. However, most often (small projects) you use websites with some kind of git system. One of the most recognized is GitHub (www.github.com) which allows you to use the git system without installing it on your computer.\nIn the free version of the GitHub website, you can store your files in public (everyone has access) repositories. We will only focus on the free version of GitHub:\ngit --version"
  },
  {
    "objectID": "info.html#github",
    "href": "info.html#github",
    "title": "All tools",
    "section": "GitHub",
    "text": "GitHub\nAt the highest level, there are individual accounts (eg. http://github.com/sebkaz or those set up by organizations. Individual users can create ** repositories ** public (public) or private (private).\nOne file should not exceed 100 MB.\nRepo (shortcut to repository) is created with Create a new repository. Each repo should have an individual name.\n\nBranches\nThe main (created by default) branch of the repository is named master.\n\n\nMost important commends\n\nclone of Your repository\n\ngit clone https://adres_repo.git\n\nIn github case, you can download the repository as a ‚Äòzip‚Äô file.\n\n\nRepository for local directory\n\n# new directory\nmkdir datamining\ncd datamining\n# init repo\ngit init\n# there sould be a .git new directory\n# add file\necho \"Info \" &gt;&gt; README.md\n\nlocal and web version connection\n\ngit remote add origin https://github.com/&lt;twojGit&gt;/nazwa.git\n\n3 steps\n\n# status check\ngit status\n# 1. add all changes\ngit add .\n# 2. commit all changes with message\ngit commit -m \" message \"\n# 3. and\ngit push origin master\nYou can watch Youtube course.\nAll the necessary programs will be delivered in the form of docker containers."
  },
  {
    "objectID": "info.html#start-with-docker",
    "href": "info.html#start-with-docker",
    "title": "All tools",
    "section": "Start with Docker",
    "text": "Start with Docker\nIn order to download the docer software to your system, go to the page.\nIf everything is installed correctly, follow these instructions:\n\nCheck the installed version\n\ndocker --version\n\nDownload and run the image Hello World and\n\ndocker run hello-world\n\nOverview of downloaded images:\n\ndocker image ls\n\ndocker images\n\nOverview of running containers:\n\ndocker ps \n\ndocker ps -all\n\nStopping a running container:\n\ndocker stop &lt;CONTAINER ID&gt;\n\nContainer removal\n\ndocker rm -f &lt;CONTAINER ID&gt;\nI also recommend short intro"
  },
  {
    "objectID": "info.html#docker-as-an-application-continuation-tool",
    "href": "info.html#docker-as-an-application-continuation-tool",
    "title": "All tools",
    "section": "Docker as an application continuation tool",
    "text": "Docker as an application continuation tool\nDocker with jupyter notebook"
  },
  {
    "objectID": "lectures/lecture1.html",
    "href": "lectures/lecture1.html",
    "title": "Lecture 1",
    "section": "",
    "text": "‚è≥ Duration: 1.5h\nüéØ Lecture Goal\nIntroducing students to the fundamentals of real-time analytics, the differences between data processing modes (batch, streaming, real-time), as well as key applications and challenges.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 1"
    ]
  },
  {
    "objectID": "lectures/lecture1.html#what-is-real-time-data-analytics",
    "href": "lectures/lecture1.html#what-is-real-time-data-analytics",
    "title": "Lecture 1",
    "section": "What is Real-Time Data Analytics?",
    "text": "What is Real-Time Data Analytics?\n\nDefinition and Key Concepts\nReal-Time Data Analytics is the process of processing and analyzing data immediately after it is generated, without the need for storage and later processing. The goal is to obtain instant insights and responses to changing conditions in business, technology, and scientific systems.\n\n\nKey Features of Real-Time Data Analytics:\n\nLow latency ‚Äì data is analyzed within milliseconds or seconds of being generated.\nStreaming vs.¬†Batch Processing ‚Äì data analysis can occur continuously (streaming) or at predefined intervals (batch).\nIntegration with IoT, AI, and ML ‚Äì real-time analytics often works in conjunction with the Internet of Things (IoT) and artificial intelligence algorithms.\nReal-time decision-making ‚Äì e.g., instant fraud detection in banking transactions.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 1"
    ]
  },
  {
    "objectID": "lectures/lecture1.html#business-applications-of-real-time-data-analytics",
    "href": "lectures/lecture1.html#business-applications-of-real-time-data-analytics",
    "title": "Lecture 1",
    "section": "Business Applications of Real-Time Data Analytics",
    "text": "Business Applications of Real-Time Data Analytics\n\nFinance and Banking\n\nFraud Detection ‚Äì real-time transaction analysis helps identify anomalies indicating fraud.\nAutomated Trading ‚Äì HFT (High-Frequency Trading) systems analyze millions of data points in fractions of a second.\nDynamic Credit Scoring ‚Äì instant risk assessment of a customer‚Äôs creditworthiness.\n\n\n\nE-Commerce and Digital Marketing\n\nReal-Time Offer Personalization ‚Äì dynamic product recommendations based on users‚Äô current behavior.\nDynamic Pricing ‚Äì companies like Uber, Amazon, and hotels adjust prices in real time based on demand.\nSocial Media Monitoring ‚Äì sentiment analysis of customer feedback and immediate response to negative comments.\n\n\n\nTelecommunications and IoT\n\nNetwork Infrastructure Monitoring ‚Äì real-time log analysis helps detect failures before they occur.\nSmart Cities ‚Äì real-time traffic analysis optimizes traffic light systems dynamically.\nIoT Analytics ‚Äì IoT devices generate data streams that can be analyzed in real time (e.g., smart energy meters).\n\n\n\nHealthcare\n\nPatient Monitoring ‚Äì real-time analysis of medical device signals to detect life-threatening conditions instantly.\nEpidemiological Analytics ‚Äì tracking disease outbreaks based on real-time data.\n\nReal-time data analytics is a key component of modern IT systems, enabling businesses to make faster and more precise decisions. It is widely used across industries‚Äîfrom finance and e-commerce to healthcare and IoT.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 1"
    ]
  },
  {
    "objectID": "lectures/lecture1.html#differences-between-batch-processing-near-real-time-analytics-and-real-time-analytics",
    "href": "lectures/lecture1.html#differences-between-batch-processing-near-real-time-analytics-and-real-time-analytics",
    "title": "Lecture 1",
    "section": "Differences Between Batch Processing, Near Real-Time Analytics, and Real-Time Analytics",
    "text": "Differences Between Batch Processing, Near Real-Time Analytics, and Real-Time Analytics\nThere are three main approaches to data processing:\n\nBatch Processing\n\nNear Real-Time Analytics\n\nReal-Time Analytics\n\nEach differs in processing speed, technological requirements, and business applications.\n\nBatch Processing\nüìå Definition:\nBatch Processing involves collecting large amounts of data and processing them at scheduled intervals (e.g., hourly, daily, or weekly).\nüìå Characteristics:\n\n‚úÖ High efficiency for large datasets\n\n‚úÖ Processes data after it has been collected\n\n‚úÖ Does not require immediate analysis\n\n‚úÖ Typically cheaper than real-time processing\n\n‚ùå Delays ‚Äì results are available only after processing is complete\n\nüìå Use Cases:\n\nGenerating financial reports at the end of a day/month\n\nAnalyzing sales trends based on historical data\n\nCreating offline machine learning models\n\nüìå Example Technologies:\n\nHadoop MapReduce\n\nApache Spark (in batch mode)\n\nGoogle BigQuery\n\nimport pandas as pd  \ndf = pd.read_csv(\"transactions.csv\")  \n\ndf['transaction_date'] = pd.to_datetime(df['transaction_date'])\ndf['month'] = df['transaction_date'].dt.to_period('M') \n\n# Agg\nmonthly_sales = df.groupby(['month'])['amount'].sum()\n\nmonthly_sales.to_csv(\"monthly_report.csv\")  \n\nprint(\"Raport save!\")\nIf you wanted to create data for an example.\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)\ndata = {\n    'transaction_id': [f'TX{str(i).zfill(4)}' for i in range(1, 1001)],\n    'amount': np.random.uniform(10, 10000, 1000), \n    'transaction_date': pd.date_range(start=\"2025-01-01\", periods=1000, freq='h'), \n    'merchant': np.random.choice(['Merchant_A', 'Merchant_B', 'Merchant_C', 'Merchant_D'], 1000),\n    'card_type': np.random.choice(['Visa', 'MasterCard', 'AmEx'], 1000)\n}\n\ndf = pd.DataFrame(data)\ncsv_file = 'transactions.csv'\ndf.to_csv(csv_file, index=False)",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 1"
    ]
  },
  {
    "objectID": "lectures/lecture1.html#near-real-time-analytics-analysis-nearly-in-real-time",
    "href": "lectures/lecture1.html#near-real-time-analytics-analysis-nearly-in-real-time",
    "title": "Lecture 1",
    "section": "Near Real-Time Analytics ‚Äì Analysis Nearly in Real Time",
    "text": "Near Real-Time Analytics ‚Äì Analysis Nearly in Real Time\nüìå Definition:\nNear Real-Time Analytics refers to data analysis that occurs with minimal delay (typically from a few seconds to a few minutes). It is used in scenarios where full real-time analysis is not necessary, but excessive delays could impact business operations.\nüìå Characteristics:\n\n‚úÖ Processes data in short intervals (a few seconds to minutes)\n\n‚úÖ Enables quick decision-making but does not require millisecond-level reactions\n\n‚úÖ Optimal balance between cost and speed\n\n‚ùå Not suitable for systems requiring immediate response\n\nüìå Use Cases:\n\nMonitoring banking transactions and detecting fraud (e.g., analysis within 30 seconds)\n\nDynamically adjusting online ads based on user behavior\n\nAnalyzing server and network logs to detect anomalies\n\nüìå Example Technologies:\n\nApache Kafka + Spark Streaming\n\nElasticsearch + Kibana (e.g., IT log analysis)\n\nAmazon Kinesis\n\nExample of a data producer sending transactions to an Apache Kafka system.\nfrom kafka import KafkaProducer\nimport json\nimport random\nimport time\nfrom datetime import datetime\n\n# Ustawienia dla producenta\nbootstrap_servers = 'localhost:9092'\ntopic = 'transactions' \n\n\ndef generate_transaction():\n    transaction = {\n        'transaction_id': f'TX{random.randint(1000, 9999)}',\n        'amount': round(random.uniform(10, 10000), 2),  \n        'transaction_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        'merchant': random.choice(['Merchant_A', 'Merchant_B', 'Merchant_C', 'Merchant_D']),\n        'card_type': random.choice(['Visa', 'MasterCard', 'AmEx']),\n    }\n    return transaction\n\nproducer = KafkaProducer(\n    bootstrap_servers=bootstrap_servers,\n    value_serializer=lambda v: json.dumps(v).encode('utf-8') \n)\n\n\nfor _ in range(1000):  \n    transaction = generate_transaction()\n    producer.send(topic, value=transaction) \n    print(f\"Sent: {transaction}\")\n    time.sleep(1) \n\nproducer.flush()\nproducer.close()\nConsument example\nfrom kafka import KafkaConsumer\nimport json  \n\nconsumer = KafkaConsumer(\n    'transactions',\n    bootstrap_servers='localhost:9092',\n    auto_offset_reset='earliest',\n    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n)\n\n\nfor message in consumer:\n    transaction = message.value\n    if transaction[\"amount\"] &gt; 8000:\n        print(f\"üö® Anomaly transaction: {transaction}\")\nExample of the DataSet\n{\n    \"transaction_id\": \"TX1234\",\n    \"amount\": 523.47,\n    \"transaction_date\": \"2025-02-11 08:10:45\",\n    \"merchant\": \"Merchant_A\",\n    \"card_type\": \"Visa\"\n}\n\nReal-Time Analytics\nüìå Definition:\nReal-Time Analytics refers to the immediate analysis of data and decision-making within fractions of a second (milliseconds to one second). It is used in systems requiring real-time responses, such as stock market transactions, IoT systems, and cybersecurity.\nüìå Characteristics:\n\n‚úÖ Extremely low latency (milliseconds to seconds)\n\n‚úÖ Enables instant system response\n\n‚úÖ Requires high computational power and scalable architecture\n\n‚ùå More expensive and technologically complex than batch processing\n\nüìå Use Cases:\n\nHigh-Frequency Trading (HFT) ‚Äì making stock market transaction decisions within milliseconds\n\nAutonomous Vehicles ‚Äì real-time analysis of data streams from cameras and sensors\n\nCybersecurity ‚Äì detecting network attacks in fractions of a second\n\nIoT Analytics ‚Äì instant anomaly detection in industrial sensor data\n\nüìå Example Technologies:\n\nApache Flink\n\nApache Storm\n\nGoogle Dataflow\n\nüîé Comparison:\n\n\n\n\n\n\n\n\n\nFeature\nBatch Processing\nNear Real-Time Analytics\nReal-Time Analytics\n\n\n\n\nLatency\nMinutes ‚Äì hours ‚Äì days\nSeconds ‚Äì minutes\nMilliseconds ‚Äì seconds\n\n\nProcessing Type\nBatch (offline)\nStreaming (but not fully immediate)\nStreaming (true real-time)\n\n\nInfrastructure Cost\nüìâ Low\nüìà Medium\nüìàüìà High\n\n\nImplementation Complexity\nüìâ Simple\nüìà Medium\nüìàüìà Difficult\n\n\nUse Cases\nReports, offline ML, historical analysis\nTransaction monitoring, dynamic ads\nHFT, IoT, real-time fraud detection\n\n\n\nüìå When to Use Batch Processing?\n\n‚úÖ When immediate analysis is not required\n\n‚úÖ When handling large volumes of data processed periodically\n\n‚úÖ When aiming to reduce costs\n\nüìå When to Use Near Real-Time Analytics?\n\n‚úÖ When analysis is needed within a short time (seconds ‚Äì minutes)\n\n‚úÖ When fresher data is required but not full real-time processing\n\n‚úÖ When seeking a balance between performance and cost\n\nüìå When to Use Real-Time Analytics?\n\n‚úÖ When every millisecond matters (e.g., stock trading, autonomous vehicles)\n\n‚úÖ When detecting fraud, anomalies, or incidents instantly\n\n‚úÖ When a system must respond to events immediately\n\nReal-time analytics is not always necessary‚Äîoften, near real-time is sufficient and more cost-effective. The key is to understand business requirements before choosing the right solution.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 1"
    ]
  },
  {
    "objectID": "lectures/lecture1.html#why-is-real-time-analytics-important",
    "href": "lectures/lecture1.html#why-is-real-time-analytics-important",
    "title": "Lecture 1",
    "section": "Why is Real-Time Analytics Important?",
    "text": "Why is Real-Time Analytics Important?\nReal-time analytics is becoming increasingly crucial across various industries as it enables organizations to make immediate decisions based on up-to-date data. Here are some key reasons why real-time analytics matters:\n\nFaster Decision-Making\nReal-time analytics allows businesses to react to changes and events instantly. This is essential in dynamic environments such as:\n\nMarketing: Ads can be adjusted in real time based on user behavior (e.g., personalized content recommendations).\n\nFinance: Fraud detection in real-time, where every minute counts in preventing financial losses.\n\n\n\nReal-Time Monitoring\nCompanies can continuously track key operational metrics. Examples:\n\nIoT (Internet of Things): Monitoring the condition of machines and equipment in factories to detect failures and prevent downtime.\n\nHealthtech: Tracking patients‚Äô vital signs and detecting anomalies, which can save lives.\n\n\n\nImproved Operational Efficiency\nReal-time analytics helps identify and address operational issues before they escalate. Examples:\n\nLogistics: Tracking shipments and monitoring transport status in real time to improve efficiency and reduce delays.\n\nRetail: Monitoring inventory levels in real-time and adjusting orders accordingly.\n\n\n\nCompetitive Advantage\nOrganizations leveraging real-time analytics gain an edge by responding faster to market changes, customer needs, and crises. With real-time insights:\n\nBusinesses can make proactive decisions ahead of competitors.\n\nCompanies can enhance customer relationships by responding instantly to their needs (e.g., adjusting offerings dynamically).\n\n\n\nEnhanced User Experience (Customer Experience)\nReal-time data analysis enables personalized user interactions as they happen. Examples:\n\nE-commerce: Analyzing shopping cart behavior in real time to offer discounts or remind users of abandoned items.\n\nStreaming Services: Optimizing video/streaming quality based on available bandwidth.\n\n\n\nAnomaly Detection and Threat Prevention\nIn today‚Äôs data-driven world, real-time anomaly detection is critical for security. Examples:\n\nCybersecurity: Detecting suspicious network activities and preventing attacks in real time (e.g., DDoS attacks, unauthorized logins).\n\nFraud Prevention: Instant identification of suspicious transactions in banking and credit card systems.\n\n\n\nCost Optimization\nReal-time analytics helps optimize resources and reduce costs. Examples:\n\nEnergy Management: Monitoring energy consumption in real time to optimize corporate energy expenses.\n\nSupply Chain Optimization: Tracking inventory and deliveries to reduce storage and transportation costs.\n\n\n\nPredictive Capabilities\nReal-time analytics supports predictive processes that anticipate future behaviors or problems and address them before they occur. Examples:\n\nPredictive Maintenance: Combining real-time data with predictive models to foresee machine failures.\n\nDemand Forecasting: Adjusting production or stock levels based on live market trends.\n\nReal-time analytics is not just about data analysis‚Äîit is a crucial element of business strategy in a world that demands agility, flexibility, and rapid adaptation. Companies that implement these technologies can significantly enhance their financial performance, customer service, operational efficiency, and competitive advantage.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 1"
    ]
  },
  {
    "objectID": "lectures/lecture2.html",
    "href": "lectures/lecture2.html",
    "title": "Lecture 2",
    "section": "",
    "text": "‚è≥ Duration: 1.5h\nüéØ Lecture Objective\nUnderstanding how data has evolved in different industries and the tools used for its analysis today.\nIn this lecture, we will present the evolution of data analysis, showing how technologies and approaches to data processing have changed over the years.\nWe will start with classical tabular structures, move through more advanced graph and text models, and finish with modern approaches to stream processing.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 2"
    ]
  },
  {
    "objectID": "lectures/lecture2.html#tabular-data-sql-tables",
    "href": "lectures/lecture2.html#tabular-data-sql-tables",
    "title": "Lecture 2",
    "section": "1. Tabular Data (SQL Tables)",
    "text": "1. Tabular Data (SQL Tables)\nInitially, data was stored in tables, where each table contained organized information in columns and rows (e.g., SQL databases).\nSuch models were perfect for structured data.\n\nüìå Features:\n‚úÖ Data divided into columns with a fixed structure.\n‚úÖ CRUD operations (Create, Read, Update, Delete) can be applied.\n‚úÖ Strict consistency and normalization rules.\n\n\nüìå Examples:\n‚û°Ô∏è Banking systems, e-commerce, ERP, CRM systems.\n\n\nüñ•Ô∏è Example Python Code (SQLite):\nimport sqlite3\nconn = sqlite3.connect(':memory:')\ncursor = conn.cursor()\ncursor.execute(\"CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT, age INTEGER)\")\ncursor.execute(\"INSERT INTO users (name, age) VALUES ('Alice', 30)\")\ncursor.execute(\"SELECT * FROM users\")\nprint(cursor.fetchall())\nconn.close()",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 2"
    ]
  },
  {
    "objectID": "lectures/lecture2.html#graph-data",
    "href": "lectures/lecture2.html#graph-data",
    "title": "Lecture 2",
    "section": "2. Graph Data",
    "text": "2. Graph Data\nAs business needs grew, graph data emerged, where relationships between objects are represented as nodes and edges.\n\nüìå Features:\n‚úÖ Data describing relationships and connections. ‚úÖ Flexible structure (graphs instead of tables). ‚úÖ Allows analysis of connections (e.g., PageRank algorithms, centrality).\n\n\nüìå Examples:\n‚û°Ô∏è Social networks (Facebook, LinkedIn), search engines (Google), recommendation systems (Netflix, Amazon).\n\n\nüñ•Ô∏è Example Python Code (Karate Graph - NetworkX) :\n\n\nCode\nimport networkx as nx\nG = nx.karate_club_graph()\nnx.draw(G, with_labels=True)",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 2"
    ]
  },
  {
    "objectID": "lectures/lecture2.html#semi-structured-data-json-xml-yaml",
    "href": "lectures/lecture2.html#semi-structured-data-json-xml-yaml",
    "title": "Lecture 2",
    "section": "3. Semi-structured Data (JSON, XML, YAML)",
    "text": "3. Semi-structured Data (JSON, XML, YAML)\nThese data are not fully structured like in SQL databases, but they have some schema.\n\nüìå Features:\n‚úÖ Hierarchical structure (e.g., key-value pairs, nested objects). ‚úÖ No strict schema (possibility to add new fields). ‚úÖ Popular in NoSQL systems and APIs.\n\n\nüìå Examples:\n‚û°Ô∏è Documents in MongoDB, configuration files, REST APIs, log files.\n\n\nüñ•Ô∏è Example Python Code (JSON):\n\n\nCode\nimport json\ndata = {'name': 'Alice', 'age': 30, 'city': 'New York'}\njson_str = json.dumps(data)\nprint(json.loads(json_str))\n\n\n{'name': 'Alice', 'age': 30, 'city': 'New York'}",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 2"
    ]
  },
  {
    "objectID": "lectures/lecture2.html#text-data-nlp",
    "href": "lectures/lecture2.html#text-data-nlp",
    "title": "Lecture 2",
    "section": "4. Text Data (NLP)",
    "text": "4. Text Data (NLP)\nText has become a key source of information, especially in sentiment analysis, chatbots, and search engines.\n\nüìå Features:\n‚úÖ Unstructured data requiring transformation. ‚úÖ Use of embeddings (e.g., Word2Vec, BERT, GPT). ‚úÖ Widely used in sentiment analysis and chatbots.\n\n\nüìå Examples:\n‚û°Ô∏è Social media, emails, chatbots, machine translation.\n\n\nüñ•Ô∏è Example Python Code :\n\n\nCode\nimport ollama\n\n# Przyk≈Çadowe zdanie\nsentence = \"Artificial intelligence is changing the world.\"\nresponse = ollama.embeddings(model='llama3.2', prompt=sentence)\nembedding = response['embedding']\nprint(embedding[:4])\n\n\n[-2.021953582763672, 1.5604140758514404, -0.5358548164367676, -1.3182345628738403]",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 2"
    ]
  },
  {
    "objectID": "lectures/lecture2.html#multimedia-data-images-sound-video",
    "href": "lectures/lecture2.html#multimedia-data-images-sound-video",
    "title": "Lecture 2",
    "section": "5. Multimedia Data (Images, Sound, Video)",
    "text": "5. Multimedia Data (Images, Sound, Video)\nModern data analysis systems also use images and sound.\n\nüìå Features:\n‚úÖ Require significant computational power (AI, deep learning). ‚úÖ Processed by CNN models (images) and RNN/Transformers (sound).\n\n\nüìå Examples:\n‚û°Ô∏è Face recognition, speech analysis, biometrics, video content analysis.\n\n\nüñ•Ô∏è Example Python Code (Image - OpenCV) :\nimport cv2\nimage = cv2.imread('cloud.jpeg')\ncv2.waitKey(0)\ncv2.destroyAllWindows()",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 2"
    ]
  },
  {
    "objectID": "lectures/lecture2.html#hadoop-map-reduce-scaling-computation-on-big-data",
    "href": "lectures/lecture2.html#hadoop-map-reduce-scaling-computation-on-big-data",
    "title": "Lecture 2",
    "section": "Hadoop Map-Reduce ‚Äì Scaling Computation on Big Data",
    "text": "Hadoop Map-Reduce ‚Äì Scaling Computation on Big Data\nWhen we talk about scalable data processing, the first association might be Google.\nBut what actually enables us to search for information in a fraction of a second while processing petabytes of data?\nüëâ Did you know that the name ‚ÄúGoogle‚Äù comes from the word ‚ÄúGoogol,‚Äù which represents the number 10¬π‚Å∞‚Å∞?\nThat‚Äôs more than the number of atoms in the known universe! üåå\n\nüî• Challenge: Can you write out the number Googol by the end of this lecture?",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 2"
    ]
  },
  {
    "objectID": "lectures/lecture2.html#why-are-sql-and-traditional-algorithms-insufficient",
    "href": "lectures/lecture2.html#why-are-sql-and-traditional-algorithms-insufficient",
    "title": "Lecture 2",
    "section": "üîç Why Are SQL and Traditional Algorithms Insufficient?",
    "text": "üîç Why Are SQL and Traditional Algorithms Insufficient?\nTraditional SQL databases and single-threaded algorithms fail when data scales beyond a single computer.\nThis is where MapReduce comes in‚Äîa revolutionary computational model developed by Google.\n\nüõ†Ô∏è Google‚Äôs Solutions for Big Data:\n‚úÖ Google File System (GFS) ‚Äì a distributed file system.\n‚úÖ Bigtable ‚Äì a system for storing massive amounts of structured data.\n‚úÖ MapReduce ‚Äì an algorithm for distributing workloads across multiple machines.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 2"
    ]
  },
  {
    "objectID": "lectures/lecture2.html#graphical-representation-of-mapreduce",
    "href": "lectures/lecture2.html#graphical-representation-of-mapreduce",
    "title": "Lecture 2",
    "section": "üñºÔ∏è Graphical Representation of MapReduce",
    "text": "üñºÔ∏è Graphical Representation of MapReduce\n\n1. Mapping Splits Tasks (Map)\nEach input is divided into smaller parts and processed in parallel.\nüåç Imagine you have a phone book and want to find all people with the last name ‚ÄúNowak‚Äù.\n‚û°Ô∏è Divide the book into sections and give each person one section to analyze.\n\n\n2. Reducing Gathers the Results (Reduce)\nAll partial results are combined into one final answer.\nüîÑ All students report their findings, and one student collects and summarizes the response.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 2"
    ]
  },
  {
    "objectID": "lectures/lecture2.html#classic-example-counting-words-in-a-text",
    "href": "lectures/lecture2.html#classic-example-counting-words-in-a-text",
    "title": "Lecture 2",
    "section": "üí° Classic Example: Counting Words in a Text",
    "text": "üí° Classic Example: Counting Words in a Text\nLet‚Äôs assume we have millions of books and we want to count how many times each word appears.\n\nüñ•Ô∏è MapReduce Code in Python (Using Multiprocessing)\nfrom multiprocessing import Pool\nfrom collections import Counter\n\n# Map function (splitting text into words)\ndef map_function(text):\n    words = text.split()\n    return Counter(words)\n\n# Reduce function (summing up results)\ndef reduce_function(counters):\n    total_count = Counter()\n    for counter in counters:\n        total_count.update(counter)\n    return total_count\n\ntexts = [\n        \"big data is amazing\",\n        \"data science and big data\",\n        \"big data is everywhere\"\n    ]\nif __name__ == '__main__':    \n    with Pool() as pool:\n        mapped_results = pool.map(map_function, texts)\n    \n    final_result = reduce_function(mapped_results)\n    print(final_result)\n\n# Counter({'data': 4, 'big': 3, 'is': 2, 'amazing': 1, 'science': 1, 'and': 1, 'everywhere': 1})\n\n\nüîπ What‚Äôs Happening Here?\n‚úÖ Each text fragment is processed independently (map). ‚úÖ The results are collected and summed (reduce). ‚úÖ Outcome: We can process terabytes of text in parallel!",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 2"
    ]
  },
  {
    "objectID": "lectures/lecture2.html#visualization-comparison-of-the-classic-approach-and-mapreduce",
    "href": "lectures/lecture2.html#visualization-comparison-of-the-classic-approach-and-mapreduce",
    "title": "Lecture 2",
    "section": "üé® Visualization ‚Äì Comparison of the Classic Approach and MapReduce",
    "text": "üé® Visualization ‚Äì Comparison of the Classic Approach and MapReduce\nüìä Old Approach ‚Äì A single computer processes everything sequentially.\nüìä New Approach (MapReduce) ‚Äì Each machine processes a fragment, and the results are aggregated.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 2"
    ]
  },
  {
    "objectID": "lectures/lecture2.html#challenge-for-you",
    "href": "lectures/lecture2.html#challenge-for-you",
    "title": "Lecture 2",
    "section": "üöÄ Challenge for You!",
    "text": "üöÄ Challenge for You!\nüîπ Find and run your own MapReduce algorithm in any programming language!\nüîπ Can you implement your own MapReduce for a different task? (e.g., log analysis, counting website clicks)",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 2"
    ]
  },
  {
    "objectID": "lectures/lecture2.html#big-data-1",
    "href": "lectures/lecture2.html#big-data-1",
    "title": "Lecture 2",
    "section": "Big Data",
    "text": "Big Data\nBig Data systems can serve as a source for data warehouses (e.g., Data Lake, Enterprise Data Hub).\nHowever, Data Warehouses are not Big Data systems!\n\n1. Data Warehouses\n\nStore highly structured data\n\nFocused on analytics and reporting processes\n\n100% accuracy\n\n\n\n2. Big Data\n\nCan handle data of any structure\n\nUsed for various data-driven purposes (analytics, data science, etc.)\n\nLess than 100% accuracy\n\n\n‚ÄúBig Data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so everyone claims they are doing it.‚Äù\n‚Äî Dan Ariely, Professor of Psychology and Behavioral Economics, Duke University\n\n\n\nOne, Two, ‚Ä¶ Four V\n\nVolume ‚Äì The size of data produced worldwide is growing at an exponential rate.\n\nVelocity ‚Äì The speed at which data is generated, transmitted, and processed.\n\nVariety ‚Äì Traditional data is alphanumeric, consisting of letters and numbers. Today, we also deal with images, audio, video files, and IoT data streams.\n\nVeracity ‚Äì Are the data complete and accurate? Do they objectively reflect reality? Are they a reliable basis for decision-making?\n\nValue ‚Äì The actual worth of the data. Ultimately, it‚Äôs about cost and benefits.\n\n\n‚ÄúThe purpose of computing is insight, not numbers.‚Äù ‚Äì R.W. Hamming, 1962.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 2"
    ]
  },
  {
    "objectID": "lectures/lecture2.html#data-processing-models",
    "href": "lectures/lecture2.html#data-processing-models",
    "title": "Lecture 2",
    "section": "Data Processing Models",
    "text": "Data Processing Models\nData has always been processed in business.\nOver the past decades, the amount of processed data has been steadily increasing, affecting the way data is prepared and handled.\n\nA Bit of History\n\n1960s: Data collections, databases\n\n1970s: Relational data models and their implementation in OLTP systems\n\n1975: First personal computers\n\n1980s: Advanced data models (extended-relational, object-oriented, application-oriented, etc.)\n\n1983: The beginning of the Internet\n\n1990s: Data mining, data warehouses, OLAP systems\n\nLater: NoSQL, Hadoop, SPARK, data lakes\n\n2002: AWS, 2005: Hadoop, Cloud computing\n\n\nMost data is stored in databases or data warehouses.\nTypically, data access is performed through applications by executing queries.\nThe method of utilizing and accessing a database is called the data processing model.\nThe two most commonly used implementations are:\n\n\nTraditional Model\nThe traditional model refers to online transaction processing (OLTP),\nwhich excels at handling real-time tasks such as customer service, order management, and sales processing.\nIt is commonly used in Enterprise Resource Planning (ERP) systems, Customer Relationship Management (CRM) software, and web-based applications.\n\nThis model provides efficient solutions for:\n\nSecure and efficient data storage\n\nTransactional data recovery after failures\n\nOptimized data access\n\nConcurrency management\n\nEvent processing ‚Üí read ‚Üí write\n\nHowever, what happens when we need to deal with:\n\nAggregation of data from multiple systems (e.g., multiple stores)\n\nReporting and data summarization\n\nOptimization of complex queries\n\nBusiness decision support\n\nResearch on these topics led to the formulation of a new data processing model and a new type of database ‚Äì Data Warehouses.\n\n\nOLAP Model\nOnline Analytical Processing (OLAP)\nOLAP supports data analysis and provides tools for multidimensional analysis\nbased on dimensions such as time, location, and product.\nThe process of extracting data from various systems into a single database is known as Extract-Transform-Load (ETL),\nwhich involves normalization, encoding, and schema transformation.\nAnalyzing data in a data warehouse mainly involves calculating aggregates (summaries) across different dimensions.\nThis process is entirely user-driven.\n\n\nExample\nImagine we have access to a data warehouse storing sales information from a supermarket.\nHow can we analyze queries such as:\n\nWhat is the total sales of products in the subsequent quarters, months, and weeks?\nWhat is the sales breakdown by product categories?\nWhat is the sales breakdown by supermarket branches?\n\nAnswers to these questions help identify bottlenecks in product sales, plan inventory levels, and compare sales across different product groups and supermarket branches.\nIn a Data Warehouse, two types of queries are most commonly executed (both in batch mode):\n\nPeriodic queries that generate business statistics, such as reporting queries.\nAd-hoc queries that support critical business decisions.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 2"
    ]
  },
  {
    "objectID": "lectures/lecture4.html",
    "href": "lectures/lecture4.html",
    "title": "Lecture 4",
    "section": "",
    "text": "‚è≥ Duration: 1.5h",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 4"
    ]
  },
  {
    "objectID": "lectures/lecture4.html#the-evolution-of-architectural-approaches",
    "href": "lectures/lecture4.html#the-evolution-of-architectural-approaches",
    "title": "Lecture 4",
    "section": "The Evolution of Architectural Approaches",
    "text": "The Evolution of Architectural Approaches\nThe development of technology, particularly the shift from monolithic to microservices architecture, has had a profound impact on modern information systems.\nMonolithic applications, which were the dominant approach in the past, consisted of a single, large unit of code. While this approach had its advantages, such as simplicity during the initial stages of system development, it also presented significant drawbacks, including challenges with scaling, limited flexibility, and complex maintenance.\nAs technology evolved, microservices emerged as a solution. This approach involves breaking an application into smaller, independent services, each responsible for a specific functionality. The shift to microservices enabled greater flexibility, easier system scaling, and faster deployment of new features. Additionally, each service can be developed, tested, and deployed independently, simplifying code management and reducing the risk of errors.\nThanks to microservices, organizations can better adapt to changing business needs, improve system availability (by isolating failures to individual services), and innovate more quickly. Furthermore, microservices promote the use of modern techniques like containerization and cloud solutions, which streamline infrastructure management and make better use of resources.\nHowever, despite the many benefits, transitioning to microservices also comes with challenges, such as:\n\nComplexity in managing communication between services\nThe need to monitor and maintain a larger number of components\nManaging distributed transactions\n\nThese challenges require new tools and approaches to management, as well as the implementation of a DevOps culture.\nWith the development of microservices, new technologies such as serverless and containerization have emerged, serving as natural extensions of system flexibility. These technologies further enhance the efficiency of managing and scaling modern applications, becoming key components of the cloud ecosystem.\n\nServerless\nServerless is a model where developers don‚Äôt have to manage servers or infrastructure. Instead, cloud providers take care of all the infrastructure, allowing developers to focus solely on the application code. The key advantage of this approach is its scalability ‚Äì applications automatically scale based on resource demand. Serverless systems allow functions to be dynamically started and stopped in response to specific events, leading to cost optimization (you only pay for the actual resource usage). This approach simplifies managing applications with variable or unpredictable traffic.\nServerless is also a great complement to microservices, enabling the deployment of independent functions in response to various events, which offers even greater flexibility. It can be used for applications such as real-time data processing, API handling, and task automation.\n\n\nContainerization\nContainerization (e.g., using Docker) is another step toward increasing flexibility. With containers, applications and their dependencies are packaged into isolated units that can be run across different environments in a consistent and predictable manner. Containers are lightweight, fast to deploy, and offer easy portability across platforms, which is crucial in microservice architectures.\nContainerization is gaining significance, especially when combined with container management tools like Kubernetes, which automatically scales applications, monitors their status, ensures high availability, and manages their lifecycle. This approach perfectly supports both microservices and serverless, enabling easy deployment, scaling, and monitoring of applications.\n\n\nCommon Goal ‚Äì Flexibility\nBoth serverless and containerization represent a further step towards flexibility, offering the ability to quickly adapt to changing conditions and demands. Together with microservices, they form a modern approach to application architecture that allows for the separation of responsibilities, easier scaling, dynamic resource allocation, and better utilization of cloud infrastructure.\nThe combination of these technologies allows businesses to rapidly deploy new features, respond to changing user needs, and minimize costs by optimizing resource usage ‚Äì all of which are particularly important in today‚Äôs fast-evolving technological landscape.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 4"
    ]
  },
  {
    "objectID": "lectures/lecture4.html#the-impact-of-technology-on-information-systems",
    "href": "lectures/lecture4.html#the-impact-of-technology-on-information-systems",
    "title": "Lecture 4",
    "section": "The Impact of Technology on Information Systems",
    "text": "The Impact of Technology on Information Systems\nNetworking communication, relational databases, cloud solutions, and Big Data have significantly transformed the way information systems are built and how work is carried out within them.\nSimilarly, information-sharing tools such as newspapers, radio, television, the internet, messaging apps, and social media have influenced human interactions and social structures. Each new technological medium shapes how people use computing and perceive its role in everyday life.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 4"
    ]
  },
  {
    "objectID": "lectures/lecture4.html#microservices-architecture",
    "href": "lectures/lecture4.html#microservices-architecture",
    "title": "Lecture 4",
    "section": "Microservices architecture",
    "text": "Microservices architecture\nThe concept of microservices is essential to understand when working on architectures. Although there are other ways to architecture software projects, microservices are famous for a good reason. They help teams be flexible and effective and help to keep software loose and structured.\nThe idea behind microservices is in the name: of software is represented as many small services that operate individually. When looking at the overall architecture, each of the microservices is inside a small black box with clearly defined inputs and outputs.\nAn often-chosen solution is to use Application Programming Interfaces ( API) to allow different microservices to communicate\n\nMain Advantages of Microservices:\n\nEfficiency ‚Äì Each service performs a single, well-defined task (‚Äúdo one thing, but do it well‚Äù).\nFlexibility ‚Äì They allow for easy modifications and scaling of the system.\nArchitecture Transparency ‚Äì The system consists of small, independent modules.\n\nMicroservices can be compared to pure functions in functional programming ‚Äì each service operates independently and has clearly defined inputs and outputs.\nTo enable communication between microservices, Application Programming Interfaces (APIs) are often used, allowing data exchange and integration between different services.\nExample of an API in Microservices ‚Äì Python & FastAPI Below is an example of a REST API microservice in Python using FastAPI, which returns user information:\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n# Przyk≈Çadowe dane u≈ºytkownik√≥w\nusers = {\n    1: {\"name\": \"Anna\", \"age\": 28},\n    2: {\"name\": \"Piotr\", \"age\": 35},\n    3: {\"name\": \"Kasia\", \"age\": 22},\n}\n\n@app.get(\"/users/{user_id}\")\ndef get_user(user_id: int):\n    \"\"\"Zwraca dane u≈ºytkownika na podstawie ID.\"\"\"\n    return users.get(user_id, {\"error\": \"User not found\"})\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n\nWe start the FastAPI server.\nWe can retrieve user data by sending a GET request to http://127.0.0.1:8000/users/1.\nThe API will return the data in JSON format, for example:\n\n{\n    \"name\": \"Anna\",\n    \"age\": 28\n}\n\n\nCommunication through API\nA central component in microservice architectures is the use of APIs. An API is a part that allows you to connect two microservices. APIs are much like websites. Like a website, the server sends You the code that represents the website. Your internet browser then interprets this code and shows you a web page.\n\nLet‚Äôs take a business case with the ML model as a service.\nLet‚Äôs assume you work for a company that sells apartments in Boston. We want to increase our sales and offer better quality services to our customers with a new mobile application which can be used even by 1 000 000 people simultaneously. We can realize this by serving a prediction of house value when the user requests for pricing over the web.\n\n\nServing a Model\n\nTraining a good ML model is ONLY the first part: You do need to make your model available to your end-users You do this by either providing access to the model on your server.\nWhen serving ML Model You need: a model, an interpreter, input data.\nImportant Metrics:\n\n\nLatency,\nCost,\nThroughput (number of requests served per unit time)\n\n\nSharing data between two or more systems has always been a fundamental requirement of software development ‚Äì DevOps vs MLOps.\n\nBuilding a system ready for a production environment is more complex than training the model itself: - Cleaning and loading appropriate and validated data - Calculating variables and serving them in the correct environment - Serving the model in the most cost-efficient way - Versioning, tracking, and sharing data, models, and other artifacts - Monitoring the infrastructure and the model - Deploying the model on scalable infrastructure - Automating the deployment and training process\nWhen you call an API, the API will receive your request. The request triggers your code to be run on the server and generates a response sent back to you. If something goes wrong, you may not receive any reply or receive an error code as an HTTP status code.\n\nClient-Server: Client (system A) requests to a URL hosted by system B, which returns a response. It‚Äôs identical to how a web browser works. A browser requests for a specific URL. The request is routed to a web server that returns an HTML (text) page.\nStateless: The client request should contain all the information necessary to respond.\n\nYou can call APIs with a lot of different tools. Sometimes, you can even use your web browser. Otherwise, tools such as CURL do the job on the command line. You can use tools such as Postman for calling APIs with the user interface.\n\nAll communication is covered in fixed rules and practices, which are called the HTTP protocol.\n\nExample: API Serving an ML Model Below is an example of an API service that exposes an ML model for predicting real estate prices, using FastAPI and Scikit-Learn:\nfrom fastapi import FastAPI\nimport pickle\nimport numpy as np\n\n# Tworzymy API\napp = FastAPI()\n\n# Wczytujemy wcze≈õniej wytrenowany model ML (np. regresjƒô liniowƒÖ)\nwith open(\"model.pkl\", \"rb\") as f:\n    model = pickle.load(f)\n\n@app.get(\"/predict/\")\ndef predict_price(area: float, bedrooms: int, age: int):\n    \"\"\"\n    Real Estate Price Prediction Based on Features:\n    - area ( m¬≤),\n    - bedrooms (#),\n    - age .\n    \"\"\"\n    features = np.array([[area, bedrooms, age]])\n    price = model.predict(features)[0]\n    return {\"estimated_price\": round(price, 2)}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n\n\n\nRequest\n\nAn Endpoint URL a domain, port, path, query string - http://mydomain:8000/getapi?&val1=43&val2=3\nThe HTTP methods - GET, POST\nHTTP headers contain authentication information, cookies metadata - Content-Type: application/json, text ‚Ä¶ Accept: application/json, Authorization: Basic abase64string, Tokens etc\nRequest body\n\nThe most common format for interaction between services is the JavaScript Object Notation format. it is a data type that very strongly resembles the dictionary format in Python - key-value object.\n{\n\"RAD\": 1,\n\"PTRATIO\": 15.3, \"INDUS\": 2.31, \"B\": 396.9,\n\"ZN\": 18,\n\"DIS\": 4.09, \"CRIM\": 0.00632, \"RM\": 6.575, \"AGE\": 65.2, \"CHAS\": 0, \"NOX\": 0.538, \"TAX\": 296, \"LSTAT\": 4.98\n}\n\n\nResponse\n\nThe response payload is defined in the response header:\n\n200 OK\nContent-Encoding: gzip\nContent-Type: text/html; charset=utf-8\nDate: Mon, 18 Jul 2016 16:06:00 GMT Server: Apache\nPath=/;\n\nHeader example: ‚ÄúContent-Type‚Äù =&gt; ‚Äúapplication/json; charset=utf-8‚Äù, ‚ÄúServer‚Äù =&gt; ‚ÄúGenie/Julia/1.8.5\nBody example:\n\n{\":input\":{\"RAD\":1,\"PTRATIO\":15.3,\"INDUS\":2.31,.....}}, {\":prediction\":[29.919737211857683]}\n\nHTTP status code: ‚Ä¢ 200 OK is used for successful requests, ‚Ä¢ 40X Access Denied ‚Ä¢ 50X Internal server error\n\n\nREST API\nThe Representational State Transfer (REST) API works just like other APIs, but it follows a certain set of style rules that make it reconizable as a REST API: - Client-server architecture - Statelessnes - Cacheability - Layered system - Uniform Interface",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 4"
    ]
  },
  {
    "objectID": "lectures/lecture4.html#publishsubscribe",
    "href": "lectures/lecture4.html#publishsubscribe",
    "title": "Lecture 4",
    "section": "Publish/Subscribe",
    "text": "Publish/Subscribe\nThe ‚ÄúPublish/Subscribe‚Äù messaging system is crucial for data-driven applications. Pub/Sub messages are a pattern in which the sender (publisher) of a piece of data (message) does not directly target a specific receiver. Pub/Sub systems often have a broker, which is a central point where the messages are stored.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 4"
    ]
  },
  {
    "objectID": "lectures/lecture4.html#apache-kafka",
    "href": "lectures/lecture4.html#apache-kafka",
    "title": "Lecture 4",
    "section": "Apache Kafka",
    "text": "Apache Kafka\nOn the Kafka website, you will find the definition:\n\nDistributed Streaming Platform\nWhat is a ‚Äúdistributed streaming platform‚Äù?\nFirst, I want to remind you what a ‚Äústream‚Äù is. Streams are simply limitless data, data that never ends. They keep coming in, and you can process them in real-time.\nAnd what does ‚Äúdistributed‚Äù mean? Distributed means that Kafka runs in a cluster, and each node in the group is called a Broker. These brokers are just servers that perform copies of Apache Kafka.\nSo, Kafka is a set of machines working together to handle and process limitless data in real-time.\nBrokers make it reliable, scalable, and fault-tolerant. But why is there a misconception that Kafka is just another ‚Äúqueue-based messaging system‚Äù?\nTo answer this, we first need to explain how a queue-based messaging system works.\n\n\nQueue-Based Messaging System\nMessage passing is simply the act of sending a message from one place to another. It has three main ‚Äúactors‚Äù: - Producer: Creates and sends messages to one or more queues. - Queue: A data structure that buffers messages, receiving them from producers and delivering them to consumers in a FIFO (First-In-First-Out) manner. Once a message is received, it is permanently removed from the queue; there is no chance of retrieving it. - Consumer: Subscribes to one or more queues and receives their messages after they are published.\nAnd that‚Äôs it; this is how message passing works. As you can see, there is nothing about streams, real-time processing, or clusters in this.\n\n\nApache Kafka Architecture\nFor more information about Kafka, you can visit this link.\nNow that we understand the basics of message passing, let‚Äôs dive into the world of Apache Kafka. In Kafka, there are two key concepts: Producers and Consumers, who work similarly to classic queue-based systems, producing and consuming messages.\n\n\nAs seen, Kafka resembles a classic messaging system; however, unlike traditional queues, Kafka uses Topics instead of the concept of a queue.\nTopics and Partitions\nA Topic is the fundamental data transmission channel in Kafka. It can be compared to a folder where messages are stored.\nEach topic has one or more partitions. This division impacts scalability and load balancing. When creating a topic, the number of partitions is defined.\n\nKey Features of Topics and Partitions:\n\nTopic is a logical unit where producers send messages, and consumers read them.\nPartition is a physical subdivision of a topic. It can be compared to files in a folder.\nOffset ‚Äì Each message in a partition gets a unique identifier (offset), which allows consumers to track which messages have already been processed.\nKafka stores messages on disk, allowing them to be read again (unlike traditional queues, where a message is deleted after being processed).\nConsumers read messages sequentially, from the oldest to the newest.\nIn case of failure, a consumer can resume processing from the last saved offset.\n\n\n\n\n\nKafka Brokers and Cluster\nKafka operates in a distributed manner ‚Äì this means it can consist of multiple brokers, which work together as a single cluster.\n\n\nKey Information about Brokers - A Broker is a single server in a Kafka cluster, responsible for storing the partitions of topics. - Each broker in the cluster has a unique identifier. - To increase availability and reliability, Kafka uses data replication. - The replication factor defines how many copies of a partition should be stored on different brokers. - If a topic has three partitions and a replication factor of three, it means each partition will be replicated across three different brokers.\nThe number of partitions should be chosen in a way that each broker handles at least one partition.\n\n\nProducers\nIn Kafka, producers are applications or services that create and send messages to topics. This works similarly to queue systems, except Kafka writes messages to partitions.\n\nHow does Kafka assign messages to partitions?\n\nMessages are distributed round-robin to available partitions.\nWe can specify a message key, and Kafka will calculate its hash to determine which partition the message will go to.\nThe message key determines the partition assignment ‚Äì once the topic is created, the number of partitions cannot be changed without disrupting this mechanism.\n\nExample of message assignment to partitions: - Message 01 goes to partition 0 of topic Topic_1. - Message 02 goes to partition 1 of the same topic. - The next message may go back to partition 0 if round-robin assignment is applied.\nfrom kafka import KafkaProducer\n\n# Tworzymy producenta Kafka\nproducer = KafkaProducer(bootstrap_servers=\"localhost:9092\")\n\n# Wysy≈Çamy wiadomo≈õƒá do tematu \"real_estate\"\ntopic = \"real_estate\"\nmessage = b\"new flat \"\n\nproducer.send(topic, message)\nproducer.flush()\n\nprint(f\"Message in topic: '{topic}'\")\n\n\n\nConsumers\nConsumers in Kafka read and process messages from topics. Each consumer can belong to a consumer group, which allows parallel message processing. - If multiple consumers belong to the same group, Kafka balances the load between them. - If one consumer fails, Kafka will automatically reassign its partitions to another active consumer.\nfrom kafka import KafkaConsumer\n\n# Tworzymy konsumenta, kt√≥ry nas≈Çuchuje temat \"real_estate\"\nconsumer = KafkaConsumer(\"real_estate\", bootstrap_servers=\"localhost:9092\")\n\nprint(\"Wait for new message...\")\n\nfor message in consumer:\n    print(f\"new message: {message.value.decode()}\")\nAnother important concept in Kafka is Consumer Groups. This is crucial when we need to scale the reading of messages. It becomes costly when a single consumer must read from many partitions, so we need to balance the load between our consumers, and this is where consumer groups come in.\nData from a single topic will be load-balanced between consumers, ensuring that our consumers can handle and process the data efficiently. The ideal scenario is to have the same number of consumers in the group as there are partitions in the topic, so each consumer only reads from one partition. When adding consumers to a group, be cautious ‚Äî if the number of consumers exceeds the number of partitions, some consumers will not read from any topic and will remain idle.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 4"
    ]
  },
  {
    "objectID": "kafka_codes/kafka1.html",
    "href": "kafka_codes/kafka1.html",
    "title": "Introduction",
    "section": "",
    "text": "Apache Kafka is a stream processing system (event streaming) that acts as a distributed message broker. It enables real-time data transmission and processing.\nThe default address of our broker is broker:9092.\nIn Apache Kafka, data is stored in structures called topics, which serve as communication queues.\nKafka management is performed using scripts. In our case, these will be .sh scripts.\n\n\nRemember to navigate to the home directory:\ncd ~\nkafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092\n\n\n\nkafka/bin/kafka-topics.sh --create --topic mytopic --bootstrap-server broker:9092\n\n\n\nThis script allows you to manually enter events via the terminal. The --property options are additional and used for analysis in this example.\nkafka/bin/kafka-console-producer.sh --bootstrap-server broker:9092 --topic mytopic --property \"parse.key=true\" --property \"key.separator=:\"\n\n\n\nOpen a new terminal in the directory where the test_key_value.py file is located and run the Consumer program in Spark.\nfrom pyspark.sql import SparkSession\n\nKAFKA_BROKER = 'broker:9092'\nKAFKA_TOPIC = 'mytopic'\n\nspark = SparkSession.builder.getOrCreate()\nspark.sparkContext.setLogLevel(\"WARN\")\n\ndf = (spark.readStream.format(\"kafka\")\n      .option(\"kafka.bootstrap.servers\", KAFKA_BROKER)\n      .option(\"subscribe\", KAFKA_TOPIC)\n      .option(\"startingOffsets\", \"earliest\")\n      .load()\n     )\n\ndf.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n .writeStream \\\n .format(\"console\") \\\n .outputMode(\"append\") \\\n .start() \\\n .awaitTermination()\nNote that Apache Spark does not have a built-in Kafka connector, so run the process using spark-submit and download the appropriate Scala package:\nspark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 test_key_value.py\n\n\n\nIn the terminal with the running producer, enter text in the following format:\njan:45\nalicja:20\nCheck what appears in the Consumer application window.\n\n\n\nAfter completing the demonstration, use Ctrl+C to close both the producer window and the Spark application.\n\nDone! Now you have a basic Apache Kafka and Spark setup for stream processing. üéâ",
    "crumbs": [
      "Home",
      "Labs",
      "Apache Kafka",
      "Introduction"
    ]
  },
  {
    "objectID": "kafka_codes/kafka1.html#check-the-list-of-topics",
    "href": "kafka_codes/kafka1.html#check-the-list-of-topics",
    "title": "Introduction",
    "section": "",
    "text": "Remember to navigate to the home directory:\ncd ~\nkafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092",
    "crumbs": [
      "Home",
      "Labs",
      "Apache Kafka",
      "Introduction"
    ]
  },
  {
    "objectID": "kafka_codes/kafka1.html#create-a-new-topic-named-mytopic",
    "href": "kafka_codes/kafka1.html#create-a-new-topic-named-mytopic",
    "title": "Introduction",
    "section": "",
    "text": "kafka/bin/kafka-topics.sh --create --topic mytopic --bootstrap-server broker:9092",
    "crumbs": [
      "Home",
      "Labs",
      "Apache Kafka",
      "Introduction"
    ]
  },
  {
    "objectID": "kafka_codes/kafka1.html#create-a-producer",
    "href": "kafka_codes/kafka1.html#create-a-producer",
    "title": "Introduction",
    "section": "",
    "text": "This script allows you to manually enter events via the terminal. The --property options are additional and used for analysis in this example.\nkafka/bin/kafka-console-producer.sh --bootstrap-server broker:9092 --topic mytopic --property \"parse.key=true\" --property \"key.separator=:\"",
    "crumbs": [
      "Home",
      "Labs",
      "Apache Kafka",
      "Introduction"
    ]
  },
  {
    "objectID": "kafka_codes/kafka1.html#consumer-in-spark",
    "href": "kafka_codes/kafka1.html#consumer-in-spark",
    "title": "Introduction",
    "section": "",
    "text": "Open a new terminal in the directory where the test_key_value.py file is located and run the Consumer program in Spark.\nfrom pyspark.sql import SparkSession\n\nKAFKA_BROKER = 'broker:9092'\nKAFKA_TOPIC = 'mytopic'\n\nspark = SparkSession.builder.getOrCreate()\nspark.sparkContext.setLogLevel(\"WARN\")\n\ndf = (spark.readStream.format(\"kafka\")\n      .option(\"kafka.bootstrap.servers\", KAFKA_BROKER)\n      .option(\"subscribe\", KAFKA_TOPIC)\n      .option(\"startingOffsets\", \"earliest\")\n      .load()\n     )\n\ndf.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n .writeStream \\\n .format(\"console\") \\\n .outputMode(\"append\") \\\n .start() \\\n .awaitTermination()\nNote that Apache Spark does not have a built-in Kafka connector, so run the process using spark-submit and download the appropriate Scala package:\nspark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 test_key_value.py",
    "crumbs": [
      "Home",
      "Labs",
      "Apache Kafka",
      "Introduction"
    ]
  },
  {
    "objectID": "kafka_codes/kafka1.html#test-data-transmission",
    "href": "kafka_codes/kafka1.html#test-data-transmission",
    "title": "Introduction",
    "section": "",
    "text": "In the terminal with the running producer, enter text in the following format:\njan:45\nalicja:20\nCheck what appears in the Consumer application window.",
    "crumbs": [
      "Home",
      "Labs",
      "Apache Kafka",
      "Introduction"
    ]
  },
  {
    "objectID": "kafka_codes/kafka1.html#terminating-the-process",
    "href": "kafka_codes/kafka1.html#terminating-the-process",
    "title": "Introduction",
    "section": "",
    "text": "After completing the demonstration, use Ctrl+C to close both the producer window and the Spark application.\n\nDone! Now you have a basic Apache Kafka and Spark setup for stream processing. üéâ",
    "crumbs": [
      "Home",
      "Labs",
      "Apache Kafka",
      "Introduction"
    ]
  }
]