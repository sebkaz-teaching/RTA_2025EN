[
  {
    "objectID": "lecture1.html",
    "href": "lecture1.html",
    "title": "Lecture 1",
    "section": "",
    "text": "⏳ Duration: 1.5h\n🎯 Lecture Goal\nIntroducing students to the fundamentals of real-time analytics, the differences between data processing modes (batch, streaming, real-time), as well as key applications and challenges."
  },
  {
    "objectID": "lecture1.html#what-is-real-time-data-analytics",
    "href": "lecture1.html#what-is-real-time-data-analytics",
    "title": "Lecture 1",
    "section": "What is Real-Time Data Analytics?",
    "text": "What is Real-Time Data Analytics?\n\nDefinition and Key Concepts\nReal-Time Data Analytics is the process of processing and analyzing data immediately after it is generated, without the need for storage and later processing. The goal is to obtain instant insights and responses to changing conditions in business, technology, and scientific systems.\n\n\nKey Features of Real-Time Data Analytics:\n\nLow latency – data is analyzed within milliseconds or seconds of being generated.\nStreaming vs. Batch Processing – data analysis can occur continuously (streaming) or at predefined intervals (batch).\nIntegration with IoT, AI, and ML – real-time analytics often works in conjunction with the Internet of Things (IoT) and artificial intelligence algorithms.\nReal-time decision-making – e.g., instant fraud detection in banking transactions."
  },
  {
    "objectID": "lecture1.html#business-applications-of-real-time-data-analytics",
    "href": "lecture1.html#business-applications-of-real-time-data-analytics",
    "title": "Lecture 1",
    "section": "Business Applications of Real-Time Data Analytics",
    "text": "Business Applications of Real-Time Data Analytics\n\nFinance and Banking\n\nFraud Detection – real-time transaction analysis helps identify anomalies indicating fraud.\nAutomated Trading – HFT (High-Frequency Trading) systems analyze millions of data points in fractions of a second.\nDynamic Credit Scoring – instant risk assessment of a customer’s creditworthiness.\n\n\n\nE-Commerce and Digital Marketing\n\nReal-Time Offer Personalization – dynamic product recommendations based on users’ current behavior.\nDynamic Pricing – companies like Uber, Amazon, and hotels adjust prices in real time based on demand.\nSocial Media Monitoring – sentiment analysis of customer feedback and immediate response to negative comments.\n\n\n\nTelecommunications and IoT\n\nNetwork Infrastructure Monitoring – real-time log analysis helps detect failures before they occur.\nSmart Cities – real-time traffic analysis optimizes traffic light systems dynamically.\nIoT Analytics – IoT devices generate data streams that can be analyzed in real time (e.g., smart energy meters).\n\n\n\nHealthcare\n\nPatient Monitoring – real-time analysis of medical device signals to detect life-threatening conditions instantly.\nEpidemiological Analytics – tracking disease outbreaks based on real-time data.\n\nReal-time data analytics is a key component of modern IT systems, enabling businesses to make faster and more precise decisions. It is widely used across industries—from finance and e-commerce to healthcare and IoT."
  },
  {
    "objectID": "lecture1.html#differences-between-batch-processing-near-real-time-analytics-and-real-time-analytics",
    "href": "lecture1.html#differences-between-batch-processing-near-real-time-analytics-and-real-time-analytics",
    "title": "Lecture 1",
    "section": "Differences Between Batch Processing, Near Real-Time Analytics, and Real-Time Analytics",
    "text": "Differences Between Batch Processing, Near Real-Time Analytics, and Real-Time Analytics\nThere are three main approaches to data processing:\n\nBatch Processing\n\nNear Real-Time Analytics\n\nReal-Time Analytics\n\nEach differs in processing speed, technological requirements, and business applications.\n\nBatch Processing\n📌 Definition:\nBatch Processing involves collecting large amounts of data and processing them at scheduled intervals (e.g., hourly, daily, or weekly).\n📌 Characteristics:\n\n✅ High efficiency for large datasets\n\n✅ Processes data after it has been collected\n\n✅ Does not require immediate analysis\n\n✅ Typically cheaper than real-time processing\n\n❌ Delays – results are available only after processing is complete\n\n📌 Use Cases:\n\nGenerating financial reports at the end of a day/month\n\nAnalyzing sales trends based on historical data\n\nCreating offline machine learning models\n\n📌 Example Technologies:\n\nHadoop MapReduce\n\nApache Spark (in batch mode)\n\nGoogle BigQuery\n\nimport pandas as pd  \ndf = pd.read_csv(\"transactions.csv\")  \n\ndf['transaction_date'] = pd.to_datetime(df['transaction_date'])\ndf['month'] = df['transaction_date'].dt.to_period('M')  # Ekstrakcja miesiąca\n\n# Agregacja danych - miesięczne sumy transakcji\nmonthly_sales = df.groupby(['month'])['amount'].sum()\n\n# Zapis wyników do pliku (np. raportu)\nmonthly_sales.to_csv(\"monthly_report.csv\")  \n\nprint(\"Raport zapisany!\")\nIf you wanted to create data for an example.\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)\ndata = {\n    'transaction_id': [f'TX{str(i).zfill(4)}' for i in range(1, 1001)],\n    'amount': np.random.uniform(10, 10000, 1000), \n    'transaction_date': pd.date_range(start=\"2025-01-01\", periods=1000, freq='h'), \n    'merchant': np.random.choice(['Merchant_A', 'Merchant_B', 'Merchant_C', 'Merchant_D'], 1000),\n    'card_type': np.random.choice(['Visa', 'MasterCard', 'AmEx'], 1000)\n}\n\ndf = pd.DataFrame(data)\ncsv_file = 'transactions.csv'\ndf.to_csv(csv_file, index=False)"
  },
  {
    "objectID": "lecture1.html#near-real-time-analytics-analysis-nearly-in-real-time",
    "href": "lecture1.html#near-real-time-analytics-analysis-nearly-in-real-time",
    "title": "Lecture 1",
    "section": "Near Real-Time Analytics – Analysis Nearly in Real Time",
    "text": "Near Real-Time Analytics – Analysis Nearly in Real Time\n📌 Definition:\nNear Real-Time Analytics refers to data analysis that occurs with minimal delay (typically from a few seconds to a few minutes). It is used in scenarios where full real-time analysis is not necessary, but excessive delays could impact business operations.\n📌 Characteristics:\n\n✅ Processes data in short intervals (a few seconds to minutes)\n\n✅ Enables quick decision-making but does not require millisecond-level reactions\n\n✅ Optimal balance between cost and speed\n\n❌ Not suitable for systems requiring immediate response\n\n📌 Use Cases:\n\nMonitoring banking transactions and detecting fraud (e.g., analysis within 30 seconds)\n\nDynamically adjusting online ads based on user behavior\n\nAnalyzing server and network logs to detect anomalies\n\n📌 Example Technologies:\n\nApache Kafka + Spark Streaming\n\nElasticsearch + Kibana (e.g., IT log analysis)\n\nAmazon Kinesis\n\nExample of a data producer sending transactions to an Apache Kafka system.\nfrom kafka import KafkaProducer\nimport json\nimport random\nimport time\nfrom datetime import datetime\n\n# Ustawienia dla producenta\nbootstrap_servers = 'localhost:9092'\ntopic = 'transactions' \n\n# Funkcja generująca przykładowe dane transakcji\ndef generate_transaction():\n    transaction = {\n        'transaction_id': f'TX{random.randint(1000, 9999)}',\n        'amount': round(random.uniform(10, 10000), 2),  # Kwota między 10 a 10 000\n        'transaction_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        'merchant': random.choice(['Merchant_A', 'Merchant_B', 'Merchant_C', 'Merchant_D']),\n        'card_type': random.choice(['Visa', 'MasterCard', 'AmEx']),\n    }\n    return transaction\n\nproducer = KafkaProducer(\n    bootstrap_servers=bootstrap_servers,\n    value_serializer=lambda v: json.dumps(v).encode('utf-8') \n)\n\n\nfor _ in range(1000):  \n    transaction = generate_transaction()\n    producer.send(topic, value=transaction) \n    print(f\"Sent: {transaction}\")\n    time.sleep(1) \n\n# Zakończenie działania producenta\nproducer.flush()\nproducer.close()\nConsument example\nfrom kafka import KafkaConsumer\nimport json  \n\n# Konsumer do pobierania danych z Kafka\nconsumer = KafkaConsumer(\n    'transactions',\n    bootstrap_servers='localhost:9092',\n    auto_offset_reset='earliest',\n    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n)\n\n# Pobieranie transakcji w niemal real-time i analiza\nfor message in consumer:\n    transaction = message.value\n    if transaction[\"amount\"] &gt; 8000:\n        print(f\"🚨 Wykryto dużą transakcję: {transaction}\")\nExample of the DataSet\n{\n    \"transaction_id\": \"TX1234\",\n    \"amount\": 523.47,\n    \"transaction_date\": \"2025-02-11 08:10:45\",\n    \"merchant\": \"Merchant_A\",\n    \"card_type\": \"Visa\"\n}\n\nReal-Time Analytics\n📌 Definition:\nReal-Time Analytics refers to the immediate analysis of data and decision-making within fractions of a second (milliseconds to one second). It is used in systems requiring real-time responses, such as stock market transactions, IoT systems, and cybersecurity.\n📌 Characteristics:\n\n✅ Extremely low latency (milliseconds to seconds)\n\n✅ Enables instant system response\n\n✅ Requires high computational power and scalable architecture\n\n❌ More expensive and technologically complex than batch processing\n\n📌 Use Cases:\n\nHigh-Frequency Trading (HFT) – making stock market transaction decisions within milliseconds\n\nAutonomous Vehicles – real-time analysis of data streams from cameras and sensors\n\nCybersecurity – detecting network attacks in fractions of a second\n\nIoT Analytics – instant anomaly detection in industrial sensor data\n\n📌 Example Technologies:\n\nApache Flink\n\nApache Storm\n\nGoogle Dataflow\n\n🔎 Comparison:\n\n\n\n\n\n\n\n\n\nFeature\nBatch Processing\nNear Real-Time Analytics\nReal-Time Analytics\n\n\n\n\nLatency\nMinutes – hours – days\nSeconds – minutes\nMilliseconds – seconds\n\n\nProcessing Type\nBatch (offline)\nStreaming (but not fully immediate)\nStreaming (true real-time)\n\n\nInfrastructure Cost\n📉 Low\n📈 Medium\n📈📈 High\n\n\nImplementation Complexity\n📉 Simple\n📈 Medium\n📈📈 Difficult\n\n\nUse Cases\nReports, offline ML, historical analysis\nTransaction monitoring, dynamic ads\nHFT, IoT, real-time fraud detection\n\n\n\n📌 When to Use Batch Processing?\n\n✅ When immediate analysis is not required\n\n✅ When handling large volumes of data processed periodically\n\n✅ When aiming to reduce costs\n\n📌 When to Use Near Real-Time Analytics?\n\n✅ When analysis is needed within a short time (seconds – minutes)\n\n✅ When fresher data is required but not full real-time processing\n\n✅ When seeking a balance between performance and cost\n\n📌 When to Use Real-Time Analytics?\n\n✅ When every millisecond matters (e.g., stock trading, autonomous vehicles)\n\n✅ When detecting fraud, anomalies, or incidents instantly\n\n✅ When a system must respond to events immediately\n\nReal-time analytics is not always necessary—often, near real-time is sufficient and more cost-effective. The key is to understand business requirements before choosing the right solution."
  },
  {
    "objectID": "lecture1.html#why-is-real-time-analytics-important",
    "href": "lecture1.html#why-is-real-time-analytics-important",
    "title": "Lecture 1",
    "section": "Why is Real-Time Analytics Important?",
    "text": "Why is Real-Time Analytics Important?\nReal-time analytics is becoming increasingly crucial across various industries as it enables organizations to make immediate decisions based on up-to-date data. Here are some key reasons why real-time analytics matters:\n\nFaster Decision-Making\nReal-time analytics allows businesses to react to changes and events instantly. This is essential in dynamic environments such as:\n\nMarketing: Ads can be adjusted in real time based on user behavior (e.g., personalized content recommendations).\n\nFinance: Fraud detection in real-time, where every minute counts in preventing financial losses.\n\n\n\nReal-Time Monitoring\nCompanies can continuously track key operational metrics. Examples:\n\nIoT (Internet of Things): Monitoring the condition of machines and equipment in factories to detect failures and prevent downtime.\n\nHealthtech: Tracking patients’ vital signs and detecting anomalies, which can save lives.\n\n\n\nImproved Operational Efficiency\nReal-time analytics helps identify and address operational issues before they escalate. Examples:\n\nLogistics: Tracking shipments and monitoring transport status in real time to improve efficiency and reduce delays.\n\nRetail: Monitoring inventory levels in real-time and adjusting orders accordingly.\n\n\n\nCompetitive Advantage\nOrganizations leveraging real-time analytics gain an edge by responding faster to market changes, customer needs, and crises. With real-time insights:\n\nBusinesses can make proactive decisions ahead of competitors.\n\nCompanies can enhance customer relationships by responding instantly to their needs (e.g., adjusting offerings dynamically).\n\n\n\nEnhanced User Experience (Customer Experience)\nReal-time data analysis enables personalized user interactions as they happen. Examples:\n\nE-commerce: Analyzing shopping cart behavior in real time to offer discounts or remind users of abandoned items.\n\nStreaming Services: Optimizing video/streaming quality based on available bandwidth.\n\n\n\nAnomaly Detection and Threat Prevention\nIn today’s data-driven world, real-time anomaly detection is critical for security. Examples:\n\nCybersecurity: Detecting suspicious network activities and preventing attacks in real time (e.g., DDoS attacks, unauthorized logins).\n\nFraud Prevention: Instant identification of suspicious transactions in banking and credit card systems.\n\n\n\nCost Optimization\nReal-time analytics helps optimize resources and reduce costs. Examples:\n\nEnergy Management: Monitoring energy consumption in real time to optimize corporate energy expenses.\n\nSupply Chain Optimization: Tracking inventory and deliveries to reduce storage and transportation costs.\n\n\n\nPredictive Capabilities\nReal-time analytics supports predictive processes that anticipate future behaviors or problems and address them before they occur. Examples:\n\nPredictive Maintenance: Combining real-time data with predictive models to foresee machine failures.\n\nDemand Forecasting: Adjusting production or stock levels based on live market trends.\n\nReal-time analytics is not just about data analysis—it is a crucial element of business strategy in a world that demands agility, flexibility, and rapid adaptation. Companies that implement these technologies can significantly enhance their financial performance, customer service, operational efficiency, and competitive advantage."
  },
  {
    "objectID": "info.html",
    "href": "info.html",
    "title": "All tools",
    "section": "",
    "text": "For our first a few laboratories we will use just python codes. Check what is Your Python3 environment.\nIn the terminal try first:\npython\n# and\npython3\nI have python3 (You shouldn’t use python 2.7 version) so i create a new and a clear python environment.\nThe easiest way how to run a JupyterLab with your new python env. For  You can choose what You want.\npython3 -m venv &lt;name of Your env&gt;\n\nsource &lt;name of your env&gt;/bin/activate\n# . env/bin/activate\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# or\npip install -r requirements.txt\n\njupyterlab\ngo to web browser: localhost:8888\nIf You want rerun jupyterlab (after computer reset) just go to Your folder and run:\nsource &lt;name of your env&gt;/bin/activate\njupyterlab"
  },
  {
    "objectID": "info.html#python-env-with-jupyter-lab",
    "href": "info.html#python-env-with-jupyter-lab",
    "title": "All tools",
    "section": "",
    "text": "For our first a few laboratories we will use just python codes. Check what is Your Python3 environment.\nIn the terminal try first:\npython\n# and\npython3\nI have python3 (You shouldn’t use python 2.7 version) so i create a new and a clear python environment.\nThe easiest way how to run a JupyterLab with your new python env. For  You can choose what You want.\npython3 -m venv &lt;name of Your env&gt;\n\nsource &lt;name of your env&gt;/bin/activate\n# . env/bin/activate\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# or\npip install -r requirements.txt\n\njupyterlab\ngo to web browser: localhost:8888\nIf You want rerun jupyterlab (after computer reset) just go to Your folder and run:\nsource &lt;name of your env&gt;/bin/activate\njupyterlab"
  },
  {
    "objectID": "info.html#python-env-with-jupyterlab-docker-version",
    "href": "info.html#python-env-with-jupyterlab-docker-version",
    "title": "All tools",
    "section": "Python env with JupyterLAB Docker Version",
    "text": "Python env with JupyterLAB Docker Version\n\nCookiecutter project\nFrom GitHub repository You can find how to use a cookiecutter for any data science project or other kind of programs.\nTo run and build full dockerfile project: Create python env and install cookiecutter library.\npython3 -m venv venv\nsource venv/bin/activate\npip --no-cache install --upgrade pip setuptools\npip install cookiecutter\nand run:\ncookiecutter https://github.com/sebkaz/jupyterlab-project\nYou can run a cookiecutter project directly from GitHub repo.\nAnswer questions:\ncd jupyterlab\ndocker-compose up -d --build\nTo stop:\ndocker-compose down\n\n\nCookiecutter with config yaml file\n\nPython, Julia, R\nAll + Apache Spark\n\nClone repo and run:\npython3 -m cookiecutter https://github.com/sebkaz/jupyterlab-project --no-input --config-file=spark_template.yml --overwrite-if-exists"
  },
  {
    "objectID": "info.html#start-with-github",
    "href": "info.html#start-with-github",
    "title": "All tools",
    "section": "Start with GitHub",
    "text": "Start with GitHub\nText from web site\nWhen You working on a project, e.g. a master’s thesis, (alone or in a team) you often need to check what changes, when and by whom were introduced to the project. The “version control system” or GIT works great for this task.\nYou can download and install Git like a regular program on any computer. However, most often (small projects) you use websites with some kind of git system. One of the most recognized is GitHub (www.github.com) which allows you to use the git system without installing it on your computer.\nIn the free version of the GitHub website, you can store your files in public (everyone has access) repositories. We will only focus on the free version of GitHub:\ngit --version"
  },
  {
    "objectID": "info.html#github",
    "href": "info.html#github",
    "title": "All tools",
    "section": "GitHub",
    "text": "GitHub\nAt the highest level, there are individual accounts (eg. http://github.com/sebkaz or those set up by organizations. Individual users can create ** repositories ** public (public) or private (private).\nOne file should not exceed 100 MB.\nRepo (shortcut to repository) is created with Create a new repository. Each repo should have an individual name.\n\nBranches\nThe main (created by default) branch of the repository is named master.\n\n\nMost important commends\n\nclone of Your repository\n\ngit clone https://adres_repo.git\n\nIn github case, you can download the repository as a ‘zip’ file.\n\n\nRepository for local directory\n\n# new directory\nmkdir datamining\ncd datamining\n# init repo\ngit init\n# there sould be a .git new directory\n# add file\necho \"Info \" &gt;&gt; README.md\n\nlocal and web version connection\n\ngit remote add origin https://github.com/&lt;twojGit&gt;/nazwa.git\n\n3 steps\n\n# status check\ngit status\n# 1. add all changes\ngit add .\n# 2. commit all changes with message\ngit commit -m \" message \"\n# 3. and\ngit push origin master\nYou can watch Youtube course.\nAll the necessary programs will be delivered in the form of docker containers."
  },
  {
    "objectID": "info.html#start-with-docker",
    "href": "info.html#start-with-docker",
    "title": "All tools",
    "section": "Start with Docker",
    "text": "Start with Docker\nIn order to download the docer software to your system, go to the page.\nIf everything is installed correctly, follow these instructions:\n\nCheck the installed version\n\ndocker --version\n\nDownload and run the image Hello World and\n\ndocker run hello-world\n\nOverview of downloaded images:\n\ndocker image ls\n\ndocker images\n\nOverview of running containers:\n\ndocker ps \n\ndocker ps -all\n\nStopping a running container:\n\ndocker stop &lt;CONTAINER ID&gt;\n\nContainer removal\n\ndocker rm -f &lt;CONTAINER ID&gt;\nI also recommend short intro"
  },
  {
    "objectID": "info.html#docker-as-an-application-continuation-tool",
    "href": "info.html#docker-as-an-application-continuation-tool",
    "title": "All tools",
    "section": "Docker as an application continuation tool",
    "text": "Docker as an application continuation tool\nDocker with jupyter notebook"
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Books and WWW pages",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\n\n\n\n\n\nA. Bellemare Mikrousługi oparte na zdarzeniach. Wykorzystanie danych w organizacji na dużą skalę Zobacz opis lub Kup\nG. Shapira, T. Palino, R. Sivaram, K. Petty Kafka The Definitive Guide. Real-time data and stream processing at scale. Second edition, 2021. O’REILLY.\nJ. Korstanje Machine Learning for Streaming Data with Python, 2022. PACKT\n\n\n\n\n\nA. Geron Uczenie maszynowe z użyciem Scikit-Learn i TensorFlow. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nSebastian Raschka Python. Uczenie maszynowe. Wydanie II. Zobacz opis lub Kup książkę.\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocą pakietów Pandas i NumPy oraz środowiska IPython. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieć. Algorytmy przyszłości. Wydanie II (ebook) Zobacz opis lub Kup książkę, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup książkę, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup książkę.\nR. Schutt, C. O’Neil Badanie danych. Raport z pierwszej lini działań. Zobacz opis lub Kup książkę.\nT. Segaran Nowe usługi 2.0. Przewodnik po analizie zbiorów danych Zobacz opis lub Kup książkę, Kup e-book\nT. Morzy Eksploracja Danych. Metody i algorytmy, PWN, 2013.\nKrzyśko, Wołyński, Górecki, Skorzybut, Systemy uczące się . WNT, 2008\n\n\n\n\n\nF. Chollet Deep Learning. Praca z językiem Python i biblioteką Keras. Zobacz opis lub Kup książkę, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie głębokie z językiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup książkę, Kup e-book\nS. Weidman Uczenie głębokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup książkę, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyć komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup książkę, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programistów. Budowanie aplikacji AI za pomocą fastai i PyTorch Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nSpark. Zaawansowana analiza danych (ebook) Zobacz opis lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\nJ. Quddus Machine Learning with Apache Spark Quick Start Guide (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind Zrozumieć programowanie Zobacz opis lub Kup książkę, Kup e-book\nA. Allain C++. Przewodnik dla początkujących Zobacz opis lub Kup książkę, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdrażanie aplikacji Zobacz opis lub Kup książkę, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zadań z pythonem. Zobacz opis lub Kup książkę, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzędzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuka tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage.\nL. Suskind, Mechanika kwantowa, teoretyczne minimum, 2014, Prószyński i s-ka"
  },
  {
    "objectID": "books.html#books",
    "href": "books.html#books",
    "title": "Books and WWW pages",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\n\n\n\n\n\nA. Bellemare Mikrousługi oparte na zdarzeniach. Wykorzystanie danych w organizacji na dużą skalę Zobacz opis lub Kup\nG. Shapira, T. Palino, R. Sivaram, K. Petty Kafka The Definitive Guide. Real-time data and stream processing at scale. Second edition, 2021. O’REILLY.\nJ. Korstanje Machine Learning for Streaming Data with Python, 2022. PACKT\n\n\n\n\n\nA. Geron Uczenie maszynowe z użyciem Scikit-Learn i TensorFlow. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nSebastian Raschka Python. Uczenie maszynowe. Wydanie II. Zobacz opis lub Kup książkę.\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocą pakietów Pandas i NumPy oraz środowiska IPython. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieć. Algorytmy przyszłości. Wydanie II (ebook) Zobacz opis lub Kup książkę, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup książkę, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup książkę.\nR. Schutt, C. O’Neil Badanie danych. Raport z pierwszej lini działań. Zobacz opis lub Kup książkę.\nT. Segaran Nowe usługi 2.0. Przewodnik po analizie zbiorów danych Zobacz opis lub Kup książkę, Kup e-book\nT. Morzy Eksploracja Danych. Metody i algorytmy, PWN, 2013.\nKrzyśko, Wołyński, Górecki, Skorzybut, Systemy uczące się . WNT, 2008\n\n\n\n\n\nF. Chollet Deep Learning. Praca z językiem Python i biblioteką Keras. Zobacz opis lub Kup książkę, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie głębokie z językiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup książkę, Kup e-book\nS. Weidman Uczenie głębokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup książkę, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyć komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup książkę, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programistów. Budowanie aplikacji AI za pomocą fastai i PyTorch Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nSpark. Zaawansowana analiza danych (ebook) Zobacz opis lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\nJ. Quddus Machine Learning with Apache Spark Quick Start Guide (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind Zrozumieć programowanie Zobacz opis lub Kup książkę, Kup e-book\nA. Allain C++. Przewodnik dla początkujących Zobacz opis lub Kup książkę, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdrażanie aplikacji Zobacz opis lub Kup książkę, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zadań z pythonem. Zobacz opis lub Kup książkę, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzędzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuka tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage.\nL. Suskind, Mechanika kwantowa, teoretyczne minimum, 2014, Prószyński i s-ka"
  },
  {
    "objectID": "books.html#www-pages",
    "href": "books.html#www-pages",
    "title": "Books and WWW pages",
    "section": "WWW Pages",
    "text": "WWW Pages\n\nSoftware\n\nGithub\nGit-instrukcja\nwww.python.org\nPyPI python libraries\nAnaconda\nDocker\n\n\n\nPython libraries for data analysis\n\nNumPy\nSciPy\nPandas\nScikit-learn\nJupyter\nMatplotlib\nBeautiful Soup\nTheano\nKeras\nTensorFlow\nVirtual ENV\n\n\n\nText editors\n\nNotepad++\nSublime Text\nVisual Studio Code\n\n\n\nMarkdown\n\nMD\n\n\n\nJupyter notebook\n\nGaleria ciekawych notatników\nIntro\nKernels\nBringing the best out of jupyter for data science\nJupyter extensions\nI don’t like notebooks\nJupyter lab\nSpeed up jupyter notebook\n\n\n\nETL\n\ndata cookbook\n\n\n\nDatasets\n\nInternet Archive\nReddit\nKDnuggets\nKaggle\nList of datasets for machine learning research\nUCI Machine Learning Repo\nPublic API\nGoogle Datatset Search\n\n\n\nPython\n\nChris Albon Technical Notes on Using Data Science & AI\n40+ Python Statistics For Data Science Resources\nPractical Business Python\n\n\n\nML course\n\nKurs Machine Learning - Andrew Ng, Stanford"
  },
  {
    "objectID": "sylabus.html",
    "href": "sylabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Real Time Analytics\nSGH Warsaw School of Economics\nECTS: 3\nLanguage: EN\nlevel: medium\nday of week: Monday/Tuesday\nTeacher: Sebastian Zając, sebastian.zajac@sgh.waw.pl\nWebsite: http://sebkaz-teaching.github.io/EN"
  },
  {
    "objectID": "sylabus.html#description",
    "href": "sylabus.html#description",
    "title": "Syllabus",
    "section": "Description",
    "text": "Description\nMaking the right decisions based on data and their analysis in business is a process and daily. Modern methods of modeling by machine learning (ML), artificial intelligence (AI), or deep learning not only allow better understanding of business, but also support making key decisions for it. The development of technology and increasingly new business concepts of working directly with the client require not only correct but also fast decisions. The classes offered are designed to provide students with experience and comprehensive theoretical knowledge in the field of real-time data processing and analysis, and to present the latest technologies (free and commercial) for the processing of structured data (originating e.g. from data warehouses) and unstructured (e.g. images, sound, video streaming) in on-line mode. The course will present the so called lambda and kappa structures for data processing into data lake along with a discussion of the problems and difficulties encountered in implementing real-time modeling for large amounts of data. Theoretical knowledge will be gained (apart from the lecture part) through the implementation of test cases in tools such as Apache Spark, Nifi, Microsoft Azure and SAS. During laboratory classes student will benefit from fully understand the latest information technologies related to real-time data processing."
  },
  {
    "objectID": "sylabus.html#list-of-topics",
    "href": "sylabus.html#list-of-topics",
    "title": "Syllabus",
    "section": "List of Topics",
    "text": "List of Topics\n\nModelling, learning and prediction in batch mode (offline learning) and incremental (online learning) modes. Problems of incremental machine learning.\nData processing models in Big Data. From flat files to Data Lake. Real-time data myth and facts\nNRT systems (near real-time systems), data acquisition, streaming and analytics.\nAlgorithms for estimating model parameters in incremental mode. Stochastic Gradient Descent.\nLambda and Kappa architecture. Designing IT architecture for real-time data processing.\nPreparation of the micro-service with the ML model for prediction use.\nStructured and unstructured data. Relational databases and NoSQL databases.\nAggregations and reporting in NoSQL databases (on the example of the MongoDB or Cassandra)\nBasic of object-oriented programming in Python in linear and logistic regression, neural network analysis using the sklearn, TensorFlow and Keras.\nIT architecture of Big Data processing. Preparation of a virtual env for Apache Spark."
  },
  {
    "objectID": "sylabus.html#conditions-for-passing",
    "href": "sylabus.html#conditions-for-passing",
    "title": "Syllabus",
    "section": "Conditions for passing",
    "text": "Conditions for passing\n\ntest 30%\npractical test 30% (IF)\ngroup project 40% (70%)"
  },
  {
    "objectID": "sylabus.html#books",
    "href": "sylabus.html#books",
    "title": "Syllabus",
    "section": "Books",
    "text": "Books\n\nS. Zajac, “Modelowanie dla biznesu, Analityka w czasie rzeczywistym - narzędzia informatyczne i biznesowe”. SGH (2022)\nFrątczak E., red. “Modelowanie dla biznesu, Regresja logistyczna, Regresja Poissona, Survival Data Mining, CRM, Credit Scoring”. SGH, Warszawa 2019.\nFrątczak E., red., “Zaawansowane metody analiz statystycznych”, Oficyna Wydawnicza SGH, Warszawa 2012.\nIndest A., Wild Knowledge. Outthik the Revolution. LID publishing.com 2017.\nReal Time Analytic. “The Key to Unlocking Customer Insights & Driving the Customer Experience”. Harvard Business Review Analytics Series, Harvard Business School Publishing, 2018.\nSvolba G., “Applying Data Science. Business Case Studies Using SAS”. SAS Institute Inc., Cary NC, USA, 2017.\nEllis B. “Real-Time Analytics Techniques to Analyze and Visualize Streaming data.” , Wiley, 2014\nFamiliar B., Barnes J. “Business in Real-Time Using Azure IoT and Cortana Intelligence Suite” Apress, 2017"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Info",
    "section": "",
    "text": "Code: 222891-D\nSemester: 2024/2025 University: SGH Warsaw School of Economics\nBasic course information can be found in the sylabus.\nRecommended reading is available in the books section."
  },
  {
    "objectID": "index.html#real-time-analytics",
    "href": "index.html#real-time-analytics",
    "title": "Info",
    "section": "",
    "text": "Code: 222891-D\nSemester: 2024/2025 University: SGH Warsaw School of Economics\nBasic course information can be found in the sylabus.\nRecommended reading is available in the books section."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Info",
    "section": "Schedule",
    "text": "Schedule\n\nLectures\nThe lectures are conducted in a hybrid mode. Attendance is optional, and in-person sessions take place in Aula VI, Building G.\n\n\n18-02-2025 (Tuesday) 11:40-13:20 - Lecture 1\n\n25-02-2025 (Tuesday) 11:40-13:20 - Lecture 2\n04-03-2025 (Tuesday) 11:40-13:20 - Lecture 3\n11-03-2025 (Tuesday) 11:40-13:20 - Lecture 4\n18-03-2025 (Tuesday) 11:40-13:20 - Lecture 5\n\nThe lecture 5 concludes with a TEST.\nFormat: 20 questions | 30 minutes Platform: MS Teams\n\n\nExam\nLectures will end with a test (last class). Positive evaluation of the test (above 13 points) entitles you to carry out the exercises.\nAfter the exercises, homework will be carried out via the MS teams’ platform. Passing all exercises and tasks entitles you to complete the project.\nThe project should be carried out in groups of no more than 5 people.\nProject requirements:\n\nThe project should present a BUSINESS PROBLEM that can be implemented using the information provided online. (This does not mean that you cannot use batch processing, e.g. to generate a model).\nData should be sent to Apache Kafka and further processed and analyzed from there.\nThe programming language is free - applies to each component of the project.\nBI tools can be used\nData sources can be a table, artificially generated data, IoT, etc."
  },
  {
    "objectID": "index.html#technology",
    "href": "index.html#technology",
    "title": "Info",
    "section": "Technology",
    "text": "Technology\nParticipating in the classes, you must know and at least use the following information technologies:\n\nGIT\nPython, Jupyter notebook, Jupyter lab, Colab\nDocker\nApache Spark, Apache Flink, Apache Kafka, Apache Beam\nDatabricks Community edition Web page."
  }
]