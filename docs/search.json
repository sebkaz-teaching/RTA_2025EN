[
  {
    "objectID": "lecture2.html",
    "href": "lecture2.html",
    "title": "Lecture 2",
    "section": "",
    "text": "⏳ Duration: 1.5h\n🎯 Lecture Objective\nUnderstanding how data has evolved in different industries and the tools used for its analysis today.\nIn this lecture, we will present the evolution of data analysis, showing how technologies and approaches to data processing have changed over the years.\nWe will start with classical tabular structures, move through more advanced graph and text models, and finish with modern approaches to stream processing."
  },
  {
    "objectID": "lecture2.html#tabular-data-sql-tables",
    "href": "lecture2.html#tabular-data-sql-tables",
    "title": "Lecture 2",
    "section": "1. Tabular Data (SQL Tables)",
    "text": "1. Tabular Data (SQL Tables)\nInitially, data was stored in tables, where each table contained organized information in columns and rows (e.g., SQL databases).\nSuch models were perfect for structured data.\n\n📌 Features:\n✅ Data divided into columns with a fixed structure.\n✅ CRUD operations (Create, Read, Update, Delete) can be applied.\n✅ Strict consistency and normalization rules.\n\n\n📌 Examples:\n➡️ Banking systems, e-commerce, ERP, CRM systems.\n\n\n🖥️ Example Python Code (SQLite):\nimport sqlite3\nconn = sqlite3.connect(':memory:')\ncursor = conn.cursor()\ncursor.execute(\"CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT, age INTEGER)\")\ncursor.execute(\"INSERT INTO users (name, age) VALUES ('Alice', 30)\")\ncursor.execute(\"SELECT * FROM users\")\nprint(cursor.fetchall())\nconn.close()"
  },
  {
    "objectID": "lecture2.html#graph-data",
    "href": "lecture2.html#graph-data",
    "title": "Lecture 2",
    "section": "2. Graph Data",
    "text": "2. Graph Data\nAs business needs grew, graph data emerged, where relationships between objects are represented as nodes and edges.\n\n📌 Features:\n✅ Data describing relationships and connections. ✅ Flexible structure (graphs instead of tables). ✅ Allows analysis of connections (e.g., PageRank algorithms, centrality).\n\n\n📌 Examples:\n➡️ Social networks (Facebook, LinkedIn), search engines (Google), recommendation systems (Netflix, Amazon).\n\n\n🖥️ Example Python Code (Karate Graph - NetworkX) :\n\n\nCode\nimport networkx as nx\nG = nx.karate_club_graph()\nnx.draw(G, with_labels=True)"
  },
  {
    "objectID": "lecture2.html#semi-structured-data-json-xml-yaml",
    "href": "lecture2.html#semi-structured-data-json-xml-yaml",
    "title": "Lecture 2",
    "section": "3. Semi-structured Data (JSON, XML, YAML)",
    "text": "3. Semi-structured Data (JSON, XML, YAML)\nThese data are not fully structured like in SQL databases, but they have some schema.\n\n📌 Features:\n✅ Hierarchical structure (e.g., key-value pairs, nested objects). ✅ No strict schema (possibility to add new fields). ✅ Popular in NoSQL systems and APIs.\n\n\n📌 Examples:\n➡️ Documents in MongoDB, configuration files, REST APIs, log files.\n\n\n🖥️ Example Python Code (JSON):\n\n\nCode\nimport json\ndata = {'name': 'Alice', 'age': 30, 'city': 'New York'}\njson_str = json.dumps(data)\nprint(json.loads(json_str))\n\n\n{'name': 'Alice', 'age': 30, 'city': 'New York'}"
  },
  {
    "objectID": "lecture2.html#text-data-nlp",
    "href": "lecture2.html#text-data-nlp",
    "title": "Lecture 2",
    "section": "4. Text Data (NLP)",
    "text": "4. Text Data (NLP)\nText has become a key source of information, especially in sentiment analysis, chatbots, and search engines.\n\n📌 Features:\n✅ Unstructured data requiring transformation. ✅ Use of embeddings (e.g., Word2Vec, BERT, GPT). ✅ Widely used in sentiment analysis and chatbots.\n\n\n📌 Examples:\n➡️ Social media, emails, chatbots, machine translation.\n\n\n🖥️ Example Python Code :\n\n\nCode\nimport ollama\n\n# Przykładowe zdanie\nsentence = \"Artificial intelligence is changing the world.\"\nresponse = ollama.embeddings(model='llama3.2', prompt=sentence)\nembedding = response['embedding']\nprint(embedding[:4])\n\n\n[-2.021953582763672, 1.5604140758514404, -0.5358548164367676, -1.3182345628738403]"
  },
  {
    "objectID": "lecture2.html#multimedia-data-images-sound-video",
    "href": "lecture2.html#multimedia-data-images-sound-video",
    "title": "Lecture 2",
    "section": "5. Multimedia Data (Images, Sound, Video)",
    "text": "5. Multimedia Data (Images, Sound, Video)\nModern data analysis systems also use images and sound.\n\n📌 Features:\n✅ Require significant computational power (AI, deep learning). ✅ Processed by CNN models (images) and RNN/Transformers (sound).\n\n\n📌 Examples:\n➡️ Face recognition, speech analysis, biometrics, video content analysis.\n\n\n🖥️ Example Python Code (Image - OpenCV) :\nimport cv2\nimage = cv2.imread('cloud.jpeg')\ncv2.waitKey(0)\ncv2.destroyAllWindows()"
  },
  {
    "objectID": "lecture2.html#hadoop-map-reduce-scaling-computation-on-big-data",
    "href": "lecture2.html#hadoop-map-reduce-scaling-computation-on-big-data",
    "title": "Lecture 2",
    "section": "Hadoop Map-Reduce – Scaling Computation on Big Data",
    "text": "Hadoop Map-Reduce – Scaling Computation on Big Data\nWhen we talk about scalable data processing, the first association might be Google.\nBut what actually enables us to search for information in a fraction of a second while processing petabytes of data?\n👉 Did you know that the name “Google” comes from the word “Googol,” which represents the number 10¹⁰⁰?\nThat’s more than the number of atoms in the known universe! 🌌\n\n🔥 Challenge: Can you write out the number Googol by the end of this lecture?"
  },
  {
    "objectID": "lecture2.html#why-are-sql-and-traditional-algorithms-insufficient",
    "href": "lecture2.html#why-are-sql-and-traditional-algorithms-insufficient",
    "title": "Lecture 2",
    "section": "🔍 Why Are SQL and Traditional Algorithms Insufficient?",
    "text": "🔍 Why Are SQL and Traditional Algorithms Insufficient?\nTraditional SQL databases and single-threaded algorithms fail when data scales beyond a single computer.\nThis is where MapReduce comes in—a revolutionary computational model developed by Google.\n\n🛠️ Google’s Solutions for Big Data:\n✅ Google File System (GFS) – a distributed file system.\n✅ Bigtable – a system for storing massive amounts of structured data.\n✅ MapReduce – an algorithm for distributing workloads across multiple machines."
  },
  {
    "objectID": "lecture2.html#graphical-representation-of-mapreduce",
    "href": "lecture2.html#graphical-representation-of-mapreduce",
    "title": "Lecture 2",
    "section": "🖼️ Graphical Representation of MapReduce",
    "text": "🖼️ Graphical Representation of MapReduce\n\n1. Mapping Splits Tasks (Map)\nEach input is divided into smaller parts and processed in parallel.\n🌍 Imagine you have a phone book and want to find all people with the last name “Nowak”.\n➡️ Divide the book into sections and give each person one section to analyze.\n\n\n2. Reducing Gathers the Results (Reduce)\nAll partial results are combined into one final answer.\n🔄 All students report their findings, and one student collects and summarizes the response."
  },
  {
    "objectID": "lecture2.html#classic-example-counting-words-in-a-text",
    "href": "lecture2.html#classic-example-counting-words-in-a-text",
    "title": "Lecture 2",
    "section": "💡 Classic Example: Counting Words in a Text",
    "text": "💡 Classic Example: Counting Words in a Text\nLet’s assume we have millions of books and we want to count how many times each word appears.\n\n🖥️ MapReduce Code in Python (Using Multiprocessing)\nfrom multiprocessing import Pool\nfrom collections import Counter\n\n# Map function (splitting text into words)\ndef map_function(text):\n    words = text.split()\n    return Counter(words)\n\n# Reduce function (summing up results)\ndef reduce_function(counters):\n    total_count = Counter()\n    for counter in counters:\n        total_count.update(counter)\n    return total_count\n\ntexts = [\n        \"big data is amazing\",\n        \"data science and big data\",\n        \"big data is everywhere\"\n    ]\nif __name__ == '__main__':    \n    with Pool() as pool:\n        mapped_results = pool.map(map_function, texts)\n    \n    final_result = reduce_function(mapped_results)\n    print(final_result)\n\n# Counter({'data': 4, 'big': 3, 'is': 2, 'amazing': 1, 'science': 1, 'and': 1, 'everywhere': 1})\n\n\n🔹 What’s Happening Here?\n✅ Each text fragment is processed independently (map). ✅ The results are collected and summed (reduce). ✅ Outcome: We can process terabytes of text in parallel!"
  },
  {
    "objectID": "lecture2.html#visualization-comparison-of-the-classic-approach-and-mapreduce",
    "href": "lecture2.html#visualization-comparison-of-the-classic-approach-and-mapreduce",
    "title": "Lecture 2",
    "section": "🎨 Visualization – Comparison of the Classic Approach and MapReduce",
    "text": "🎨 Visualization – Comparison of the Classic Approach and MapReduce\n📊 Old Approach – A single computer processes everything sequentially.\n📊 New Approach (MapReduce) – Each machine processes a fragment, and the results are aggregated."
  },
  {
    "objectID": "lecture2.html#challenge-for-you",
    "href": "lecture2.html#challenge-for-you",
    "title": "Lecture 2",
    "section": "🚀 Challenge for You!",
    "text": "🚀 Challenge for You!\n🔹 Find and run your own MapReduce algorithm in any programming language!\n🔹 Can you implement your own MapReduce for a different task? (e.g., log analysis, counting website clicks)"
  },
  {
    "objectID": "lecture2.html#big-data-1",
    "href": "lecture2.html#big-data-1",
    "title": "Lecture 2",
    "section": "Big Data",
    "text": "Big Data\nBig Data systems can serve as a source for data warehouses (e.g., Data Lake, Enterprise Data Hub).\nHowever, Data Warehouses are not Big Data systems!\n\n1. Data Warehouses\n\nStore highly structured data\n\nFocused on analytics and reporting processes\n\n100% accuracy\n\n\n\n2. Big Data\n\nCan handle data of any structure\n\nUsed for various data-driven purposes (analytics, data science, etc.)\n\nLess than 100% accuracy\n\n\n“Big Data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so everyone claims they are doing it.”\n— Dan Ariely, Professor of Psychology and Behavioral Economics, Duke University\n\n\n\nOne, Two, … Four V\n\nVolume – The size of data produced worldwide is growing at an exponential rate.\n\nVelocity – The speed at which data is generated, transmitted, and processed.\n\nVariety – Traditional data is alphanumeric, consisting of letters and numbers. Today, we also deal with images, audio, video files, and IoT data streams.\n\nVeracity – Are the data complete and accurate? Do they objectively reflect reality? Are they a reliable basis for decision-making?\n\nValue – The actual worth of the data. Ultimately, it’s about cost and benefits.\n\n\n“The purpose of computing is insight, not numbers.” – R.W. Hamming, 1962."
  },
  {
    "objectID": "lecture2.html#data-processing-models",
    "href": "lecture2.html#data-processing-models",
    "title": "Lecture 2",
    "section": "Data Processing Models",
    "text": "Data Processing Models\nData has always been processed in business.\nOver the past decades, the amount of processed data has been steadily increasing, affecting the way data is prepared and handled.\n\nA Bit of History\n\n1960s: Data collections, databases\n\n1970s: Relational data models and their implementation in OLTP systems\n\n1975: First personal computers\n\n1980s: Advanced data models (extended-relational, object-oriented, application-oriented, etc.)\n\n1983: The beginning of the Internet\n\n1990s: Data mining, data warehouses, OLAP systems\n\nLater: NoSQL, Hadoop, SPARK, data lakes\n\n2002: AWS, 2005: Hadoop, Cloud computing\n\n\nMost data is stored in databases or data warehouses.\nTypically, data access is performed through applications by executing queries.\nThe method of utilizing and accessing a database is called the data processing model.\nThe two most commonly used implementations are:\n\n\nTraditional Model\nThe traditional model refers to online transaction processing (OLTP),\nwhich excels at handling real-time tasks such as customer service, order management, and sales processing.\nIt is commonly used in Enterprise Resource Planning (ERP) systems, Customer Relationship Management (CRM) software, and web-based applications.\n\nThis model provides efficient solutions for:\n\nSecure and efficient data storage\n\nTransactional data recovery after failures\n\nOptimized data access\n\nConcurrency management\n\nEvent processing → read → write\n\nHowever, what happens when we need to deal with:\n\nAggregation of data from multiple systems (e.g., multiple stores)\n\nReporting and data summarization\n\nOptimization of complex queries\n\nBusiness decision support\n\nResearch on these topics led to the formulation of a new data processing model and a new type of database – Data Warehouses.\n\n\nOLAP Model\nOnline Analytical Processing (OLAP)\nOLAP supports data analysis and provides tools for multidimensional analysis\nbased on dimensions such as time, location, and product.\nThe process of extracting data from various systems into a single database is known as Extract-Transform-Load (ETL),\nwhich involves normalization, encoding, and schema transformation.\nAnalyzing data in a data warehouse mainly involves calculating aggregates (summaries) across different dimensions.\nThis process is entirely user-driven.\n\n\nExample\nImagine we have access to a data warehouse storing sales information from a supermarket.\nHow can we analyze queries such as:\n\nWhat is the total sales of products in the subsequent quarters, months, and weeks?\nWhat is the sales breakdown by product categories?\nWhat is the sales breakdown by supermarket branches?\n\nAnswers to these questions help identify bottlenecks in product sales, plan inventory levels, and compare sales across different product groups and supermarket branches.\nIn a Data Warehouse, two types of queries are most commonly executed (both in batch mode):\n\nPeriodic queries that generate business statistics, such as reporting queries.\nAd-hoc queries that support critical business decisions."
  },
  {
    "objectID": "lecture1.html",
    "href": "lecture1.html",
    "title": "Lecture 1",
    "section": "",
    "text": "⏳ Duration: 1.5h\n🎯 Lecture Goal\nIntroducing students to the fundamentals of real-time analytics, the differences between data processing modes (batch, streaming, real-time), as well as key applications and challenges."
  },
  {
    "objectID": "lecture1.html#what-is-real-time-data-analytics",
    "href": "lecture1.html#what-is-real-time-data-analytics",
    "title": "Lecture 1",
    "section": "What is Real-Time Data Analytics?",
    "text": "What is Real-Time Data Analytics?\n\nDefinition and Key Concepts\nReal-Time Data Analytics is the process of processing and analyzing data immediately after it is generated, without the need for storage and later processing. The goal is to obtain instant insights and responses to changing conditions in business, technology, and scientific systems.\n\n\nKey Features of Real-Time Data Analytics:\n\nLow latency – data is analyzed within milliseconds or seconds of being generated.\nStreaming vs. Batch Processing – data analysis can occur continuously (streaming) or at predefined intervals (batch).\nIntegration with IoT, AI, and ML – real-time analytics often works in conjunction with the Internet of Things (IoT) and artificial intelligence algorithms.\nReal-time decision-making – e.g., instant fraud detection in banking transactions."
  },
  {
    "objectID": "lecture1.html#business-applications-of-real-time-data-analytics",
    "href": "lecture1.html#business-applications-of-real-time-data-analytics",
    "title": "Lecture 1",
    "section": "Business Applications of Real-Time Data Analytics",
    "text": "Business Applications of Real-Time Data Analytics\n\nFinance and Banking\n\nFraud Detection – real-time transaction analysis helps identify anomalies indicating fraud.\nAutomated Trading – HFT (High-Frequency Trading) systems analyze millions of data points in fractions of a second.\nDynamic Credit Scoring – instant risk assessment of a customer’s creditworthiness.\n\n\n\nE-Commerce and Digital Marketing\n\nReal-Time Offer Personalization – dynamic product recommendations based on users’ current behavior.\nDynamic Pricing – companies like Uber, Amazon, and hotels adjust prices in real time based on demand.\nSocial Media Monitoring – sentiment analysis of customer feedback and immediate response to negative comments.\n\n\n\nTelecommunications and IoT\n\nNetwork Infrastructure Monitoring – real-time log analysis helps detect failures before they occur.\nSmart Cities – real-time traffic analysis optimizes traffic light systems dynamically.\nIoT Analytics – IoT devices generate data streams that can be analyzed in real time (e.g., smart energy meters).\n\n\n\nHealthcare\n\nPatient Monitoring – real-time analysis of medical device signals to detect life-threatening conditions instantly.\nEpidemiological Analytics – tracking disease outbreaks based on real-time data.\n\nReal-time data analytics is a key component of modern IT systems, enabling businesses to make faster and more precise decisions. It is widely used across industries—from finance and e-commerce to healthcare and IoT."
  },
  {
    "objectID": "lecture1.html#differences-between-batch-processing-near-real-time-analytics-and-real-time-analytics",
    "href": "lecture1.html#differences-between-batch-processing-near-real-time-analytics-and-real-time-analytics",
    "title": "Lecture 1",
    "section": "Differences Between Batch Processing, Near Real-Time Analytics, and Real-Time Analytics",
    "text": "Differences Between Batch Processing, Near Real-Time Analytics, and Real-Time Analytics\nThere are three main approaches to data processing:\n\nBatch Processing\n\nNear Real-Time Analytics\n\nReal-Time Analytics\n\nEach differs in processing speed, technological requirements, and business applications.\n\nBatch Processing\n📌 Definition:\nBatch Processing involves collecting large amounts of data and processing them at scheduled intervals (e.g., hourly, daily, or weekly).\n📌 Characteristics:\n\n✅ High efficiency for large datasets\n\n✅ Processes data after it has been collected\n\n✅ Does not require immediate analysis\n\n✅ Typically cheaper than real-time processing\n\n❌ Delays – results are available only after processing is complete\n\n📌 Use Cases:\n\nGenerating financial reports at the end of a day/month\n\nAnalyzing sales trends based on historical data\n\nCreating offline machine learning models\n\n📌 Example Technologies:\n\nHadoop MapReduce\n\nApache Spark (in batch mode)\n\nGoogle BigQuery\n\nimport pandas as pd  \ndf = pd.read_csv(\"transactions.csv\")  \n\ndf['transaction_date'] = pd.to_datetime(df['transaction_date'])\ndf['month'] = df['transaction_date'].dt.to_period('M') \n\n# Agg\nmonthly_sales = df.groupby(['month'])['amount'].sum()\n\nmonthly_sales.to_csv(\"monthly_report.csv\")  \n\nprint(\"Raport save!\")\nIf you wanted to create data for an example.\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)\ndata = {\n    'transaction_id': [f'TX{str(i).zfill(4)}' for i in range(1, 1001)],\n    'amount': np.random.uniform(10, 10000, 1000), \n    'transaction_date': pd.date_range(start=\"2025-01-01\", periods=1000, freq='h'), \n    'merchant': np.random.choice(['Merchant_A', 'Merchant_B', 'Merchant_C', 'Merchant_D'], 1000),\n    'card_type': np.random.choice(['Visa', 'MasterCard', 'AmEx'], 1000)\n}\n\ndf = pd.DataFrame(data)\ncsv_file = 'transactions.csv'\ndf.to_csv(csv_file, index=False)"
  },
  {
    "objectID": "lecture1.html#near-real-time-analytics-analysis-nearly-in-real-time",
    "href": "lecture1.html#near-real-time-analytics-analysis-nearly-in-real-time",
    "title": "Lecture 1",
    "section": "Near Real-Time Analytics – Analysis Nearly in Real Time",
    "text": "Near Real-Time Analytics – Analysis Nearly in Real Time\n📌 Definition:\nNear Real-Time Analytics refers to data analysis that occurs with minimal delay (typically from a few seconds to a few minutes). It is used in scenarios where full real-time analysis is not necessary, but excessive delays could impact business operations.\n📌 Characteristics:\n\n✅ Processes data in short intervals (a few seconds to minutes)\n\n✅ Enables quick decision-making but does not require millisecond-level reactions\n\n✅ Optimal balance between cost and speed\n\n❌ Not suitable for systems requiring immediate response\n\n📌 Use Cases:\n\nMonitoring banking transactions and detecting fraud (e.g., analysis within 30 seconds)\n\nDynamically adjusting online ads based on user behavior\n\nAnalyzing server and network logs to detect anomalies\n\n📌 Example Technologies:\n\nApache Kafka + Spark Streaming\n\nElasticsearch + Kibana (e.g., IT log analysis)\n\nAmazon Kinesis\n\nExample of a data producer sending transactions to an Apache Kafka system.\nfrom kafka import KafkaProducer\nimport json\nimport random\nimport time\nfrom datetime import datetime\n\n# Ustawienia dla producenta\nbootstrap_servers = 'localhost:9092'\ntopic = 'transactions' \n\n\ndef generate_transaction():\n    transaction = {\n        'transaction_id': f'TX{random.randint(1000, 9999)}',\n        'amount': round(random.uniform(10, 10000), 2),  \n        'transaction_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        'merchant': random.choice(['Merchant_A', 'Merchant_B', 'Merchant_C', 'Merchant_D']),\n        'card_type': random.choice(['Visa', 'MasterCard', 'AmEx']),\n    }\n    return transaction\n\nproducer = KafkaProducer(\n    bootstrap_servers=bootstrap_servers,\n    value_serializer=lambda v: json.dumps(v).encode('utf-8') \n)\n\n\nfor _ in range(1000):  \n    transaction = generate_transaction()\n    producer.send(topic, value=transaction) \n    print(f\"Sent: {transaction}\")\n    time.sleep(1) \n\nproducer.flush()\nproducer.close()\nConsument example\nfrom kafka import KafkaConsumer\nimport json  \n\nconsumer = KafkaConsumer(\n    'transactions',\n    bootstrap_servers='localhost:9092',\n    auto_offset_reset='earliest',\n    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n)\n\n\nfor message in consumer:\n    transaction = message.value\n    if transaction[\"amount\"] &gt; 8000:\n        print(f\"🚨 Anomaly transaction: {transaction}\")\nExample of the DataSet\n{\n    \"transaction_id\": \"TX1234\",\n    \"amount\": 523.47,\n    \"transaction_date\": \"2025-02-11 08:10:45\",\n    \"merchant\": \"Merchant_A\",\n    \"card_type\": \"Visa\"\n}\n\nReal-Time Analytics\n📌 Definition:\nReal-Time Analytics refers to the immediate analysis of data and decision-making within fractions of a second (milliseconds to one second). It is used in systems requiring real-time responses, such as stock market transactions, IoT systems, and cybersecurity.\n📌 Characteristics:\n\n✅ Extremely low latency (milliseconds to seconds)\n\n✅ Enables instant system response\n\n✅ Requires high computational power and scalable architecture\n\n❌ More expensive and technologically complex than batch processing\n\n📌 Use Cases:\n\nHigh-Frequency Trading (HFT) – making stock market transaction decisions within milliseconds\n\nAutonomous Vehicles – real-time analysis of data streams from cameras and sensors\n\nCybersecurity – detecting network attacks in fractions of a second\n\nIoT Analytics – instant anomaly detection in industrial sensor data\n\n📌 Example Technologies:\n\nApache Flink\n\nApache Storm\n\nGoogle Dataflow\n\n🔎 Comparison:\n\n\n\n\n\n\n\n\n\nFeature\nBatch Processing\nNear Real-Time Analytics\nReal-Time Analytics\n\n\n\n\nLatency\nMinutes – hours – days\nSeconds – minutes\nMilliseconds – seconds\n\n\nProcessing Type\nBatch (offline)\nStreaming (but not fully immediate)\nStreaming (true real-time)\n\n\nInfrastructure Cost\n📉 Low\n📈 Medium\n📈📈 High\n\n\nImplementation Complexity\n📉 Simple\n📈 Medium\n📈📈 Difficult\n\n\nUse Cases\nReports, offline ML, historical analysis\nTransaction monitoring, dynamic ads\nHFT, IoT, real-time fraud detection\n\n\n\n📌 When to Use Batch Processing?\n\n✅ When immediate analysis is not required\n\n✅ When handling large volumes of data processed periodically\n\n✅ When aiming to reduce costs\n\n📌 When to Use Near Real-Time Analytics?\n\n✅ When analysis is needed within a short time (seconds – minutes)\n\n✅ When fresher data is required but not full real-time processing\n\n✅ When seeking a balance between performance and cost\n\n📌 When to Use Real-Time Analytics?\n\n✅ When every millisecond matters (e.g., stock trading, autonomous vehicles)\n\n✅ When detecting fraud, anomalies, or incidents instantly\n\n✅ When a system must respond to events immediately\n\nReal-time analytics is not always necessary—often, near real-time is sufficient and more cost-effective. The key is to understand business requirements before choosing the right solution."
  },
  {
    "objectID": "lecture1.html#why-is-real-time-analytics-important",
    "href": "lecture1.html#why-is-real-time-analytics-important",
    "title": "Lecture 1",
    "section": "Why is Real-Time Analytics Important?",
    "text": "Why is Real-Time Analytics Important?\nReal-time analytics is becoming increasingly crucial across various industries as it enables organizations to make immediate decisions based on up-to-date data. Here are some key reasons why real-time analytics matters:\n\nFaster Decision-Making\nReal-time analytics allows businesses to react to changes and events instantly. This is essential in dynamic environments such as:\n\nMarketing: Ads can be adjusted in real time based on user behavior (e.g., personalized content recommendations).\n\nFinance: Fraud detection in real-time, where every minute counts in preventing financial losses.\n\n\n\nReal-Time Monitoring\nCompanies can continuously track key operational metrics. Examples:\n\nIoT (Internet of Things): Monitoring the condition of machines and equipment in factories to detect failures and prevent downtime.\n\nHealthtech: Tracking patients’ vital signs and detecting anomalies, which can save lives.\n\n\n\nImproved Operational Efficiency\nReal-time analytics helps identify and address operational issues before they escalate. Examples:\n\nLogistics: Tracking shipments and monitoring transport status in real time to improve efficiency and reduce delays.\n\nRetail: Monitoring inventory levels in real-time and adjusting orders accordingly.\n\n\n\nCompetitive Advantage\nOrganizations leveraging real-time analytics gain an edge by responding faster to market changes, customer needs, and crises. With real-time insights:\n\nBusinesses can make proactive decisions ahead of competitors.\n\nCompanies can enhance customer relationships by responding instantly to their needs (e.g., adjusting offerings dynamically).\n\n\n\nEnhanced User Experience (Customer Experience)\nReal-time data analysis enables personalized user interactions as they happen. Examples:\n\nE-commerce: Analyzing shopping cart behavior in real time to offer discounts or remind users of abandoned items.\n\nStreaming Services: Optimizing video/streaming quality based on available bandwidth.\n\n\n\nAnomaly Detection and Threat Prevention\nIn today’s data-driven world, real-time anomaly detection is critical for security. Examples:\n\nCybersecurity: Detecting suspicious network activities and preventing attacks in real time (e.g., DDoS attacks, unauthorized logins).\n\nFraud Prevention: Instant identification of suspicious transactions in banking and credit card systems.\n\n\n\nCost Optimization\nReal-time analytics helps optimize resources and reduce costs. Examples:\n\nEnergy Management: Monitoring energy consumption in real time to optimize corporate energy expenses.\n\nSupply Chain Optimization: Tracking inventory and deliveries to reduce storage and transportation costs.\n\n\n\nPredictive Capabilities\nReal-time analytics supports predictive processes that anticipate future behaviors or problems and address them before they occur. Examples:\n\nPredictive Maintenance: Combining real-time data with predictive models to foresee machine failures.\n\nDemand Forecasting: Adjusting production or stock levels based on live market trends.\n\nReal-time analytics is not just about data analysis—it is a crucial element of business strategy in a world that demands agility, flexibility, and rapid adaptation. Companies that implement these technologies can significantly enhance their financial performance, customer service, operational efficiency, and competitive advantage."
  },
  {
    "objectID": "info.html",
    "href": "info.html",
    "title": "All tools",
    "section": "",
    "text": "For our first a few laboratories we will use just python codes. Check what is Your Python3 environment.\nIn the terminal try first:\npython\n# and\npython3\nI have python3 (You shouldn’t use python 2.7 version) so i create a new and a clear python environment.\nThe easiest way how to run a JupyterLab with your new python env. For  You can choose what You want.\npython3 -m venv &lt;name of Your env&gt;\n\nsource &lt;name of your env&gt;/bin/activate\n# . env/bin/activate\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# or\npip install -r requirements.txt\n\njupyterlab\ngo to web browser: localhost:8888\nIf You want rerun jupyterlab (after computer reset) just go to Your folder and run:\nsource &lt;name of your env&gt;/bin/activate\njupyterlab"
  },
  {
    "objectID": "info.html#python-env-with-jupyter-lab",
    "href": "info.html#python-env-with-jupyter-lab",
    "title": "All tools",
    "section": "",
    "text": "For our first a few laboratories we will use just python codes. Check what is Your Python3 environment.\nIn the terminal try first:\npython\n# and\npython3\nI have python3 (You shouldn’t use python 2.7 version) so i create a new and a clear python environment.\nThe easiest way how to run a JupyterLab with your new python env. For  You can choose what You want.\npython3 -m venv &lt;name of Your env&gt;\n\nsource &lt;name of your env&gt;/bin/activate\n# . env/bin/activate\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# or\npip install -r requirements.txt\n\njupyterlab\ngo to web browser: localhost:8888\nIf You want rerun jupyterlab (after computer reset) just go to Your folder and run:\nsource &lt;name of your env&gt;/bin/activate\njupyterlab"
  },
  {
    "objectID": "info.html#python-env-with-jupyterlab-docker-version",
    "href": "info.html#python-env-with-jupyterlab-docker-version",
    "title": "All tools",
    "section": "Python env with JupyterLAB Docker Version",
    "text": "Python env with JupyterLAB Docker Version\n\nCookiecutter project\nFrom GitHub repository You can find how to use a cookiecutter for any data science project or other kind of programs.\nTo run and build full dockerfile project: Create python env and install cookiecutter library.\npython3 -m venv venv\nsource venv/bin/activate\npip --no-cache install --upgrade pip setuptools\npip install cookiecutter\nand run:\ncookiecutter https://github.com/sebkaz/jupyterlab-project\nYou can run a cookiecutter project directly from GitHub repo.\nAnswer questions:\ncd jupyterlab\ndocker-compose up -d --build\nTo stop:\ndocker-compose down\n\n\nCookiecutter with config yaml file\n\nPython, Julia, R\nAll + Apache Spark\n\nClone repo and run:\npython3 -m cookiecutter https://github.com/sebkaz/jupyterlab-project --no-input --config-file=spark_template.yml --overwrite-if-exists"
  },
  {
    "objectID": "info.html#start-with-github",
    "href": "info.html#start-with-github",
    "title": "All tools",
    "section": "Start with GitHub",
    "text": "Start with GitHub\nText from web site\nWhen You working on a project, e.g. a master’s thesis, (alone or in a team) you often need to check what changes, when and by whom were introduced to the project. The “version control system” or GIT works great for this task.\nYou can download and install Git like a regular program on any computer. However, most often (small projects) you use websites with some kind of git system. One of the most recognized is GitHub (www.github.com) which allows you to use the git system without installing it on your computer.\nIn the free version of the GitHub website, you can store your files in public (everyone has access) repositories. We will only focus on the free version of GitHub:\ngit --version"
  },
  {
    "objectID": "info.html#github",
    "href": "info.html#github",
    "title": "All tools",
    "section": "GitHub",
    "text": "GitHub\nAt the highest level, there are individual accounts (eg. http://github.com/sebkaz or those set up by organizations. Individual users can create ** repositories ** public (public) or private (private).\nOne file should not exceed 100 MB.\nRepo (shortcut to repository) is created with Create a new repository. Each repo should have an individual name.\n\nBranches\nThe main (created by default) branch of the repository is named master.\n\n\nMost important commends\n\nclone of Your repository\n\ngit clone https://adres_repo.git\n\nIn github case, you can download the repository as a ‘zip’ file.\n\n\nRepository for local directory\n\n# new directory\nmkdir datamining\ncd datamining\n# init repo\ngit init\n# there sould be a .git new directory\n# add file\necho \"Info \" &gt;&gt; README.md\n\nlocal and web version connection\n\ngit remote add origin https://github.com/&lt;twojGit&gt;/nazwa.git\n\n3 steps\n\n# status check\ngit status\n# 1. add all changes\ngit add .\n# 2. commit all changes with message\ngit commit -m \" message \"\n# 3. and\ngit push origin master\nYou can watch Youtube course.\nAll the necessary programs will be delivered in the form of docker containers."
  },
  {
    "objectID": "info.html#start-with-docker",
    "href": "info.html#start-with-docker",
    "title": "All tools",
    "section": "Start with Docker",
    "text": "Start with Docker\nIn order to download the docer software to your system, go to the page.\nIf everything is installed correctly, follow these instructions:\n\nCheck the installed version\n\ndocker --version\n\nDownload and run the image Hello World and\n\ndocker run hello-world\n\nOverview of downloaded images:\n\ndocker image ls\n\ndocker images\n\nOverview of running containers:\n\ndocker ps \n\ndocker ps -all\n\nStopping a running container:\n\ndocker stop &lt;CONTAINER ID&gt;\n\nContainer removal\n\ndocker rm -f &lt;CONTAINER ID&gt;\nI also recommend short intro"
  },
  {
    "objectID": "info.html#docker-as-an-application-continuation-tool",
    "href": "info.html#docker-as-an-application-continuation-tool",
    "title": "All tools",
    "section": "Docker as an application continuation tool",
    "text": "Docker as an application continuation tool\nDocker with jupyter notebook"
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Books and WWW pages",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\n\n\n\n\n\nA. Bellemare Mikrousługi oparte na zdarzeniach. Wykorzystanie danych w organizacji na dużą skalę Zobacz opis lub Kup\nG. Shapira, T. Palino, R. Sivaram, K. Petty Kafka The Definitive Guide. Real-time data and stream processing at scale. Second edition, 2021. O’REILLY.\nJ. Korstanje Machine Learning for Streaming Data with Python, 2022. PACKT\n\n\n\n\n\nA. Geron Uczenie maszynowe z użyciem Scikit-Learn i TensorFlow. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nSebastian Raschka Python. Uczenie maszynowe. Wydanie II. Zobacz opis lub Kup książkę.\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocą pakietów Pandas i NumPy oraz środowiska IPython. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieć. Algorytmy przyszłości. Wydanie II (ebook) Zobacz opis lub Kup książkę, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup książkę, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup książkę.\nR. Schutt, C. O’Neil Badanie danych. Raport z pierwszej lini działań. Zobacz opis lub Kup książkę.\nT. Segaran Nowe usługi 2.0. Przewodnik po analizie zbiorów danych Zobacz opis lub Kup książkę, Kup e-book\nT. Morzy Eksploracja Danych. Metody i algorytmy, PWN, 2013.\nKrzyśko, Wołyński, Górecki, Skorzybut, Systemy uczące się . WNT, 2008\n\n\n\n\n\nF. Chollet Deep Learning. Praca z językiem Python i biblioteką Keras. Zobacz opis lub Kup książkę, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie głębokie z językiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup książkę, Kup e-book\nS. Weidman Uczenie głębokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup książkę, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyć komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup książkę, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programistów. Budowanie aplikacji AI za pomocą fastai i PyTorch Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nSpark. Zaawansowana analiza danych (ebook) Zobacz opis lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\nJ. Quddus Machine Learning with Apache Spark Quick Start Guide (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind Zrozumieć programowanie Zobacz opis lub Kup książkę, Kup e-book\nA. Allain C++. Przewodnik dla początkujących Zobacz opis lub Kup książkę, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdrażanie aplikacji Zobacz opis lub Kup książkę, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zadań z pythonem. Zobacz opis lub Kup książkę, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzędzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuka tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage.\nL. Suskind, Mechanika kwantowa, teoretyczne minimum, 2014, Prószyński i s-ka"
  },
  {
    "objectID": "books.html#books",
    "href": "books.html#books",
    "title": "Books and WWW pages",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\n\n\n\n\n\nA. Bellemare Mikrousługi oparte na zdarzeniach. Wykorzystanie danych w organizacji na dużą skalę Zobacz opis lub Kup\nG. Shapira, T. Palino, R. Sivaram, K. Petty Kafka The Definitive Guide. Real-time data and stream processing at scale. Second edition, 2021. O’REILLY.\nJ. Korstanje Machine Learning for Streaming Data with Python, 2022. PACKT\n\n\n\n\n\nA. Geron Uczenie maszynowe z użyciem Scikit-Learn i TensorFlow. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nSebastian Raschka Python. Uczenie maszynowe. Wydanie II. Zobacz opis lub Kup książkę.\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocą pakietów Pandas i NumPy oraz środowiska IPython. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieć. Algorytmy przyszłości. Wydanie II (ebook) Zobacz opis lub Kup książkę, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup książkę, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup książkę.\nR. Schutt, C. O’Neil Badanie danych. Raport z pierwszej lini działań. Zobacz opis lub Kup książkę.\nT. Segaran Nowe usługi 2.0. Przewodnik po analizie zbiorów danych Zobacz opis lub Kup książkę, Kup e-book\nT. Morzy Eksploracja Danych. Metody i algorytmy, PWN, 2013.\nKrzyśko, Wołyński, Górecki, Skorzybut, Systemy uczące się . WNT, 2008\n\n\n\n\n\nF. Chollet Deep Learning. Praca z językiem Python i biblioteką Keras. Zobacz opis lub Kup książkę, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie głębokie z językiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup książkę, Kup e-book\nS. Weidman Uczenie głębokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup książkę, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyć komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup książkę, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programistów. Budowanie aplikacji AI za pomocą fastai i PyTorch Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nSpark. Zaawansowana analiza danych (ebook) Zobacz opis lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\nJ. Quddus Machine Learning with Apache Spark Quick Start Guide (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind Zrozumieć programowanie Zobacz opis lub Kup książkę, Kup e-book\nA. Allain C++. Przewodnik dla początkujących Zobacz opis lub Kup książkę, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdrażanie aplikacji Zobacz opis lub Kup książkę, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zadań z pythonem. Zobacz opis lub Kup książkę, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzędzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuka tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage.\nL. Suskind, Mechanika kwantowa, teoretyczne minimum, 2014, Prószyński i s-ka"
  },
  {
    "objectID": "books.html#www-pages",
    "href": "books.html#www-pages",
    "title": "Books and WWW pages",
    "section": "WWW Pages",
    "text": "WWW Pages\n\nSoftware\n\nGithub\nGit-instrukcja\nwww.python.org\nPyPI python libraries\nAnaconda\nDocker\n\n\n\nPython libraries for data analysis\n\nNumPy\nSciPy\nPandas\nScikit-learn\nJupyter\nMatplotlib\nBeautiful Soup\nTheano\nKeras\nTensorFlow\nVirtual ENV\n\n\n\nText editors\n\nNotepad++\nSublime Text\nVisual Studio Code\n\n\n\nMarkdown\n\nMD\n\n\n\nJupyter notebook\n\nGaleria ciekawych notatników\nIntro\nKernels\nBringing the best out of jupyter for data science\nJupyter extensions\nI don’t like notebooks\nJupyter lab\nSpeed up jupyter notebook\n\n\n\nETL\n\ndata cookbook\n\n\n\nDatasets\n\nInternet Archive\nReddit\nKDnuggets\nKaggle\nList of datasets for machine learning research\nUCI Machine Learning Repo\nPublic API\nGoogle Datatset Search\n\n\n\nPython\n\nChris Albon Technical Notes on Using Data Science & AI\n40+ Python Statistics For Data Science Resources\nPractical Business Python\n\n\n\nML course\n\nKurs Machine Learning - Andrew Ng, Stanford"
  },
  {
    "objectID": "sylabus.html",
    "href": "sylabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Real Time Analytics\nSGH Warsaw School of Economics\nECTS: 3\nLanguage: EN\nlevel: medium\nday of week: Monday/Tuesday\nTeacher: Sebastian Zając, sebastian.zajac@sgh.waw.pl\nWebsite: http://sebkaz-teaching.github.io/EN"
  },
  {
    "objectID": "sylabus.html#description",
    "href": "sylabus.html#description",
    "title": "Syllabus",
    "section": "Description",
    "text": "Description\nMaking the right decisions based on data and their analysis in business is a process and daily. Modern methods of modeling by machine learning (ML), artificial intelligence (AI), or deep learning not only allow better understanding of business, but also support making key decisions for it. The development of technology and increasingly new business concepts of working directly with the client require not only correct but also fast decisions. The classes offered are designed to provide students with experience and comprehensive theoretical knowledge in the field of real-time data processing and analysis, and to present the latest technologies (free and commercial) for the processing of structured data (originating e.g. from data warehouses) and unstructured (e.g. images, sound, video streaming) in on-line mode. The course will present the so called lambda and kappa structures for data processing into data lake along with a discussion of the problems and difficulties encountered in implementing real-time modeling for large amounts of data. Theoretical knowledge will be gained (apart from the lecture part) through the implementation of test cases in tools such as Apache Spark, Nifi, Microsoft Azure and SAS. During laboratory classes student will benefit from fully understand the latest information technologies related to real-time data processing."
  },
  {
    "objectID": "sylabus.html#list-of-topics",
    "href": "sylabus.html#list-of-topics",
    "title": "Syllabus",
    "section": "List of Topics",
    "text": "List of Topics\n\nModelling, learning and prediction in batch mode (offline learning) and incremental (online learning) modes. Problems of incremental machine learning.\nData processing models in Big Data. From flat files to Data Lake. Real-time data myth and facts\nNRT systems (near real-time systems), data acquisition, streaming and analytics.\nAlgorithms for estimating model parameters in incremental mode. Stochastic Gradient Descent.\nLambda and Kappa architecture. Designing IT architecture for real-time data processing.\nPreparation of the micro-service with the ML model for prediction use.\nStructured and unstructured data. Relational databases and NoSQL databases.\nAggregations and reporting in NoSQL databases (on the example of the MongoDB or Cassandra)\nBasic of object-oriented programming in Python in linear and logistic regression, neural network analysis using the sklearn, TensorFlow and Keras.\nIT architecture of Big Data processing. Preparation of a virtual env for Apache Spark."
  },
  {
    "objectID": "sylabus.html#conditions-for-passing",
    "href": "sylabus.html#conditions-for-passing",
    "title": "Syllabus",
    "section": "Conditions for passing",
    "text": "Conditions for passing\n\ntest 30%\npractical test 30% (IF)\ngroup project 40% (70%)"
  },
  {
    "objectID": "sylabus.html#books",
    "href": "sylabus.html#books",
    "title": "Syllabus",
    "section": "Books",
    "text": "Books\n\nS. Zajac, “Modelowanie dla biznesu, Analityka w czasie rzeczywistym - narzędzia informatyczne i biznesowe”. SGH (2022)\nFrątczak E., red. “Modelowanie dla biznesu, Regresja logistyczna, Regresja Poissona, Survival Data Mining, CRM, Credit Scoring”. SGH, Warszawa 2019.\nFrątczak E., red., “Zaawansowane metody analiz statystycznych”, Oficyna Wydawnicza SGH, Warszawa 2012.\nIndest A., Wild Knowledge. Outthik the Revolution. LID publishing.com 2017.\nReal Time Analytic. “The Key to Unlocking Customer Insights & Driving the Customer Experience”. Harvard Business Review Analytics Series, Harvard Business School Publishing, 2018.\nSvolba G., “Applying Data Science. Business Case Studies Using SAS”. SAS Institute Inc., Cary NC, USA, 2017.\nEllis B. “Real-Time Analytics Techniques to Analyze and Visualize Streaming data.” , Wiley, 2014\nFamiliar B., Barnes J. “Business in Real-Time Using Azure IoT and Cortana Intelligence Suite” Apress, 2017"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Info",
    "section": "",
    "text": "Code: 222891-D\nSemester: 2024/2025 University: SGH Warsaw School of Economics\nBasic course information can be found in the sylabus.\nRecommended reading is available in the books section."
  },
  {
    "objectID": "index.html#real-time-analytics",
    "href": "index.html#real-time-analytics",
    "title": "Info",
    "section": "",
    "text": "Code: 222891-D\nSemester: 2024/2025 University: SGH Warsaw School of Economics\nBasic course information can be found in the sylabus.\nRecommended reading is available in the books section."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Info",
    "section": "Schedule",
    "text": "Schedule\n\nLectures\nThe lectures are conducted in a hybrid mode. Attendance is optional, and in-person sessions take place in Aula VI, Building G.\n\n\n18-02-2025 (Tuesday) 11:40-13:20 - Lecture 1\n\n\n25-02-2025 (Tuesday) 11:40-13:20 - Lecture 2\n\n\n04-03-2025 (Tuesday) 11:40-13:20 - Online Lecture 3\n\n11-03-2025 (Tuesday) 11:40-13:20 - Lecture 4\n18-03-2025 (Tuesday) 11:40-13:20 - Lecture 5\n\nThe lecture 5 concludes with a TEST.\nFormat: 20 questions | 30 minutes Platform: MS Teams\n\n\nTEST\nLectures will end with a test (last class). Positive evaluation of the test (above 13 points) entitles you to carry out the exercises.\nAfter the exercises, homework will be carried out via the MS teams’ platform. Passing all exercises and tasks entitles you to complete the project.\nThe project should be carried out in groups of no more than 5 people.\nProject requirements:\n\nThe project should present a BUSINESS PROBLEM that can be implemented using the information provided online. (This does not mean that you cannot use batch processing, e.g. to generate a model).\nData should be sent to Apache Kafka and further processed and analyzed from there.\nThe programming language is free - applies to each component of the project.\nBI tools can be used\nData sources can be a table, artificially generated data, IoT, etc."
  },
  {
    "objectID": "index.html#technology",
    "href": "index.html#technology",
    "title": "Info",
    "section": "Technology",
    "text": "Technology\nParticipating in the classes, you must know and at least use the following information technologies:\n\nGIT\nPython, Jupyter notebook, Jupyter lab, Colab\nDocker\nApache Spark, Apache Flink, Apache Kafka, Apache Beam\nDatabricks Community edition Web page."
  },
  {
    "objectID": "lecture3.html",
    "href": "lecture3.html",
    "title": "Lecture 3",
    "section": "",
    "text": "⏳ Duration: 1.5h\n🎯 Lecture Goal\nIntroducing students to the fundamentals of real-time analytics."
  },
  {
    "objectID": "lecture3.html#definitions",
    "href": "lecture3.html#definitions",
    "title": "Lecture 3",
    "section": "Definitions",
    "text": "Definitions\n\nLearn more about streaming data\n\nDefinition 1 – Event is anything that can be observed at a specific point in time. It is generated as a direct result of an action.\nDefinition 2 – In the context of data, an event is an immutable record in a data stream, encoded as JSON, XML, CSV, or in a binary format.\nDefinition 3 – A continuous event stream is an infinite set of individual events ordered over time, such as logs from devices.\nDefinition 4 – A data stream refers to data created incrementally over time, generated from static sources (e.g., database, file read lines) or dynamic sources (e.g., logs, sensors, functions).\nA business is an organization that generates and responds to a continuous stream of events.\n\n\n\nStream Analytics\nStream Analytics (also called Event Stream Processing) refers to processing large amounts of data as they are generated, in real-time.\nRegardless of the technology used, all data exists as a continuous stream of events, including: - User actions on websites,\n- System logs,\n- Sensor measurements."
  },
  {
    "objectID": "lecture3.html#time-in-real-time-data-analysis",
    "href": "lecture3.html#time-in-real-time-data-analysis",
    "title": "Lecture 3",
    "section": "Time in Real-Time Data Analysis",
    "text": "Time in Real-Time Data Analysis\nIn batch processing, we analyze historical data, and the timing of the process is unrelated to when the events actually occurred.\nIn stream processing, there are two key concepts of time: 1. Event Time – the actual moment the event occurred. 2. Processing Time – the moment the system processes the event.\n\nIdeal Data Processing\nIn an ideal scenario, processing happens immediately after the event occurs:\n\n\n\nReal Data Processing\nIn reality, data processing always involves some delay, visible as points below the ideal processing line (below the diagonal in the chart):\n\nIn stream processing applications, the difference between event time and processing time is crucial. Common causes of delays include:\n\nData transmission over the network,\nLack of communication between the device and the network.\n\nAn example of this is tracking the location of a car via a GPS application – passing through a tunnel might cause temporary data loss.\n\n\nHandling Delays in Stream Processing\nDelays in event processing can be managed in two main ways: 1. Monitoring the number of missed events and triggering an alarm if the number of rejected events exceeds a threshold. 2. Applying correction using watermarking, which is an additional mechanism that accounts for delayed events.\nThe process of real-time event processing can be represented as a step function:\n\nNot all events contribute to the analysis – some may be discarded due to excessive delays.\nBy using watermarking, additional time is allowed for the appearance of delayed events. This process includes all events above the dashed line. However, there might still be cases where some points are skipped.\n\nThe situations illustrated in the charts explicitly indicate why the concept of time is a critical factor and needs to be clearly defined at the business requirements level. Assigning timestamps to data (events) is a complex task."
  },
  {
    "objectID": "lecture3.html#time-windows-in-stream-analytics",
    "href": "lecture3.html#time-windows-in-stream-analytics",
    "title": "Lecture 3",
    "section": "Time Windows in Stream Analytics",
    "text": "Time Windows in Stream Analytics\nIn stream processing, time windows allow grouping data into time-limited segments, enabling event analysis within specific time intervals. Depending on the use case, various types of windows are applied, tailored to the characteristics of the data and analytical requirements.\n\n\n1. Tumbling Window\nA tumbling window is a fixed-length window that does not overlap – each event belongs to only one window.\n✅ Characteristics:\n- Fixed window length\n- No overlap between windows\n- Ideal for dividing data into equal time segments\n📌 Example: Analyzing the number of orders in an online store every 5 minutes.\n\n\n\n\n2. Sliding Window\nA sliding window includes all events occurring within a specific time interval, where the window slides continuously.\n✅ Characteristics:\n- Each event can belong to multiple windows\n- The window shifts by a specified interval\n- Useful for detecting trends and anomalies\n📌 Example: Tracking the average temperature over the last 10 minutes, updated every 2 minutes.\n\n\n\n\n3. Hopping Window\nA hopping window is similar to a tumbling window, but it allows overlapping windows, meaning one event can belong to multiple windows. It is used to smooth data.\n✅ Characteristics:\n- Fixed window length\n- Overlapping windows\n- Useful for noise reduction in data\n📌 Example: Analyzing the number of website visitors every 10 minutes, but updated every 5 minutes to better capture trends.\n\n\n\n\n4. Session Window\nA session window groups events based on activity periods and closes after a specified period of inactivity.\n✅ Characteristics:\n- Dynamic window length\n- Defined by user activity\n- Used in user session analysis\n📌 Example: Analyzing user sessions on a website – the session lasts as long as the user is active, but ends after 15 minutes of inactivity.\n\n\n\nSummary\nDifferent types of time windows are applied depending on the data’s characteristics and the analysis objectives. Choosing the right window impacts the accuracy of results and the efficiency of the analytical system.\n\n\n\n\n\n\n\n\nWindow Type\nCharacteristics\nUse Cases\n\n\n\n\nTumbling\nFixed length, no overlap\nPeriodic reports\n\n\nSliding\nFixed length, overlapping windows\nTrend detection, anomaly detection\n\n\nHopping\nFixed length, partial overlap\nData smoothing\n\n\nSession\nDynamic length, activity-dependent\nUser session analysis\n\n\n\nEach window type has its unique use cases and helps with better interpretation of streaming data. The choice of method depends on business needs and the nature of the analyzed data.\nIn stream data analysis, interpreting time is a complex issue due to: 1. Different systems having different clocks, leading to inconsistencies, 2. Data arriving with delays, requiring watermarking and time window techniques, 3. Different approaches to event time vs. processing time impacting result accuracy."
  }
]