{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e569a8a8",
   "metadata": {},
   "source": [
    "# Intorduction to Apache Spark \n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Lab6\").getOrCreate()\n",
    "```\n",
    "\n",
    "```python\n",
    "spark\n",
    "\n",
    "SparkSession - in-memory\n",
    "\n",
    "SparkContext\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "## Example 1 - time series as batch processing\n",
    "\n",
    "```python\n",
    "temp = ((12.5, \"2019-01-02 12:00:00\"),\n",
    "(17.6, \"2019-01-02 12:00:20\"),\n",
    "(14.6,  \"2019-01-02 12:00:30\"),\n",
    "(22.9,  \"2019-01-02 12:01:15\"),\n",
    "(17.4,  \"2019-01-02 12:01:30\"),\n",
    "(25.8,  \"2019-01-02 12:03:25\"),\n",
    "(27.1,  \"2019-01-02 12:02:40\"),\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "To create a Spark DataFrame ypu can use a `createDataFrame` method on spark object. \n",
    "In the Spark DataFrame we can specify the schema of the data.\n",
    "\n",
    "Let's defined our schema as follows:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"temp\", DoubleType(), True),\n",
    "    StructField(\"time\", StringType(), True),\n",
    "])\n",
    "```\n",
    "As you can see all elements beyond the column name and True parameter are considered as python objects. \n",
    "\n",
    "```python\n",
    "df = (spark.createDataFrame(temp, schema=schema)\n",
    "      .withColumn(\"time\", to_timestamp(\"time\")))\n",
    "```\n",
    "\n",
    "Let's see how it looks like now.\n",
    "```python\n",
    "df.printSchema()\n",
    "\n",
    "root\n",
    " |-- temperatura: double (nullable = true)\n",
    " |-- czas: timestamp (nullable = true)\n",
    "\n",
    "\n",
    "``` \n",
    "\n",
    "Next, let's see the data:\n",
    "```python\n",
    "print(\"Schema:\")\n",
    "df.show()\n",
    "+-----------+-------------------+\n",
    "|temperatura|               czas|\n",
    "+-----------+-------------------+\n",
    "|       12.5|2019-01-02 12:00:00|\n",
    "|       17.6|2019-01-02 12:00:20|\n",
    "|       14.6|2019-01-02 12:00:30|\n",
    "|       22.9|2019-01-02 12:01:15|\n",
    "|       17.4|2019-01-02 12:01:30|\n",
    "|       25.8|2019-01-02 12:03:25|\n",
    "|       27.1|2019-01-02 12:02:40|\n",
    "+-----------+-------------------+\n",
    "```\n",
    "\n",
    "### Spark as SQL\n",
    "\n",
    "In Spark we can use SQL queries to get data from our DataFrame.\n",
    "\n",
    "```python\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"SELECT * FROM df where temp > 21\").show()\n",
    "+-------------------+-----------+\n",
    "|               czas|temperatura|\n",
    "+-------------------+-----------+\n",
    "|2019-01-02 12:01:15|       22.9|\n",
    "|2019-01-02 12:03:25|       25.8|\n",
    "|2019-01-02 12:02:40|       27.1|\n",
    "+-------------------+-----------+\n",
    "```\n",
    "\n",
    "### Data gruping\n",
    "\n",
    "Standard groupby on data from a time series will give us a result with a number of rows in each group. Because the time variables have different values, the number of groups obtained will be equal to the number of rows in the table.\n",
    "\n",
    "\n",
    " \n",
    "```python\n",
    "df2 = df.groupBy(\"time\").count()\n",
    "df2.show()\n",
    "```\n",
    "\n",
    "You can use the `window` function to group data by time intervals. For example, we can group data by 30-second intervals.\n",
    "\n",
    "\n",
    "```python\n",
    "# Thumbling window\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df2 = df.groupBy(F.window(\"time\",\"30 seconds\")).count()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "+------------------------------------------+-----+\n",
    "|window                                    |count|\n",
    "+------------------------------------------+-----+\n",
    "|{2019-01-02 12:00:00, 2019-01-02 12:00:30}|2    |\n",
    "|{2019-01-02 12:00:30, 2019-01-02 12:01:00}|1    |\n",
    "|{2019-01-02 12:01:00, 2019-01-02 12:01:30}|1    |\n",
    "|{2019-01-02 12:01:30, 2019-01-02 12:02:00}|1    |\n",
    "|{2019-01-02 12:03:00, 2019-01-02 12:03:30}|1    |\n",
    "|{2019-01-02 12:02:30, 2019-01-02 12:03:00}|1    |\n",
    "+------------------------------------------+-----+\n",
    "```\n",
    "Let's check the schema of the resulting DataFrame\n",
    "\n",
    "```python\n",
    "df2.printSchema()\n",
    "root\n",
    " |-- window: struct (nullable = false)\n",
    " |    |-- start: timestamp (nullable = true)\n",
    " |    |-- end: timestamp (nullable = true)\n",
    " |-- count: long (nullable = false)\n",
    "``` \n",
    "\n",
    "The main difference between pandas dataframes and spark dataframes is that in spark dataframes you can use complex types - for example `struct`.\n",
    "\n",
    "\n",
    "# ðŸ”Œ Data sources in Spark Structured Streaming\n",
    "\n",
    "Spark Streaming can be used to process data in real time from various sources. The most popular sources of streaming data are:\n",
    "\n",
    "## âœ… rate â€” only for testing\n",
    "\n",
    "-\tin each dataframe we have the following columns:\n",
    "-\ttimestamp \n",
    "-\tvalue â€“ count numbers (0, 1, 2, â€¦).\n",
    "\n",
    "```python\n",
    "df = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
    "```\n",
    "### ðŸ“¡ Stream sources:\n",
    "\n",
    "`Socket` (only for testing:  nc -lk 9999): Read stream from a socket.\n",
    "\n",
    "`Files`: connect to a directory and read files as streaming data. Can be used with the following file formats: CSV, JSON, ORC or Parquet (np. .csv, .json, .parquet).\n",
    "\n",
    "`Kafka`: Read data stream from Apache Kafka.\n",
    "\n",
    "\n",
    "Let's create a stream from the `rate` source:\n",
    "\n",
    "```python\n",
    "%%file streamrate.py\n",
    "## run with spark-submit streamrate.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"StreamingDemo\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "df = (spark.readStream\n",
    "      .format(\"rate\")\n",
    "      .option(\"rowsPerSecond\", 1)\n",
    "      .load()\n",
    ")\n",
    "\n",
    "\n",
    "query = (df.writeStream \n",
    "    .format(\"console\") \n",
    "    .outputMode(\"append\") \n",
    "    .option(\"truncate\", False) \n",
    "    .start()\n",
    ") \n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "But, You can run it also on jupyter notebook:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"StreamingDemo\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "def process_batch(df, batch_id, tstop=5):\n",
    "    print(f\"Batch ID: {batch_id}\")\n",
    "    df.show(truncate=False)\n",
    "    if batch_id == tstop:\n",
    "        df.stop()\n",
    "\n",
    "\n",
    "df = (spark.readStream\n",
    "      .format(\"rate\")\n",
    "      .option(\"rowsPerSecond\", 1)\n",
    "      .load()\n",
    ")\n",
    "\n",
    "query = (df.writeStream \n",
    "    .format(\"console\") \n",
    "    .outputMode(\"append\")\n",
    "    .foreachBatch(process_batch)\n",
    "    .option(\"truncate\", False) \n",
    "    .start()\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
