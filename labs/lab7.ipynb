{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c50f9aa4",
   "metadata": {},
   "source": [
    "# Zarzadzanie zrodlami danych strumieniowych i segmentacja klientow\n",
    "\n",
    "## üîÑ Wprowadzenie\n",
    "\n",
    "W tym laboratorium zapoznasz sie z roznymi metodami zasilania danych strumieniowych w Apache Spark oraz zastosowaniem prostych transformacji, filtrowania i segmentacji klientow w czasie rzeczywistym.\n",
    "\n",
    "\n",
    "## üí° Pomocnicza funkcja do wyswietlania naszych danych strumieniowych\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb748ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_counter = {\"count\": 0}\n",
    "\n",
    "def process_batch(df, batch_id):\n",
    "    batch_counter[\"count\"] += 1\n",
    "    print(f\"Batch ID: {batch_id}\")\n",
    "    df.show(truncate=False)\n",
    "    if batch_counter[\"count\"] % 5 == 0:\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cac79e1",
   "metadata": {},
   "source": [
    "## üîπ rate jako ≈∫r√≥d≈Ço kontrolowanego strumienia\n",
    "\n",
    "### ‚úÖ Zadanie 1\n",
    "\n",
    "1. Przygotuj strumien danych z `format('rate')`, ustaw `rowsPerSecond` na 5.\n",
    "2. Utworz kolumne `user_id`: `expr(\"concat('u', cast(rand()*100 as int))\")`\n",
    "3. Dodaj kolumne `event_type`: `expr(\"case when rand() > 0.7 then 'purchase' else 'view' end\")`\n",
    "\n",
    "```python\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "spark = SparkSession.builder.appName(\"StreamingDemo\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "rate_df = (spark....)\n",
    "\n",
    "events = (rate_df....)\n",
    "    \n",
    "query = (events.writeStream\n",
    "         .format(\"console\")\n",
    "         .foreachBatch(process_batch)\n",
    "         .start())\n",
    "```\n",
    "\n",
    "\n",
    "## üîπ Filtrowanie danych bez agregacji (append mode)\n",
    "\n",
    "Zobacz jak dzia≈Ça poni≈ºszy kod i na jego podstawie:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"AppendExample\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# ≈πr√≥d≈Ço rate - generuje timestamp + value\n",
    "rate_df = (spark.readStream\n",
    "           .format(\"rate\")\n",
    "           .option(\"rowsPerSecond\", 5)\n",
    "           .load())\n",
    "\n",
    "# Filtracja bez potrzeby agregacji (bezstanowe przetwarzanie)\n",
    "filtered = rate_df.filter(col(\"value\") % 2 == 0) \\\n",
    "                  .withColumn(\"info\", expr(\"concat('even:', value)\"))\n",
    "\n",
    "# outputMode = append ‚Üí pokazuje tylko nowe wiersze, bez stanu\n",
    "query = (filtered.writeStream \n",
    "    .outputMode(\"append\") \n",
    "    .format(\"console\") \n",
    "    .option(\"truncate\", False) \n",
    "    .foreachBatch(process_batch)\n",
    "    .start()\n",
    "        )\n",
    "```\n",
    "\n",
    "1. Skorzystaj z danych z poprzedniego zadania.\n",
    "2. Wyfiltruj tylko `purchase`.\n",
    "\n",
    "\n",
    "```python\n",
    "purchases = events....\n",
    "\n",
    "query = (purchases.writeStream\n",
    "         .format(\"console\")\n",
    "         .outputMode(\"append\")\n",
    "         .foreachBatch(process_batch)\n",
    "         .start())\n",
    "\n",
    "```\n",
    "\n",
    "## üîπ ≈πr√≥d≈Ço plikowe (JSON)\n",
    "\n",
    "### ‚úÖ Generator danych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c8c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file generator.py\n",
    "# generator.py\n",
    "import json, os, random, time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "output_dir = \"data/stream\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "event_types = [\"view\", \"cart\", \"purchase\"]\n",
    "categories = [\"electronics\", \"books\", \"fashion\", \"home\", \"sports\"]\n",
    "\n",
    "def generate_event():\n",
    "    return {\n",
    "        \"user_id\": f\"u{random.randint(1, 50)}\",\n",
    "        \"event_type\": random.choices(event_types, weights=[0.6, 0.25, 0.15])[0],\n",
    "        \"timestamp\": (datetime.utcnow() - timedelta(seconds=random.randint(0, 300))).isoformat(),\n",
    "        \"product_id\": f\"p{random.randint(100, 120)}\",\n",
    "        \"category\": random.choice(categories),\n",
    "        \"price\": round(random.uniform(10, 1000), 2)\n",
    "    }\n",
    "\n",
    "# Simulate file-based streaming\n",
    "while True:\n",
    "    batch = [generate_event() for _ in range(50)]\n",
    "    filename = f\"{output_dir}/events_{int(time.time())}.json\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        for e in batch:\n",
    "            f.write(json.dumps(e) + \"\\n\")\n",
    "    print(f\"Wrote: {filename}\")\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed87728d",
   "metadata": {},
   "source": [
    "### ‚úÖ Schemat danych:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"user_id\": \"u123\",\n",
    "  \"event_type\": \"purchase\", // albo \"view\", \"cart\", \"click\"\n",
    "  \"timestamp\": \"2025-05-09T15:24:00Z\",\n",
    "  \"product_id\": \"p456\",\n",
    "  \"category\": \"electronics\",\n",
    "  \"price\": 299.99\n",
    "}\n",
    "```\n",
    "\n",
    "1. Utw√≥rz zmiennƒÖ `schema`, kt√≥ra zrealizuje schamat danych naszej ramki. Wykorzystaj `StringType()`, `TimestampType()`,`DoubleType()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492d896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RealTimeEcommerce\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# StringType(), TimestampType(), DoubleType()\n",
    "\n",
    "schema = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6236d7d3",
   "metadata": {},
   "source": [
    "### ‚úÖ Odczyt danych z katalogu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6406dd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = (spark.readStream\n",
    "          .schema(schema)\n",
    "          .json(\"data/stream\"))\n",
    "\n",
    "query = (stream.writeStream\n",
    "         .format(\"console\")\n",
    "         .foreachBatch(process_batch)\n",
    "         .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18824d29",
   "metadata": {},
   "source": [
    "## üîπ Bezstanowe zliczanie zdarzen\n",
    "\n",
    "1. Przygotuj zmiennƒÖ agg1 zliczajƒÖcƒÖ zdarzenia nale≈ºƒÖce do danej grupy `event_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7225651",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg1 = (stream....)\n",
    "\n",
    "# pamietaj, ≈ºe agregacje wymagajƒÖ opcji complete\n",
    "query = (agg1\n",
    "         .writeStream\n",
    "         .outputMode(\"complete\")\n",
    "         .format(\"console\")\n",
    "         .foreachBatch(process_batch)\n",
    "         .start()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0563b5ad",
   "metadata": {},
   "source": [
    "## üîπ Agregacja w oknach czasowych\n",
    "\n",
    "`withWatermark(\"timestamp\", \"1 minute\")`\n",
    "   \n",
    "üí° Do czego s≈Çu≈ºy: Informuje Sparka, ≈ºe dane przychodzƒÖ z op√≥≈∫nieniem i nale≈ºy je przetwarzaƒá tylko do okre≈õlonego limitu wstecz (tutaj: 1 minuta).\n",
    "\n",
    "üö® Dlaczego wa≈ºne: Bez watermarku Spark trzyma≈Çby w pamiƒôci wszystkie dane, by m√≥c je jeszcze pogrupowaƒá. Watermark pozwala zwolniƒá pamiƒôƒá.\n",
    "\n",
    "1. Pogrupuj typy zdarzen w thumbling window, w oknie co 5 minut\n",
    "2. dodaj watermark z ustawieniem na 1 minutƒô. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4faf54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed = (stream...)\n",
    "\n",
    "query = (\n",
    "    windowed.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .foreachBatch(process_batch)\n",
    "    .format(\"console\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89195398",
   "metadata": {},
   "source": [
    "3. Zmie≈Ñ thumbling window na sliding window z szeroko≈õciƒÖ okna 5 minut i startem nowego okna co 1 minutƒô. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c850c5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed = (stream...)\n",
    "\n",
    "query = (\n",
    "    windowed.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .foreachBatch(process_batch)\n",
    "    .format(\"console\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da52b64",
   "metadata": {},
   "source": [
    "\n",
    "## üîπ Segmentacja klientow\n",
    "\n",
    "üß© Logika segmentacji:\n",
    "\n",
    "1. je≈õli by≈Ç purchase ‚Üí \"Buyer\"\n",
    "2. je≈õli by≈Ç cart, ale nie purchase ‚Üí \"Cart abandoner\"\n",
    "3. je≈õli tylko view ‚Üí \"Lurker\"\n",
    "\n",
    "\n",
    "`groupBy(window(...), \"user_id\")`\n",
    "\n",
    "üí° Do czego s≈Çu≈ºy: Grupujemy dane per u≈ºytkownik w konkretnym przedziale czasu (oknie 5-minutowym).\n",
    "\n",
    "‚è±Ô∏è window(\"timestamp\", \"5 minutes\"): Funkcja okna czasowego ‚Äì ka≈ºda grupa bƒôdzie dotyczyƒá jednego u≈ºytkownika w konkretnym 5-minutowym interwale.\n",
    "\n",
    "`agg(collect_set(\"event_type\"))`\n",
    "\n",
    "üí° Do czego s≈Çu≈ºy: Zbiera wszystkie typy zdarze≈Ñ (view, cart, purchase) danego u≈ºytkownika w danym oknie.\n",
    "\n",
    "üß† Dlaczego `collect_set` a nie `collect_list`?: collect_set usuwa duplikaty ‚Äî interesuje nas tylko czy co≈õ siƒô zdarzy≈Ço, a nie ile razy.\n",
    "\n",
    "5. withColumn(... expr(...))\n",
    "\n",
    "üí° Do czego s≈Çu≈ºy: Na podstawie zbioru zdarze≈Ñ okre≈õlamy segment u≈ºytkownika.\n",
    "\n",
    "üîé `array_contains`: Funkcja sprawdzajƒÖca, czy dany typ zdarzenia znajduje siƒô w tablicy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "üß† Co warto wiedzieƒá:\n",
    "\n",
    "- Segmentacja to klasyczne zastosowanie agregacji i transformacji strumienia.\n",
    "- ≈ÅƒÖczenie window + watermark jest kluczowe do kontroli stanu.\n",
    "- collect_set umo≈ºliwia prostƒÖ analizƒô zachowa≈Ñ, bez potrzeby przechowywania surowych danych.\n",
    "- expr() daje elastyczno≈õƒá, by u≈ºywaƒá sk≈Çadni SQL wewnƒÖtrz kodu DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d171cd79",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Porownanie trybow `outputMode`\n",
    "\n",
    "| outputMode | Opis                                        | Kiedy uzywac                           | Wymagania                 |\n",
    "| ---------- | ------------------------------------------- | -------------------------------------- | ------------------------- |\n",
    "| `append`   | Wypisywane sa **tylko nowe wiersze**        | Filtrowanie, wzbogacanie bez agregacji | Nie dziala z `groupBy`    |\n",
    "| `update`   | Wypisywane sa tylko **zmienione wiersze**   | Agregacje z watermarkami               | Wymaga zarzadzania stanem |\n",
    "| `complete` | Wypisywane jest **calosciowe podsumowanie** | Podsumowania okien, snapshoty          | Moze byc kosztowne        |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
