{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16ca2c89-c745-4f46-8602-a3349b27c807",
   "metadata": {},
   "source": [
    "# Apache Spark on RDD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d5381f-7d0b-4de0-be1e-72a8e64db9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName=\"myAppName\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f21b5e-d0e7-4c4c-add9-004e3dcf91dc",
   "metadata": {},
   "source": [
    "## RDD\n",
    "\n",
    "- Resilient Distributed Dataset\n",
    "- Main abstraction on Spark Core\n",
    "- Two main functions:\n",
    "    - Actions:\n",
    "        - RDD as input - number as output.\n",
    "    - Transformations:\n",
    "        - lazy operation\n",
    "        - RDD as input and RDD as output\n",
    "\n",
    "- In-Memory\n",
    "- Immutable \n",
    "- Lazy evaluated\n",
    "- Parallel\n",
    "- Partitioned\n",
    "\n",
    "## Important information\n",
    "\n",
    "\n",
    "Term                   |Definition\n",
    "----                   |-------\n",
    "RDD                    |Resilient Distributed Dataset\n",
    "Transformation         |Spark operation that produces an RDD\n",
    "Action                 |Spark operation that produces a local object\n",
    "Spark Job              |Sequence of transformations on data with a final action\n",
    "\n",
    "\n",
    "RDD create:\n",
    "\n",
    "Method                      |Result\n",
    "----------                               |-------\n",
    "`sc.parallelize(array)`                  |Create RDD of elements of array (or list)\n",
    "`sc.textFile(path/to/file)`                      |Create RDD of lines from file\n",
    "\n",
    "Transformations\n",
    "\n",
    "Transformation Example                          |Result\n",
    "----------                               |-------\n",
    "`filter(lambda x: x % 2 == 0)`           |Discard non-even elements\n",
    "`map(lambda x: x * 2)`                   |Multiply each RDD element by `2`\n",
    "`map(lambda x: x.split())`               |Split each string into words\n",
    "`flatMap(lambda x: x.split())`           |Split each string into words and flatten sequence\n",
    "`sample(withReplacement=True,0.25)`      |Create sample of 25% of elements with replacement\n",
    "`union(rdd)`                             |Append `rdd` to existing RDD\n",
    "`distinct()`                             |Remove duplicates in RDD\n",
    "`sortBy(lambda x: x, ascending=False)`   |Sort elements in descending order\n",
    "\n",
    "Actions\n",
    "\n",
    "Action                             |Result\n",
    "----------                             |-------\n",
    "`collect()`                            |Convert RDD to in-memory list \n",
    "`take(3)`                              |First 3 elements of RDD \n",
    "`top(3)`                               |Top 3 elements of RDD\n",
    "`takeSample(withReplacement=True,3)`   |Create sample of 3 elements with replacement\n",
    "`sum()`                                |Find element sum (assumes numeric elements)\n",
    "`mean()`                               |Find element mean (assumes numeric elements)\n",
    "`stdev()`                              |Find element deviation (assumes numeric elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdd4f25-1b77-4c00-9bbc-ee8327581059",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['Books', 'DVD', 'CD', 'PenDrive'] \n",
    "\n",
    "key_rdd = sc.parallelize(keywords)  \n",
    "\n",
    "key_rdd.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5716e7-51af-4828-8f5c-eb48cf39329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_small = key_rdd.map(lambda x: x.lower()) \n",
    "\n",
    "key_small.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2827b6-a724-410d-91a0-e24aae226bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8204ee-ec92-40ae-92ec-c612138aca52",
   "metadata": {},
   "source": [
    "## Map reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a61951-804f-4a3b-b753-666b496b1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"new\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fc53bb-1ffd-4b21-be04-2b06af0808cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tekst = sc.textFile(\"MobyDick.txt\")\n",
    "tekst.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab50e81-46a6-4554-b138-fa3777b57f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Word Count on RDD \n",
    "sc.textFile(\"MobyDick.txt\")\\\n",
    ".map(lambda x: re.findall(r\"[a-z']+\", x.lower())) \\\n",
    ".flatMap(lambda x: [(y, 1) for y in x]).reduceByKey(lambda x,y: x + y)\\\n",
    ".take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f34798-8cc4-49b0-b2f8-8fb53a5652aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e40055-4582-4b32-941a-e31d70d96a94",
   "metadata": {},
   "source": [
    "## SPARK STREAMING - OLD Version\n",
    " \n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-arch.png\"/>\n",
    "\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-flow.png\"/>\n",
    "\n",
    "SPARK STREAMING with `discretized stream` *DStream* (RDD sequence).\n",
    "\n",
    "\n",
    "\n",
    "For Spark Streaming you need  2 cores.\n",
    "\n",
    "----\n",
    "- **StreamingContext(sparkContext, batchDuration)** - represents a connection with cluster, `batchDuration` \n",
    "- **socketTextStream(hostname, port)** - Create the stream \n",
    "- **flatMap(f), map(f), reduceByKey(f)** - transformations for DRR\n",
    "- **pprint(n)** \n",
    "- **StreamingContext.start()** - just start stream\n",
    "- **StreamingContext.awaitTermination(timeout)** wait for termination\n",
    "- **StreamingContext.stop(stopSparkContext, stopGraceFully)** - end stream\n",
    "\n",
    "You can generate StreamingContext from an SparkContext object.\n",
    "\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-dstream.png\"/>\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-dstream-ops.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f7ea5-78df-40ee-8d13-ebeff4a2f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread\n",
    "# and batch interval of 1 second\n",
    "\n",
    "sc = SparkContext(\"local[2]\", \"NetworkWordCount2\")\n",
    "ssc = StreamingContext(sc, 2)\n",
    "\n",
    "# DStream\n",
    "lines = ssc.socketTextStream(\"localhost\", 9998)\n",
    "\n",
    "words = lines.flatMap(lambda x: re.findall(r\"[a-z']+\", x.lower()))\n",
    "\n",
    "wordCounts = words.map(lambda word: (word,1)).reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9514ca8-f7ea-4114-ab5f-09f2fb41da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!nc -lk 9998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258e6d8d-126f-4d29-94a3-c91d138c8b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before starting, run a stream data\n",
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()\n",
    "ssc.stop()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32242bbd-fc7f-4bc4-b904-fc6c399676a4",
   "metadata": {},
   "source": [
    "### Stream from a socket\n",
    "\n",
    "```bash\n",
    "nc -lk 9998\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4078e7-d090-45ab-9484-f8e31b4d4823",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file start_stream.py\n",
    "\n",
    "from socket import *\n",
    "import time\n",
    "\n",
    "rdd = list()\n",
    "with open(\"MobyDick.txt\", 'r') as ad:\n",
    "    for line in ad:\n",
    "        rdd.append(line)\n",
    "\n",
    "HOST = 'localhost'\n",
    "PORT = 9998\n",
    "ADDR = (HOST, PORT)\n",
    "tcpSock = socket(AF_INET, SOCK_STREAM)\n",
    "tcpSock.bind(ADDR)\n",
    "tcpSock.listen(5)\n",
    "\n",
    "\n",
    "while True:\n",
    "    c, addr = tcpSock.accept()\n",
    "    print('got connection')\n",
    "    for line in rdd:\n",
    "        try:\n",
    "            c.send(line.encode())\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            break\n",
    "    c.close()\n",
    "    print('disconnected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd4ab29-3ea0-4706-b84a-5b209b4ced71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file streamWordCount.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.appName(\"Stream_DF\").getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    \n",
    "    lines = (spark\n",
    "         .readStream\n",
    "         .format(\"socket\")\n",
    "         .option(\"host\", \"localhost\")\n",
    "         .option(\"port\", 9998)\n",
    "         .load())\n",
    "\n",
    "    words = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "    word_counts = words.groupBy(\"word\").count()\n",
    "\n",
    "    streamingQuery = (word_counts\n",
    "         .writeStream\n",
    "         .format(\"console\")\n",
    "         .outputMode(\"complete\")\n",
    "         .trigger(processingTime=\"5 second\")\n",
    "         .start())\n",
    "\n",
    "    streamingQuery.awaitTermination()\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b485a8-4d34-49b3-96ce-712b24112cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "batch_counter = {\"count\": 0}\n",
    "\n",
    "def process_batch(df, batch_id):\n",
    "    batch_counter[\"count\"] += 1\n",
    "    print(f\"Batch ID: {batch_id}\")\n",
    "    df.show(truncate=False)\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Stream_DF\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    \n",
    "lines = (spark\n",
    "         .readStream\n",
    "         .format(\"socket\")\n",
    "         .option(\"host\", \"localhost\")\n",
    "         .option(\"port\", 9998)\n",
    "         .load())\n",
    "\n",
    "words = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "word_counts = words.groupBy(\"word\").count()\n",
    "\n",
    "streamingQuery = (word_counts.writeStream\n",
    "         .format(\"console\")\n",
    "         .outputMode(\"complete\")\n",
    "         .foreachBatch(process_batch) \n",
    "         .trigger(processingTime=\"5 second\")\n",
    "         .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbf1595-90b8-496e-b68d-ea7c0c335470",
   "metadata": {},
   "source": [
    "# Apache Kafka + Apache Spark stream\n",
    "\n",
    "1. Check if you have a topics in your kafka env\n",
    "    ```bash\n",
    "    cd ~ \n",
    "    kafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092\n",
    "    ```\n",
    "2. create new topic `streamXX` \n",
    "```bash\n",
    "cd ~ \n",
    "kafka/bin/kafka-topics.sh --bootstrap-server broker:9092 --create --topic streamXX\n",
    "```\n",
    "\n",
    "1. Check again topics list `streamXX`\n",
    "\n",
    "2. Run new terminal with producer\n",
    "```bash\n",
    "cd ~ \n",
    "kafka/bin/kafka-console-producer.sh --bootstrap-server broker:9092 --topic stream\n",
    "```\n",
    "Check if you can send and recive messages\n",
    "\n",
    "```bash\n",
    "cd ~ \n",
    "kafka/bin/kafka-console-consumer.sh  --bootstrap-server broker:9092 --topic streamXX \n",
    "```\n",
    "\n",
    "Close producer terminal. \n",
    "\n",
    "## Run stream code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9368ff49-580d-4507-8bfc-95586b9fd791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stream.py\n"
     ]
    }
   ],
   "source": [
    "%%file stream.py\n",
    "\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "\n",
    "KAFKA_SERVER = \"broker:9092\"\n",
    "TOPIC = 'stream'\n",
    "LAG = 2\n",
    "\n",
    "def create_producer(server):\n",
    "    return KafkaProducer(\n",
    "        bootstrap_servers=[server],\n",
    "        value_serializer=lambda x: json.dumps(x).encode(\"utf-8\"),\n",
    "        api_version=(3, 7, 0),\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    producer = create_producer(KAFKA_SERVER)\n",
    "    try:\n",
    "        while True:\n",
    "\n",
    "            message = {\n",
    "                \"time\" : str(datetime.now() )  ,\n",
    "                \"id\" : random.choice([\"a\", \"b\", \"c\", \"d\", \"e\"])     ,\n",
    "                \"temperatura\" : random.randint(-100,100)  ,\n",
    "                \"cisnienie\" :  random.randint(0,50)   ,\n",
    "            }\n",
    "  \n",
    "            producer.send(TOPIC, value=message)\n",
    "            sleep(LAG)\n",
    "    except KeyboardInterrupt:\n",
    "        producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1ff505-9591-4254-a1f4-270e0e68622f",
   "metadata": {},
   "source": [
    "1.  Run in terminal  `stream.py` file\n",
    "```bash\n",
    "python stream.py\n",
    "```\n",
    "\n",
    "## APACHE SPARK \n",
    "\n",
    "let's create spark script for data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f779145-e8db-462f-97dc-538a40438177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%file app.py\n",
    "\n",
    "# spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1 app.py\n",
    "\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "SERVER = \"broker:9092\"\n",
    "TOPIC = 'stream'\n",
    "\n",
    "schema = StructType(\n",
    "        [\n",
    "            StructField(\"time\", TimestampType()),\n",
    "            StructField(\"id\", StringType()),\n",
    "            StructField(\"temperatura\", IntegerType()),\n",
    "            StructField(\"cisnienie\", IntegerType()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "SCHEMA = \"\"\"time Timestamp, id String, temperatura Int, cisnienie Int \"\"\" # DDL string\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "     \n",
    "    raw = (\n",
    "        spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", SERVER)\n",
    "        .option(\"subscribe\", TOPIC)\n",
    "        .load()\n",
    "    )\n",
    "    # query =  (\n",
    "    #     raw.writeStream\n",
    "    #     .outputMode(\"append\")\n",
    "    #     .format(\"console\")\n",
    "    #     .option(\"truncate\", False)\n",
    "    #     .start()\n",
    "    # )\n",
    "    parsed = (raw.select(\"timestamp\", from_json(decode(col(\"value\"), \"utf-8\"), SCHEMA).alias(\"moje_dane\"))\n",
    "                .select(\"timestamp\", \"moje_dane.*\")\n",
    "             )\n",
    "    # query =  (\n",
    "    #     parsed.writeStream\n",
    "    #     .outputMode(\"append\")\n",
    "    #     .format(\"console\")\n",
    "    #     .option(\"truncate\", False)\n",
    "    #     .start()\n",
    "    # )\n",
    "    # gr = parsed.agg(avg(\"temperatura\"), avg(\"cisnienie\"))\n",
    "    # query =  (gr.writeStream\n",
    "    #     .outputMode(\"update\")\n",
    "    #     .format(\"console\")\n",
    "    #     .option(\"truncate\", False)\n",
    "    #     .start()\n",
    "    # )\n",
    "    gr = (parsed.withWatermark(\"timestamp\", \"5 seconds\")\n",
    "    .groupBy(window(\"timestamp\", \"10 seconds\", \"7 seconds\"))\n",
    "    .agg(avg(\"temperatura\"), avg(\"cisnienie\"))\n",
    "         )\n",
    "    query =  (gr.writeStream\n",
    "        .outputMode(\"complete\")\n",
    "        .format(\"console\")\n",
    "        .option(\"truncate\", False)\n",
    "        .start()\n",
    "    )\n",
    "    query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
